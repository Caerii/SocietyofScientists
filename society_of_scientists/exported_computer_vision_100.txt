summary: To build a smarter and safer city, a secure, efficient, and sustainable transportation system is a key requirement. The autonomous driving system (ADS) plays an important role in the development of smart transportation and is considered one of the major challenges facing the automotive sector in recent decades. A car equipped with an autonomous driving system (ADS) comes with various cutting-edge functionalities such as adaptive cruise control, collision alerts, automated parking, and more. A primary area of research within ADAS involves identifying road obstacles in construction zones regardless of the driving environment. This paper presents an innovative and highly accurate road obstacle detection model utilizing computer vision technology that can be activated in construction zones and functions under diverse drift conditions, ultimately contributing to build a safer road transportation system. The model developed with the YOLO framework achieved a mean average precision exceeding 94% and demonstrated an inference time of 1.6 milliseconds on the validation dataset, underscoring the robustness of the methodology applied to mitigate hazards and risks for autonomous vehicles. 


summary: This paper presents a new system for precise Tilapia feeding using computer vision and IoT technology. By monitoring water quality and analyzing fish size and count, the system determines optimal feed amounts.  YOLOv8 was used for keypoint detection to estimate Tilapia weight from length, achieving 94% precision on 3,500 annotated images. The researchers claim this approach could increase production up to 58 times compared to traditional farms. 


summary: This paper presents DuFNet, a novel fast architecture for real-time semantic segmentation. DuFNet builds upon the BiSeNet framework and introduces two new structures: Semantic Information Flow (SIF) and Fringe Information Flow (FIF). SIF encodes the input stage by stage using a ResNet18 backbone, providing context information for the feature fusion module. FIF, consisting of a pooling layer, upsampling operator, and projection convolution layer, enhances spatial detail. Compared to BiSeNet, DuFNet achieves faster speed and comparable performance (72.34% mIoU accuracy and 78 FPS) on the Cityscapes Dataset. 


summary: This paper focuses on automating the process of determining stem water potential (SWP), a crucial metric in precision agriculture.  The authors use a Scholander Pressure Chamber, a standard tool for SWP measurement, and develop computer vision methods to automatically detect stems and classify xylem wetness, which is a key indicator of plant water status.  They collected and annotated video data, applied various vision and machine learning techniques, and conducted extensive model evaluation.  Their best-performing model, combining YOLOv8n for stem detection and ResNet50 for xylem wetness classification, achieved a Top-1 accuracy of 80.98%. 


summary: This paper provides a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5.  Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. YOLOv8 demonstrates high accuracy and real-time capabilities across diverse hardware platforms on benchmarks like Microsoft COCO and Roboflow 100.  Additionally, the study explores YOLOv8's developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment.  Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field. 


summary: This paper examines the relationship between computer vision research and the development of surveillance technologies. Analyzing over 40,000 computer vision papers and patents from the past three decades, the authors found that the majority of these documents self-report enabling the extraction of data about humans, particularly their bodies. This study exposes the pipeline through which computer vision research has been used in the development of surveillance technologies, showing that institutions like universities and "big tech" companies contribute significantly to this process. The researchers discovered that the number of computer vision papers with downstream surveillance patents has increased dramatically since the 1990s, and that the field's history is deeply intertwined with military applications. Furthermore, the paper identifies patterns in the language used in these documents that obscure the extent of surveillance. 


summary: This paper proposes a mobile imaging system for large-scale road safety analysis using computer vision. The system uses a camera-mounted vehicle to capture videos of unconstrained roads and identifies irregularities like missing streetlights, defective traffic signs, and traffic violations. The authors analyze the performance of computer vision algorithms across diverse settings (road types, lighting conditions, traffic density) and evaluate the system on 2000km of road footage, providing quantitative metrics for road safety. They also develop an interactive dashboard for visual inspection and action initiation.  The system is highly automated, affordable, and scalable, offering an alternative to expensive manual inspections or static CCTV infrastructure. 


summary: This paper proposes a reconfigurable CMOS image sensor (CIS) system that improves energy efficiency by skipping uneventful regions or rows during the sensor's readout phase. The skipping is guided by a novel masking algorithm, which optimizes both the sensor and back-end neural networks for applications like autonomous driving and AR/VR. This hardware-algorithm co-design achieves up to 53% reduction in front-end sensor energy while maintaining state-of-the-art accuracy in tasks like object detection and gaze estimation. 


summary: This paper describes the experiences of a computer engineering undergraduate student working in the field of computer vision and robotics.  The student explored how optical flow can be used to detect moving objects when a camera is in motion. The paper highlights challenges faced and strategies used to overcome them. It also discusses the development of both technical and interpersonal skills, including teamwork and diversity. 


summary: This paper proposes a vision-based system for automatic grocery tracking in smart homes. The system combines real-time 360° views of home grocery storage with retail shelving data and a fruits dataset to accurately detect and track groceries. By integrating this system with supply chain and user food interest prediction systems, the paper suggests the potential for fully automated grocery ordering. 


summary: This report focuses on rapidly annotating video footage with bounding boxes for new objects. It presents a UI and workflow designed to expedite this process for any novel target. 


summary: Existing computer vision and object detection methods strongly rely on neural networks and deep learning. This active research area is used for applications such as autonomous driving, aerial photography, protection, and monitoring. Futuristic object detection methods rely on rectangular, boundary boxes drawn over an object to accurately locate its location. The modern object recognition algorithms, however, are vulnerable to multiple factors, such as illumination, occlusion, viewing angle, or camera rotation as well as cost. Therefore, deep learning-based object recognition will significantly increase the recognition speed and compatible external interference. In this study, we use convolutional neural networks (CNN) to recognize items, the neural networks have the advantages of end-to-end, sparse relation, and sharing weights. This article aims to classify the name of the various object based on the position of an object's detected box. Instead, under different distances, we can get recognition results with different confidence. Through this study, we find that this model's accuracy through recognition is mainly influenced by the proportion of objects and the number of samples. When we have a small proportion of an object on camera, then we get higher recognition accuracy; if we have a much small number of samples, we can get greater accuracy in recognition. The epidemic has a great impact on the world economy where designing a cheaper object recognition system is the need of time. First, gather enough samples (custom) as our dataset and use the appropriate Yolov2 model to complete the training and testing. We rendered two separate

summary: This research paper explores the potential of simple visual sensors, like single photoreceptors, to solve vision tasks. The authors question whether such simple sensors can perform comparably to high-resolution cameras and how their design influences effectiveness. They find that a few photoreceptors can indeed solve tasks like visual navigation and control, achieving performance similar to high-resolution cameras. Furthermore, they highlight the crucial role of sensor design and introduce a computational optimization algorithm for finding effective sensor designs. Finally, a human survey reveals that the computationally-found design performs well compared to intuitive designs created by humans. 


summary: Object detection for fisheye cameras in autonomous driving is relatively unexplored due to the radial distortion in the periphery. This paper explores different object detection output representations, including rotated bounding boxes, ellipses, generic polygons, and polar arc/angle representations. The proposed FisheyeDetNet model with polygon representation achieves the best performance, reaching a mAP score of 49.5% on a Valeo fisheye surround-view dataset. This is the first detailed study on object detection using fisheye cameras for autonomous driving scenarios. 


summary: This paper proposes an improved self-checkout system for retail using a modified YOLOv10 network. By incorporating the detection head structure from YOLOv8, the system achieves enhanced product recognition accuracy. Further improvements are made through a specialized post-processing algorithm tailored for self-checkout scenarios. The system outperforms existing methods in both accuracy and checkout speed, offering a new solution for retail automation and insights into optimizing deep learning models for practical applications. 


summary: Early detection of apple flowers, both open and unopened, is crucial for precision thinning and pollination in orchards. This paper proposes a vision system using the YOLOv5 object detection algorithm to identify early-stage flowers in unstructured orchard environments. The system identifies individual flower centroids and groups them into clusters using K-means clustering. The system achieved an accuracy of 81.9% mAP on commercial orchard images for open and unopened flower detection. This research offers a potential solution for automated crop load management in apple orchards. 


summary: DroneVis is a Python library for automating computer vision algorithms on Parrot drones. It offers a variety of computer vision tasks and models, and is easily customizable. The library is well-documented with usage guidelines and examples, available on Github. 


summary: This paper presents two computer vision systems developed to automatically count crustacean larvae in industrial ponds. The first system utilizes an iPhone 11 camera and is tested in indoor conditions with different illumination. The second system employs a DSLR Nikon D510 camera and is tested outdoors. Both systems are based on the 'YOLOv5' CNN model for object detection. The study achieved an accuracy of 88.4% and mAP of 0.855 for the first system, and 86% accuracy and mAP of 0.801 for the second system. Additionally, a growth function for Macrobrachium Rosenberg's larvae is developed to predict fisheries outcomes and analyze fish population dynamics. 


summary: This paper explores the use of computer vision and machine learning to analyze crowd motion in video footage for enhanced security and surveillance.  The authors propose a novel approach that categorizes motion into Arcs, Lanes, Converging/Diverging, and Random/Block motions using Motion Information Images and Blockwise dominant motion data. The paper evaluates different optical flow techniques, CNN models, and machine learning models to achieve this categorization.  The results demonstrate promising accuracy and can be used to train anomaly-detection models, provide behavioral insights based on motion, and enhance scene comprehension. 


summary: This paper describes a driver drowsiness detection system that uses deep learning and OpenCV. The system analyzes facial landmarks from a driver's face to identify drowsiness patterns. It's designed for real-time video processing and has shown high accuracy in detecting drowsiness. The authors believe this system can improve road safety by alerting drivers to fatigue. The code for this system is available on GitHub. 


summary: This paper proposes a cost-effective alternative to graphics tablets for educators, called "Do-It-Yourself Graphics Tab" or "DIY Graphics Tab". This system uses a webcam and computer vision to capture images of a person writing on paper and then outputs the content of the writing onto the screen. It addresses challenges such as hand occlusion, paper movement, poor lighting, and perspective distortion. The paper also includes user experience evaluations from teachers and students. 


summary: This paper investigates the YOLOv5 model for identifying cattle in yards, focusing on the unique biometric solution of muzzle patterns. The study analyzed the performance of eight backbones with the YOLOv5 model and the influence of mosaic augmentation.  The results indicate that YOLOv5 with transformer achieves the best performance, with a mean Average Precision (mAP) 0.5 of 0.995 and mAP 0.5:0.95 of 0.9366. The study also demonstrates the improvement in accuracy achieved by utilizing mosaic augmentation across all backbones. Furthermore, the model demonstrates the ability to detect cattle even with partial muzzle images. Overall, the research concludes that YOLOv5 holds significant potential for automatic cattle identification. 


summary: This document does not appear to contain the abstract of a paper. It seems to be a webpage about arXiv, a platform for sharing research papers. The abstract for a research paper will usually appear directly below the title and author information, often marked with the word "Abstract." It's likely this webpage only provides links to the main arXiv site and Cornell University, where arXiv is hosted. 


summary: For distant iris recognition, long focal length lenses are often used to ensure high resolution. This reduces the depth of field and leads to defocus blur. To address this, we present AquulaCam, an end-to-end autofocus camera that actively refocuses on the iris area of moving objects using a focus-tunable lens. Our computational algorithm predicts the best focus position from a single blurred image and generates a lens diopter control signal automatically, enabling real-time focus tracking of the iris area. We built a testing bench to collect real-world focal stacks for evaluation, achieving an autofocus speed of over 50 fps. The results demonstrate the advantages of our camera for biometric perception in static and dynamic scenes. The code is available at https://github.com/Debatrix/AquulaCam. 


summary: It is a common practice to think of a video as a sequence of images (frames), and re-use deep neural network models that are trained only on images for similar analytics tasks on videos. This paper shows that this "leap of faith" that deep learning models that work well on images will also work well on videos is actually flawed. Even when a video camera is viewing a scene that is not changing in any human-perceptible way, and external factors like video compression and environment (lighting) are controlled, the accuracy of video analytics application fluctuates noticeably. This occurs because successive frames produced by the video camera may look similar visually, but are perceived quite differently by the video analytics applications. The root cause for these fluctuations is the dynamic camera parameter changes that a video camera automatically makes in order to capture and produce a visually pleasing video. The camera inadvertently acts as an "unintentional adversary" because these slight changes in the image pixel values in consecutive frames have a noticeably adverse impact on the accuracy of insights from video analytics tasks that re-use image-trained deep learning models. To address this inadvertent adversarial effect from the camera, the paper explores the use of transfer learning techniques to improve learning in video analytics tasks through the transfer of knowledge from learning on image analytics tasks.  


summary: Traffic safety is a major concern worldwide, with helmet usage being a key factor in preventing head injuries and fatalities caused by motorcycle accidents. This study proposes a real-time helmet violation detection system that utilizes a unique data processing strategy, referred to as few-shot data sampling, to develop a robust model with fewer annotations. The system employs the YOLOv8 object detection model for detecting helmet violations in real-time from video frames. Experimental results demonstrate the effectiveness, efficiency, and robustness of the proposed system, which achieved 7th place in the 2023 AI City Challenge, Track 5, with an mAP score of 0.5861. The code for the few-shot data sampling technique is available at https://github.com/aboah1994/few-shot-Video-Data-Sampling.git. 


summary: Convolutional Neural Networks (CNNs) are widely used in computer vision but are computationally intensive. This paper proposes that many pixels in input images are irrelevant to the task and can be excluded to improve efficiency. The authors demonstrate that on three popular datasets (COCO, MOT Challenge, and PASCAL VOC), roughly 48% of pixels are irrelevant. They introduce the focused convolution, which modifies a CNN's convolutional layers to reject irrelevant pixels, resulting in a 45% reduction in inference latency, energy consumption, and multiply-add count on an embedded device, without loss of accuracy. 


summary: This paper proposes an automated crop field surveillance system using computer vision to address the issue of wild animal trespassing, which causes significant crop damage and financial losses for farmers.  The system aims to reduce crop loss by providing a cost-effective and efficient solution compared to traditional methods like fencing, which are often impractical and expensive. 


summary: This paper proposes ARC, a vision-based system for automatic retail checkout. ARC uses a Convolutional Neural Network to identify items placed under a webcam, aiming to make the checkout process faster, more autonomous, and more convenient. The system was evaluated using a dataset of 100 retail items and achieved reasonable accuracy. The project code and dataset are publicly available. 


summary: This study compares five YOLOv5 variants (YOLOv5n6s, YOLOv5s6s, YOLOv5m6s, YOLOv5l6s, and YOLOv5x6s) for vehicle detection in different environments. The models were evaluated on their ability to detect various vehicle types (Car, Bus, Truck, Bicycle, and Motorcycle) under varying conditions (lighting, occlusion, and weather). Performance metrics like precision, recall, F1-score, and mean Average Precision were used to assess the models' accuracy. YOLOv5n6s showed a good balance of precision and recall, especially for Cars. YOLOv5s6s and YOLOv5m6s improved recall, while YOLOv5l6s performed well for Cars but struggled with Motorcycles and Bicycles. YOLOv5x6s excelled with Buses and Cars but faced challenges identifying Motorcycles. 


summary: Facial analysis is an active research area in computer vision, with many practical applications. Most of the existing studies focus on addressing one specific task and maximizing its performance. In this work, we present a system-level design of a real-time facial analysis system. With a collection of deep neural networks for object detection, classification, and regression, the system recognizes age, gender, facial expression, and facial similarity for each person that appears in the camera view. We investigate the parallelization and interplay of individual tasks. Results on common off-the-shelf architecture show that the system's accuracy is comparable to the state-of-the-art methods, and the recognition speed satisfies real-time requirements. Moreover, we propose a multitask network for jointly predicting the first three attributes, i.e., age, gender, and facial expression. Source code and trained models are available at https://github.com/mahehu/ TUT-live-age-estimator. 


summary: ## Abstract:

Given the rising urban population and the consequential rise in traffic congestion, the implementation of smart parking systems has emerged as a critical matter of concern. Smart parking solutions use cameras, sensors, and algorithms like computer vision to find available parking spaces. This method improves parking place recognition, reduces traffic and pollution, and optimizes travel time. In recent years, computer vision-based approaches have been widely used. However, most existing studies rely on manually labeled parking spots, which has implications for the cost and practicality of implementation. To solve this problem, we propose a novel approach PakLoc, which automatically localizes parking spots. Furthermore, we present the PakSke module, which automatically adjust the rotation and the size of detected bounding box. The efficacy of our proposed methodology on the PKLot dataset results in a significant reduction in human labor of 94.25\%. Another fundamental aspect of a smart parking system is its capacity to accurately determine and indicate the state of parking spots within a parking lot. The conventional approach involves employing classification techniques to forecast the condition of parking spots based on the bounding boxes derived from manually labeled grids. In this study, we provide a novel approach called PakSta for identifying the state of parking spots automatically. Our method utilizes object detector from PakLoc to simultaneously determine the occupancy status of all parking lots within a video frame. Our proposed method PakSta exhibits a competitive performance on the PKLot dataset when compared to other classification methods. 


summary: This paper evaluates and compares the cutting-edge object detection algorithms, YOLOv7, RetinaNet with ResNet50 backbone, RetinaNet with EfficientNet and mask RCNN. It aims to improve the occlusion problem that is to detect hidden cattle from a huge dataset captured by drones using deep learning algorithms for accurate cattle detection. Experimental results showed YOLOv7 was superior with precision of 0.612 when compared to the other two algorithms. The proposed method proved superior to the usual competing algorithms for cow face detection, especially in very difficult cases. 


summary: This study proposes a novel system for automated class attendance using deep learning and image processing. The system utilizes facial recognition techniques to capture and record student presence in educational institutions. The system is designed to be cost-effective, leveraging existing technological infrastructure without the need for additional equipment. The study aims to improve security and streamline attendance processes in educational settings by implementing deep learning algorithms for object detection from images. The proposed system is currently in the testing phase and will be deployed in a school during the 2022-2023 academic year. 


summary: This research investigates the use of deep learning in autonomous driving computer vision technology, focusing on its impact on system performance.  The article analyzes how deep learning techniques like convolutional neural networks (CNN), multi-task joint learning, and deep reinforcement learning are utilized in areas like image recognition, real-time object tracking and classification, environmental perception, decision support, path planning, and navigation. The study reveals that the proposed system boasts over 98% accuracy in image recognition, target tracking, and classification, while demonstrating efficient performance in environmental perception, decision support, path planning, and navigation. The conclusion highlights the potential of deep learning to significantly enhance the accuracy and real-time responsiveness of autonomous driving systems, though acknowledges ongoing challenges in environmental perception and decision support. 


summary: This project explores how machine learning and computer vision could be used to improve accessibility for people with visual impairments. Existing applications on the market have low accuracy and only provide audio feedback. This project aims to build a mobile application that provides audio and haptic feedback about a user's surroundings in real-time, including features for scanning text, detecting objects, and recognizing currency. 


summary: The accuracy of Object Detection is a technology that protects variations of the same objects of the same type (category) of computer and videos.  Common objects to detect on time in this application include a variety of vehicles and other means of transportation and footing.  If we want to exclude different objects from an event we have to use a certain process as Object Localization and has to locate one object in the redesigned time plans. 


summary: This paper proposes a new method for athlete detection based on visual images and an artificial intelligence system. The method uses a binary function based on importance and a space representation model to track the pedestrian's body in detail. The color band learning method is also used to update the target template online to deal with changes in target appearance. The experimental results show that the method has a stable tracking effect even when the appearance and posture of the pedestrians change significantly. 


summary: This paper proposes a novel method for intelligent streetlight management using smart CCTV cameras and semantic segmentation. The system uses a U-Net model with ResNet-34 as its backbone to detect pedestrians and vehicles in real-time video footage from a CCTV camera. Based on the presence or absence of these objects, the streetlights automatically adjust their brightness, turning on at night and dimming when there is no activity. This system aims to conserve energy by optimizing streetlight usage and reducing unnecessary lighting. The approach is designed to be straightforward, cost-effective, and energy-efficient, offering a more sustainable alternative to traditional streetlight management methods. 


summary: Image classification models, including convolutional neural networks (CNNs), perform well on a variety of classification tasks but struggle under conditions of partial occlusion. Methods to improve performance under occlusion, including data augmentation, part-based clustering, and more inherently robust architectures, including Vision Transformer (ViT) models, have, to some extent, been evaluated on their ability to classify objects under partial occlusion. However, evaluations of these methods have largely relied on images containing artificial occlusion, which are typically computer-generated and therefore inexpensive to label. Additionally, methods are rarely compared against each other, and many methods are compared against early, now outdated, deep learning models. We contribute the Image Recognition Under Occlusion (IRUO) dataset, based on the recently developed Occluded Video Instance Segmentation (OVIS) dataset. IRUO utilizes real-world and artificially occluded images to test and benchmark leading methods' robustness to partial occlusion in visual recognition tasks. In addition, we contribute the design and results of a human study using images from IRUO that evaluates human classification performance at multiple levels and types of occlusion. We find that modern CNN-based models show improved recognition accuracy on occluded images compared to earlier CNN-based models, and ViT-based models are more accurate than CNN-based models on occluded images, performing only modestly worse than human accuracy. We also find that certain types of occlusion, including diffuse occlusion, where relevant objects are seen through "holes" in occluders such as fences and leaves,

summary: DifuzCam replaces the camera lens with a different optical element that interferes with incoming light. This design reduces camera size and weight but typically results in low-quality reconstructed images. The paper proposes using a pre-trained diffusion model with a control network and learned transformation to improve reconstruction, achieving state-of-the-art results in quality and perceptuality. The model can even leverage textual descriptions of the scene to further enhance image reconstruction. This method can be applied to other imaging systems for improved results. 


summary: This paper proposes a method for analyzing art classroom teaching behavior using intelligent image recognition. The method uses FFmpeg CODEC for fast target detection, extracts MHI-HOG joint features from the identified foreground target area, and then employs a BP neural network support vector machine joint classifier based on a look-up table for behavior recognition. The motion detection method based on H.264 FFmpeg CODEC video achieves a detection accuracy of 95%, with foreground detection taking only 10 ms, saving 90% of time. The behavior classification and recognition using MHI-HOG joint features show significant improvement, reaching a comprehensive recognition rate of 95%. The integrated BP neural network support vector machine exhibits 97% accuracy in extracting, classifying, and recognizing single sample characteristics. This research aims to identify and analyze classroom behavior and validate the effectiveness of the proposed collaborative classifiers for creating an intelligent classroom environment. 


summary: The paper describes a Raspberry Pi based system that uses a camera to detect and count objects within a target area. Python was used for programming due to its compatibility with the Pi and its ease of use. The system achieved an average efficiency of 90.206%, demonstrating its reliability in detecting and counting objects in images. 


summary: Open-vocabulary detection (OVD) aims to detect objects beyond predefined categories. YOLO-World, a pioneering model incorporating the YOLO series into OVD, is efficient but limited by its neck feature fusion mechanism. Mamba-YOLO-World, a novel YOLO-based OVD model, uses the MambaFusion Path Aggregation Network (MambaFusion-PAN) to address these limitations. This network employs a State Space Model-based feature fusion mechanism with linear complexity and globally guided receptive fields, leveraging multi-modal input sequences and mamba hidden states for selective scanning. Experimental results show that Mamba-YOLO-World outperforms the original YOLO-World on COCO and LVIS benchmarks, while maintaining comparable parameters and FLOPs, and surpasses existing state-of-the-art OVD methods with fewer resources. 


summary: Precision agriculture aims to utilize technological tools to improve productivity in the agro-food sector. This work draws inspiration from bees' UV vision to develop a remote sensing system tailored for UV-reflectance in flower detection. This approach enables feature-rich images for deep learning strawberry flower detection, applicable to a scalable, cost-effective aerial monitoring robotic system. The paper compares the performance of the UV-G-B image detector with a similar system utilizing RGB images. 


summary: Real-time object detection in indoor settings is challenging due to variable lighting and complex backgrounds. This research focuses on developing a refined dataset and a CNN detection model with an attention mechanism, specifically for indoor environments. This approach enhances feature discernment in cluttered scenes, demonstrating competitive accuracy and speed while opening new avenues for real-time indoor object detection research and applications. 


summary: This paper proposes FA-YOLO, an improved object detection model based on YOLOv9. The model utilizes a Fine-grained Multi-scale Dynamic Selection Module (FMDS Module) to select and fuse multi-scale features, leading to improved detection accuracy for various target sizes. Additionally, the Adaptive Gated Multi-branch Focus Fusion Module (AGMF Module) enhances feature fusion by combining gated unit, FMDS Module, and TripletAttention branch outputs. Experimental results demonstrate that FA-YOLO achieves a superior mean Average Precision (mAP) of 66.1% on the PASCAL VOC 2007 dataset, surpassing YOLOv9's performance by 1.0%. The paper further highlights improved detection accuracies for small, medium, and large targets compared to YOLOv9. 


summary: This paper proposes a novel design idea and some implementation methods to perform real-time multi-area target individual detection on video using free drawing. Users can draw areas of interest on the video, which are then used for object detection. The shape of the drawn areas is customizable and the detection areas work independently. The detection results are displayed with a GUI designed using Tkinter. The core design is model-independent, and the code is open source on GitHub. 


summary: Current parking area perception algorithms focus on detecting vacant slots within a limited range, relying on error-prone homographic projection for both labeling and inference. However, recent advancements in Advanced Driver Assistance System (ADAS) require interaction with end-users through comprehensive and intelligent Human-Machine Interfaces (HMIs). These interfaces should present a complete perception of the parking area going from distinguishing vacant slots' entry lines to the orientation of other parked vehicles. This paper introduces Multi-Task Fisheye Cross View Transformers (MT F-CVT), which leverages features from a four-camera fisheye Surround-view Camera System (SVCS) with multihead attentions to create a detailed Bird-Eye View (BEV) grid feature map. Features are processed by both a segmentation decoder and a Polygon-Yolo based object detection decoder for parking slots and vehicles. Trained on data labeled using LiDAR, MT F-CVT positions objects within a 25m x 25m real open-road scenes with an average error of only 20 cm. Our larger model achieves an F-1 score of 0.89. Moreover the smaller model operates at 16 fps on an Nvidia Jetson Orin embedded board, with similar detection results to the larger one. MT F-CVT demonstrates robust generalization capability across different vehicles and camera rig configurations. A demo video from an unseen vehicle and camera rig is available at: this https URL. 


summary: Automating fruit and vegetable detection in real-world scenarios is essential for modern agriculture. This paper presents an end-to-end pipeline for this task, introducing a new dataset named FRUVEG67 with 67 classes of fruits and vegetables captured in unconstrained environments. They develop a semi-supervised data annotation algorithm (SSDA) to label images and propose a Fruit and Vegetable Detection Network (FVDNet), an ensemble version of YOLOv7 with three distinct grid configurations. FVDNet uses averaging for bounding-box prediction and a voting mechanism for class prediction, integrating Jensen-Shannon divergence (JSD) with focal loss to improve smaller object detection. Experimental results show FVDNet outperforms previous YOLO versions with a mean average precision (mAP) of 0.78.  Its efficacy is also tested on open-category refrigerator images, demonstrating promising results. 


summary: In static monitoring cameras, useful contextual information can stretch far beyond the few seconds typical video understanding models might see: subjects may exhibit similar behavior over multiple days, and background objects remain static. Due to power and storage constraints, sampling frequencies are low, often no faster than one frame per second, and sometimes are irregular due to the use of a motion trigger. In order to perform well in this setting, models must be robust to irregular sampling rates. In this paper we propose a method that leverages temporal context from the unlabeled frames of a novel camera to improve performance at that camera. Specifically, we propose an attention-based approach that allows our model, Context R-CNN, to index into a long term memory bank constructed on a per-camera basis and aggregate contextual features from other frames to boost object detection performance on the current frame. We apply Context R-CNN to two settings: (1) species detection using camera traps, and (2) vehicle detection in traffic cameras, showing in both settings that Context R-CNN leads to performance gains over strong baselines. Moreover, we show that increasing the contextual time horizon leads to improved results. When applied to camera trap data from the Snapshot Serengeti dataset, Context R-CNN with context from up to a month of images outperforms a single-frame baseline by 17.9% mAP, and outperforms S3D (a 3d convolution based baseline) by 11.2% mAP. 


summary: Dog owners typically recognize pain cues in their dogs, but automatic pain state recognition is challenging. This paper proposes a novel video-based two-stream deep neural network approach for this problem. It extracts and preprocesses body keypoints, computing features from both keypoints and the RGB video representation. The approach handles self-occlusions and missing keypoints. It also presents a unique video-based dog behavior dataset, annotated for pain presence, and reports good classification results. This work is among the first to address machine learning-based dog pain estimation. 


summary: This paper presents a computer vision-based framework for real-time detection of road traffic crashes (RTCs) using surveillance cameras. The system consists of five modules: vehicle detection using YOLO, vehicle tracking with MOSSE, collision estimation for accident detection, violent flow descriptor (ViF) and SVM classification for crash prediction, and finally, emergency notification via GPS and GSM modules. The goal is to achieve high accuracy with minimal false alarms, using a pipelined approach for efficient implementation. 


summary: This study evaluates the performance of YOLOv8 model configurations for instance segmentation of strawberries into ripe and unripe stages in an open field environment. The YOLOv8n model demonstrated superior segmentation accuracy with a mean Average Precision (mAP) of 80.9%, outperforming other YOLOv8 configurations. In terms of inference speed, YOLOv8n processed images at 12.9 milliseconds, while YOLOv8s, the least-performing model, processed at 22.2 milliseconds. Over 86 test images, YOLOv8n detected 235 ripe fruits and 51 unripe fruits, achieving the fastest inference speed of 24.2 milliseconds. These results highlight the potential of advanced object segmentation algorithms for complex visual recognition tasks in open-field agriculture. 


summary: Object detection is crucial for tasks like traffic management and autonomous vehicles, but detecting small objects in images from distant cameras is challenging due to their size, varied shapes, and cluttered backgrounds. This paper introduces SOD-YOLOv8, a model specifically designed for detecting small objects in scenarios with many of them. It improves multi-path fusion within YOLOv8, incorporates a fourth detection layer for high-resolution information, and utilizes an Efficient Multi-Scale Attention Module (EMA) for better feature extraction. The paper also introduces Powerful-IoU (PIoU) as a replacement for CIoU to enhance detection accuracy. SOD-YOLOv8 significantly outperforms other models in various metrics, particularly in recall, precision, and mAP, without significantly increasing computational cost. The model demonstrates its effectiveness in real-world traffic scenes, proving its reliability and ability to detect small objects even in challenging environments. 


summary: This paper explores the application of computer vision within the realm of computational intelligence. The authors utilize face recognition as a case study, employing image recognition and K-means clustering algorithms to extract feature points and generate a classification model. The study demonstrates the potential of computer vision for tasks such as object recognition, obstacle avoidance, and 3D information extraction, highlighting its role as a key component of artificial intelligence advancement. 


summary: Estimating vehicles' locations is a key component in intelligent traffic management systems (ITMSs). Traditionally, stationary sensors have been used, but the development of advanced sensing and communication technologies on modern vehicles (MVs) allows them to be used as mobile sensors to estimate traffic data. This study explores the capabilities of a monocular camera mounted on an MV to estimate the geolocation of an observed vehicle in a global positioning system (GPS) coordinate system. We propose a new methodology by integrating deep learning, image processing, and geometric computation to address the observed-vehicle localization problem. Our methodology and algorithms, tested using real-world traffic data, effectively estimate the observed vehicle's latitude and longitude dynamically. 


summary: The MINSU (Mobile Inventory and Scanning Unit) algorithm uses computer vision to determine the remaining quantity/fullness of a cabinet. The algorithm follows a five-step process: object detection, foreground subtraction, K-means clustering, percentage estimation, and counting. Object detection identifies the cabinet's location, while foreground subtraction removes the background. K-means clustering simplifies the image for analysis. Finally, percentage estimation and counting determine the material's proportion inside the cabinet, estimating its quantity. 


summary: This paper proposes using an event camera as a Region Proposal Network (RPN) in object detection tasks. The event camera, analogous to rods in the human eye, detects changes in pixel intensity and provides a computationally efficient method for generating region proposals for moving objects. Replacing the traditional RPN with an event camera in a two-stage detector like Mask R-CNN results in faster performance while maintaining comparable accuracy, making it suitable for high-speed perception applications. 


summary: This paper investigates real-time pedestrian recognition on small physical-size computers with low computational resources. Three methods are presented: improved Local Binary Pattern (LBP) features and Adaboost classifier, optimized Histogram of Oriented Gradients (HOG) and Support Vector Machine, and fast Convolutional Neural Networks (CNNs). Results show all three methods achieve real-time pedestrian recognition with accuracy exceeding 95% and speed surpassing 5 fps on a small platform with a 1.8 GHz Intel i5 CPU. These methods can be easily applied to small mobile devices. 


summary: By analyzing the motion of people and other objects in a scene, we demonstrate how to infer depth, occlusion, lighting, and shadow information from video taken from a single camera viewpoint. This information is then used to composite new objects into the same scene with a high degree of automation and realism. In particular, when a user places a new object (2D cut-out) in the image, it is automatically rescaled, relit, occluded properly, and casts realistic shadows in the correct direction relative to the sun, and which conform properly to scene geometry. We demonstrate results (best viewed in supplementary video) on a range of scenes and compare to alternative methods for depth estimation and shadow compositing. 


summary: Accurate crop row detection is often challenged by the varying field conditions present in real-world arable fields. Traditional colour based segmentation is unable to cater for all such variations. The lack of comprehensive datasets in agricultural environments limits the researchers from developing robust segmentation models to detect crop rows. We present a dataset for crop row detection with 11 field variations from Sugar Beet and Maize crops. We also present a novel crop row detection algorithm for visual servoing in crop row fields. Our algorithm can detect crop rows against varying field conditions such as curved crop rows, weed presence, discontinuities, growth stages, tramlines, shadows and light levels. Our method only uses RGB images from a front-mounted camera on a Husky robot to predict crop rows. Our method outperformed the classic colour based crop row detection baseline. Dense weed presence within inter-row space and discontinuities in crop rows were the most challenging field conditions for our crop row detection algorithm. Our method can detect the end of the crop row and navigate the robot towards the headland area when it reaches the end of the crop row. 


summary: This paper explores the use of computer vision techniques for automatically counting bees, which can be used to monitor bee colony health, blooming periods, and the effects of agricultural spraying. Three methods were compared on two datasets, with the ResNet-50 convolutional neural network classifier achieving the best results, with 87% accuracy on BUT1 and 93% accuracy on BUT2. 


summary: Online exams via video conference software like Zoom have been widely adopted due to COVID-19.  However, it is challenging for teachers to supervise online exams from simultaneously displayed student Zoom windows.  This paper presents iExam, an intelligent online exam monitoring and analysis system that utilizes face detection to assist invigilators in real-time student identification and detects common abnormal behaviors (e.g., face disappearing, rotating faces, and replacing with a different person) via a face recognition-based post-exam video analysis.  iExam overcomes several challenges, including capturing exam video streams and analyzing them in real time, utilizing left-corner names displayed on each student's Zoom window to gather ground truth for student faces with dynamic positions, and performing experimental comparisons and optimizations to shorten training and testing time on teachers' PCs.  Evaluations show that iExam achieves high accuracy (90.4% for real-time face detection and 98.4% for post-exam face recognition) while maintaining acceptable runtime performance. The source code is available at https://github.com/VPRLab/iExam. 


summary: Current computer vision (CV) systems use an image signal processing (ISP) unit to convert raw images to visually pleasing RGB images. This process, however, adds unnecessary processing steps for high-level CV applications where visual quality is not paramount. We propose a method to invert the ISP pipeline, enabling training of CV models directly on raw images. We release a raw version of the COCO dataset, showing a 7.1% increase in accuracy for ISP-less systems trained on it. We also introduce energy-efficient in-pixel demosaicing for further accuracy improvement, achieving an 8.1% increase in mAP. Finally, we demonstrate a 20.5% increase in mAP using few-shot learning with novel data. This research enables low-power, ISP-less CV systems, reducing bandwidth and energy requirements. 


summary: This study analyzes the dynamic relationship between industry and academic research in computer vision, using data from top-5 vision conferences. The study finds that the proportion of papers published by industry-affiliated researchers is increasing, and that academics are increasingly joining or collaborating with companies. The analysis further examines the impact of industry presence on the distribution of research topics and citation patterns. The results show that while the research topics are similar in industry and academic papers, there is a strong preference towards citing industry papers. The study explores potential reasons for this citation bias, such as code availability and influence. 


summary: This paper proposes an improved YOLO v4 model, called YOLO v4+, to address the challenges of grape detection in unstructured environments. The model uses a simple, parameterless attention mechanism to refine extracted features and a multi-scale feature fusion module to alleviate feature information loss. It also incorporates focal loss function with adjusted hyperparameters. YOLO v4+ achieves an average precision of 94.25% and an F1 score of 93%, outperforming the original YOLO v4. This model demonstrates better generalization ability and can enhance the applicability and robustness of robotic systems for grape harvesting. 


summary: This paper proposes a real-time object detection model for smart surveillance systems (3s) utilizing the YOLO v3 deep learning architecture. The model is trained on the MS COCO dataset, containing 328,000 annotated image instances. A transfer learning approach is implemented to reduce training time and computing resources. The proposed model achieves an accuracy of 99.71% and an improved mAP of 61.5, demonstrating high performance in detecting objects in surveillance footage. 


summary: This paper focuses on the traditional obstacle detection and type recognition method, whose recognition accuracy, reliability and universality are difficult to meet the technical requirements of intelligent vehicles and unmanned vehicles. The dilated convolutional neural network has the ability to learn autonomously, using the original image as input, without the cumbersome preprocessing process and can extract features of the target object one by one to achieve more accurate recognition. This design will be based on the expanded convolutional neural network, design an obstacle type detection and obstacle recognition application with high recognition accuracy, and good generalization, in which this paper applies the hierarchical structure of the expanded convolutional neural network weight sharing to learn the characteristics of various types of obstacles and extract the global features with characterization significance, combined with the ROI algorithm to achieve real-time obstacle detection and high accuracy type recognition. 


summary: Teaching Computer Science (CS) with handwritten programs on paper offers key educational benefits. It encourages focused learning and careful thought compared to using IDEs with support tools. The familiarity of pen and paper also reduces cognitive load for students new to computers. This approach also opens up opportunities for students with limited computer access. However, a lack of teaching methods and supporting software for handwritten programs is a significant obstacle. 

Optical character recognition (OCR) of handwritten code is challenging due to varied handwriting styles and the importance of indentation. This paper proposes two innovative solutions: 

1. Combining OCR with an indentation recognition module and a language model for post-OCR error correction. This method outperforms existing systems, reducing error from 30% to 5% with minimal hallucination.

2. Utilizing a multimodal language model to recognize handwritten programs in an end-to-end manner. 

The authors believe this research can inspire further pedagogical exploration and contribute to making CS education universally accessible. They also release a dataset of handwritten programs and code for future research. 


summary: This paper introduces CCTVCV, the first computer vision model capable of accurately detecting CCTV and video surveillance cameras in images and video frames. The model was trained on 8387 images with 10419 CCTV camera instances and achieved an accuracy of up to 98.7%. The authors present a comprehensive comparison of model performance and discuss core challenges associated with this research.  They also outline possible privacy-, safety-, and security-related practical applications, including prototypes of CCTV-aware privacy and safety routing and navigation. Finally, they release relevant data and code as open-data and open-source to validate and extend their work. 


summary: Activity recognition computer vision algorithms can be used to detect the presence of autism-related behaviors, including what are termed "restricted and repetitive behaviors", or stimming, by diagnostic instruments. The limited data that exist in this domain are usually recorded with a handheld camera which can be shaky or even moving, posing a challenge for traditional feature representation approaches for activity detection which mistakenly capture the camera's motion as a feature. To address these issues, we first document the advantages and limitations of current feature representation techniques for activity recognition when applied to head banging detection. We then propose a feature representation consisting exclusively of head pose keypoints. We create a computer vision classifier for detecting head banging in home videos using a time-distributed convolutional neural network (CNN) in which a single CNN extracts features from each frame in the input sequence, and these extracted features are fed as input to a long short-term memory (LSTM) network. On the binary task of predicting head banging and no head banging within videos from the Self Stimulatory Behaviour Dataset (SSBD), we reach a mean F1-score of 90.77% using 3-fold cross validation (with individual fold F1-scores of 83.3%, 89.0%, and 100.0%) when ensuring that no child who appeared in the train set was in the test set for all folds. This work documents a successful technique for training a computer vision classifier which can detect human motion with few training examples

summary: This study investigates the effectiveness of various deep learning techniques for recognizing violence in videos. The authors benchmark these techniques on a complex dataset and then evaluate the impact of increasing the dataset size. They found that increasing the dataset size from 500 to 1,600 videos resulted in a significant average accuracy improvement of 6% across four models. 


summary: Hermit crabs play a crucial role in coastal ecosystems, and monitoring them is essential for understanding their response to climate change and pollution. Traditional monitoring methods are time-consuming and labor-intensive. This study proposes a new method combining UAV-based remote sensing with Super-Resolution Reconstruction (SRR) and the CRAB-YOLO detection network. SRR enhances image quality, improving detection accuracy, while the CRAB-YOLO network integrates improvements for detection accuracy and computational efficiency. This approach achieved a significant improvement in detection accuracy, offering a cost-effective and automated solution for extensive hermit crab monitoring. 


summary: This paper proposes a YOLOv8-based framework for layout hotspot detection, enhancing the efficiency of the design rule checking (DRC) process. It introduces a PCA-guided augmentation technique to improve pattern-matching effectiveness by extracting auxiliary information from the layout image and incorporating it as an additional color channel. This augmentation enhances multi-hotspot detection accuracy and reduces false alarms. Evaluations on four datasets demonstrate a precision of 83% and recall of 86%, with a false alarm rate below 7.4%. The approach also shows a 10% improvement in detecting never-seen-before hotspots. 


summary: Shortage of labor in fruit crop production has become a significant challenge. Mechanized and automated machines are a promising alternative to labor-intensive orchard operations. This study proposes a machine vision system using YOLOv8-based instance segmentation to identify trunks and branches of apple trees in the dormant season. Using an RGB-D sensor, the system estimates branch diameter and orientation via Principal Component Analysis, which is then used to calculate limb cross-sectional area (LCSA) and estimate crop-load. The results show a high level of performance in identifying trunks and branches in a dynamic commercial orchard environment, with RMSE of 2.08 mm for branch diameter estimation and 3.95 for crop-load estimation. The system integrates farm management practices into automated decision-making for optimal fruit yield and quality. 


summary: Absence of tamper-proof cattle identification technology was a significant problem preventing insurance companies from providing livestock insurance. This lack of technology had devastating financial consequences for marginal farmers as they did not have the opportunity to claim compensation for any unexpected events such as the accidental death of cattle in Bangladesh. Using machine learning and deep learning algorithms, we have solved the bottleneck of cattle identification by developing and introducing a muzzle-based cattle identification system. The uniqueness of cattle muzzles has been scientifically established, which resembles human fingerprints. This is the fundamental premise that prompted us to develop a cattle identification system that extracts the uniqueness of cattle muzzles. For this purpose, we collected 32,374 images from 826 cattle. Contrast-limited adaptive histogram equalization (CLAHE) with sharpening filters was applied in the preprocessing steps to remove noise from images. We used the YOLO algorithm for cattle muzzle detection in the image and the FaceNet architecture to learn unified embeddings from muzzle images using squared $L_2$ distances. Our system performs with an accuracy of $96.489\%$, $F_1$ score of $97.334\%$, and a true positive rate (tpr) of $87.993\%$ at a remarkably low false positive rate (fpr) of $0.098\%$. This reliable and efficient system for identifying cattle can significantly advance livestock insurance and precision farming. 


summary: This paper introduces INDRA (INdian Dataset for RoAd crossing), the first dataset capturing videos of Indian roads from the pedestrian point-of-view. INDRA contains 104 videos comprising of 26k 1080p frames, each annotated with a binary road crossing safety label and vehicle bounding boxes. The authors trained various classifiers to predict road crossing safety on this data, ranging from SVMs to convolutional neural networks (CNNs). The best performing model, DilatedRoadCrossNet, is a novel single-image architecture tailored for deployment on the Nvidia Jetson Nano and achieves 79% recall at 90% precision on unseen images.  Finally, they present a wearable road crossing assistant running DilatedRoadCrossNet, which can help the blind cross Indian roads in real-time. 


summary: This paper presents a fall detection system using the YOLOv5mu model, achieving a mean average precision (mAP) of 0.995, indicating high accuracy in identifying fall events in smart home environments. The system incorporates advanced data augmentation techniques for improved robustness and adaptability, enabling precise, real-time fall detection. This is critical for enhancing safety and emergency response for residents. Future work will focus on integrating contextual data and exploring multi-sensor approaches to improve performance and applicability across diverse environments. 


summary: Smart surveillance is a challenging task due to the lack of a precise definition of an anomaly and the high computational cost associated with processing large amounts of data. This research addresses these challenges by developing a resource-efficient framework for anomaly recognition using surveillance videos. The proposed Temporal based Anomaly Recognizer (TAR) framework combines a partial shift strategy with a 2D convolutional architecture-based model, namely MobileNetV2. The framework achieves an accuracy of 88% on the UCF Crime dataset, surpassing state-of-the-art methods, while also being computationally efficient enough to handle six video streams simultaneously in real-time. The model also exhibits strong performance on the UCF Crime2Local dataset for multiclass anomaly recognition, achieving 52.7% accuracy. 


summary: The paper focuses on real-time vehicle and pedestrian detection for applications in autonomous driving, AI, and video surveillance. It addresses the challenge of recognizing small objects and pedestrians in real-time, which is crucial for traffic safety and efficiency. The study proposes a deep learning framework, testing different versions of YOLOv8 and RT-DETR models on a dataset of complex urban environments. The YOLOv8 Large model emerges as the most effective, particularly for pedestrian detection, achieving high precision and robustness. The results, including Mean Average Precision and recall rates, demonstrate the model's potential to significantly improve traffic monitoring and safety. This work contributes to real-time, reliable detection in computer vision, establishing new benchmarks for traffic management systems. 


summary: Accurately detecting student behavior in classroom videos can aid in analyzing their classroom performance and improving teaching effectiveness. This paper proposes a Student Classroom Behavior Detection method based on improved YOLOv7. The authors created the Student Classroom Behavior dataset (SCB-Dataset) with 18.4k labels and 4.2k images, focusing on hand raising, reading, and writing behaviors. To enhance detection accuracy, they integrated the biformer attention module and Wise-IoU into the YOLOv7 network. Experiments on the SCB-Dataset resulted in an mAP@0.5 of 79%, a 1.8% improvement over previous results. The SCB-Dataset and code are available at https://github.com/Whiffe/SCB-dataset. 


summary: This paper proposes a visual geo-localization system that determines the geographic locations of places (buildings and road intersections) from images without using GPS data. It combines Scale-Invariant Feature Transform (SIFT) for place recognition, traditional image processing for identifying road junction types, and deep learning with the VGG16 model for classifying road junctions. The most effective techniques have been integrated into an offline mobile application, making it accessible to users in GPS-denied environments. 


summary: This study evaluated the performance of YOLOv8, YOLOv9, and YOLOv10 object detection algorithms for detecting fruitlets (green fruit) in commercial orchards.  The research tested 17 different configurations of the algorithms across 5 apple varieties.  YOLOv9 outperformed YOLOv10 and YOLOv8 in terms of mAP@50, while YOLOv10x achieved the highest precision and recall. YOLOv10b, YOLOv10l, and YOLOv10x had the fastest post-processing speeds, while YOLOv8n exhibited the highest inference speed. 


summary: Autofocus is an important task for digital cameras, yet current approaches often exhibit poor performance. We propose a learning-based approach to this problem, and provide a realistic dataset of sufficient size for effective learning. Our dataset is labeled with per-pixel depths obtained from multi-view stereo. Using this dataset, we apply modern deep classification models and an ordinal regression loss to obtain an efficient learning-based autofocus technique. We demonstrate that our approach provides a significant improvement compared with previous learned and non-learned methods: our model reduces the mean absolute error by a factor of 3.6 over the best comparable baseline algorithm. Our dataset and code are publicly available. 


summary: Robotic apple harvesting is becoming increasingly important due to labor shortages and rising costs. This paper proposes a novel deep learning-based apple detection framework called DeepApple, which uses a suppression Mask R-CNN to improve accuracy and robustness in challenging orchard environments. DeepApple employs a suppression branch to suppress non-apple features, leading to a higher F1-score of 0.905 and a detection time of 0.25 seconds per frame compared to state-of-the-art methods. The framework was trained on a comprehensive dataset of 'Gala' and 'Blondee' apples collected under varying lighting conditions. 


summary: This paper tackles real-time performance challenges of Human Action Recognition (HAR) on embedded platforms. It addresses these challenges by introducing: 1) an experimental study identifying Optical Flow (OF) extraction as the main latency bottleneck, 2) an analysis of the trade-off between traditional and deep learning approaches to OF extraction, 3) a novel single-shot neural network architecture for motion feature extraction called Integrated Motion Feature Extractor (IMFE), and 4) a real-time HAR system tailored for embedded platforms called RT-HARE. This system achieves a video frame rate of 30 frames per second while maintaining high recognition accuracy on an Nvidia Jetson Xavier NX platform. 


summary: This paper proposes an efficient and layout-independent Automatic License Plate Recognition (ALPR) system based on the YOLO object detector. The system uses a unified approach for license plate (LP) detection and layout classification to improve recognition results through post-processing rules. The system was trained using images from various datasets with data augmentation techniques to achieve robustness under different conditions. The proposed system achieved an average end-to-end recognition rate of 96.8% across eight public datasets, outperforming previous works and commercial systems in some datasets and achieving competitive results in others. The system also demonstrates impressive frame-per-second (FPS) rates on a high-end GPU, enabling real-time performance even with four vehicles in the scene. Additionally, the authors manually labeled 38,351 bounding boxes on 6,239 images from public datasets and made the annotations publicly available. 


summary: Embodied perception requires low latency to ensure responsiveness for autonomous agents. However, standard offline evaluation ignores the latency incurred in real-time applications, where the world changes while an algorithm processes an image. This paper introduces "streaming accuracy" as a metric that jointly evaluates latency and accuracy, considering the amount of data ignored during computation. A meta-benchmark is proposed to convert any image understanding task into a streaming one, focusing on object detection and instance segmentation in urban video streams. Results demonstrate that a "sweet spot" exists for optimal streaming accuracy, asynchronous tracking and future forecasting emerge as key representations, and dynamic scheduling can minimize latency by strategically pausing computation. 


summary: This paper proposes a novel algorithm for intelligent identification of moving objects in natural environments, which is crucial for applications like video surveillance. The algorithm combines Gaussian Mixture Model (GMM), background subtraction, HSV color model, feature extraction, and neural networks to address challenges caused by natural factors like wind, sunlight, and lighting changes. The method accurately detects moving objects, eliminates shadows, and updates the background dynamically. The algorithm utilizes Hu's moment invariants for feature extraction and trains a back propagation neural network (BPNN) to classify objects as human or pets. Experimental results demonstrate the algorithm's robustness and real-time performance in natural environments. 


summary: VisBuddy is a voice-based smart assistant designed for visually challenged individuals. It utilizes image captioning to describe surroundings, optical character recognition (OCR) for reading text, object detection for finding objects, and web scraping for news updates. VisBuddy combines deep learning and the Internet of Things to provide a cost-effective, all-in-one solution for supporting visually challenged individuals in their daily activities. 


summary: Deep neural networks (DNNs) are the state-of-the-art for computer vision problems, but their size and complexity make them difficult to deploy on resource-constrained Internet-of-Things (IoT) devices. This paper surveys recent advancements in low-power and energy-efficient DNN implementations that improve deployability without sacrificing accuracy. These techniques focus on reducing memory requirements and arithmetic operations, and are categorized into neural network compression, network architecture search and design, and compiler and graph optimizations. The paper examines both convolutional and transformer DNNs, highlighting advantages, disadvantages, and open research problems. 


summary: Gun violence is a critical security concern, and the development of effective gun detection algorithms for real-world scenarios, particularly in Closed Circuit Television (CCTV) surveillance data, is crucial. However, detecting guns in CCTV images poses significant challenges due to the small size, nonsalient appearance, and frequent occlusion of firearms. This paper presents a comprehensive benchmark, called CCTV-Gun, designed to address these challenges. The benchmark features carefully selected and annotated CCTV images with challenge factors like blur and occlusion. Additionally, it introduces a new cross-dataset evaluation protocol to assess the generalizing abilities of gun detection algorithms. The paper provides an in-depth analysis of both classical and state-of-the-art object detection algorithms, demonstrating their effectiveness on this challenging task.  The CCTV-Gun benchmark will facilitate further research and development in this area, ultimately contributing to enhanced security. 


summary: We consider the problem of unsupervised camera pose estimation. Given an input video sequence, our goal is to estimate the camera pose (i.e. the camera motion) between consecutive frames. Traditionally, this problem is tackled by placing strict constraints on the transformation vector or by incorporating optical flow through a complex pipeline. We propose an alternative approach that utilizes a compositional re-estimation process for camera pose estimation. Given an input, we first estimate a depth map. Our method then iteratively estimates the camera motion based on the estimated depth map. Our approach significantly improves the predicted camera motion both quantitatively and visually. Furthermore, the re-estimation resolves the problem of out-of-boundaries pixels in a novel and simple way. Another advantage of our approach is that it is adaptable to other camera pose estimation approaches. Experimental analysis on KITTI benchmark dataset demonstrates that our method outperforms existing state-of-the-art approaches in unsupervised camera ego-motion estimation. 


summary: This paper presents a lightweight pipeline for video-based delivery detection that can run on resource-constrained doorbell cameras. The pipeline uses motion cues to generate activity proposals, which are then classified with a mobile-friendly 3DCNN network. The authors designed a novel semi-supervised attention module to help the network learn robust spatio-temporal features and an evidence-based optimization objective to quantify prediction uncertainty. Experimental results on a curated delivery dataset demonstrate the effectiveness of the proposed pipeline compared to alternatives. 


summary: This paper presents a new deep learning approach for camera localization called AtLoc, which utilizes attention mechanisms to focus on geometrically robust objects and features. This method significantly improves the robustness of single-image camera localization, leading to state-of-the-art performance on benchmark datasets. AtLoc effectively rejects dynamic objects and illumination changes, ultimately achieving better global camera pose regression performance compared to existing methods.  The source code for AtLoc is available at the provided Github link. 


summary: This paper proposes a deep learning model that reconstructs dark visual scenes to clear scenes like daylight, and the method recognizes visual actions for the autonomous vehicle. The proposed model achieved 87.3 percent accuracy for scene reconstruction and 89.2 percent in scene understanding and detection tasks. 


summary: Current computer vision (CV) systems typically use an image signal processing (ISP) unit to convert raw sensor data into visually pleasing RGB images. However, this processing can be computationally expensive, especially for low-power devices. This paper explores a new approach to enable ISP-less CV by inverting the ISP pipeline to train models directly on raw sensor data. This method eliminates the need for traditional ISP processing and improves efficiency. The authors release a raw version of the COCO dataset, allowing for training models directly on raw data. Furthermore, they propose an energy-efficient analog in-pixel demosaicing technique to enhance the accuracy of ISP-less CV models. Experimental results demonstrate significant improvements in test accuracy and energy efficiency compared to traditional methods. This approach holds promise for developing low-power, high-performance computer vision systems for resource-constrained devices. 


