summary: This paper argues that the distinction between hardware and software is not applicable to biological systems like the human brain. Instead, the authors propose that the hardware itself embodies the software. This perspective calls for a cautious approach in neuromorphic engineering, emphasizing the importance of respecting and harnessing the underlying physics of non-biological intelligent systems. The authors believe that neuroscience's role in neuromorphic computing should focus on identifying the physics-agnostic principles of biological intelligence, allowing for their adaptation and application to any physical hardware. 


summary: Large language models (LLMs) like ChatGPT and GPT-4 have shown amazing capabilities in AI tasks, but their size and context length make them difficult to serve efficiently on current cloud hardware. To solve this, the authors propose Chiplet Cloud, an ASIC AI supercomputer architecture that uses chiplets to optimize the cost per generated token. This architecture uses thousands of replicated chiplet accelerator modules to collaboratively generate tokens, achieving unprecedented TCO per token by fitting all model parameters within the on-chip SRAMs of the chiplets. This design overcomes bandwidth limitations and reduces the overall cost of deploying and running LLMs. Chiplet Cloud can achieve up to 94x and 15x improvement in TCO/Token compared to A100 GPU and TPUv4 clouds, significantly reducing the cost of realistically serving modern LLMs. 


summary: The paper "The hardware lottery" discusses how the paradigm for hardware, software, and algorithm development is shifting. After decades of encouraging independent development in these areas, there are now catalysts for greater collaboration.  


summary: This paper proposes a novel, special-purpose, and high-efficiency hardware architecture for convolutional neural networks. The proposed architecture maximizes the utilization of multipliers by designing the computational circuit with the same structure as that of the computational flow of the model, rather than mapping computations to fixed hardware. In addition, a specially designed filter circuit simultaneously provides all the data of the receptive field, using only one memory read operation during each clock cycle; this allows the computation circuit to operate seamlessly without idle cycles. Our reference system based on the proposed architecture uses 97% of the peak-multiplication capability in actual computations required by the computation model throughout the computation period. The efficiency of the proposed architecture is close to an ideally efficient system that cannot be improved further in terms of the performance-to-resource ratio. An implementation based on the proposed hardware architecture has been applied in commercial AI products. 


summary: The use of deep learning has grown at an exponential rate, giving rise to numerous specialized hardware and software systems for deep learning. Because the design space of deep learning software stacks and hardware accelerators is diverse and vast, prior work considers software optimizations separately from hardware architectures, effectively reducing the search space. Unfortunately, this bifurcated approach means that many profitable design points are never explored. This paper instead casts the problem as hardware/software co-design, with the goal of automatically identifying desirable points in the joint design space. The key to our solution is a new constrained Bayesian optimization framework that avoids invalid solutions by exploiting the highly constrained features of this design space, which are semicontinuous/semi-discrete. We evaluate our optimization framework by applying it to a variety of neural models, improving the energy-delay product by 18% (ResNet) and 40% (DQN) over hand-tuned state-of-the-art systems, as well as demonstrating strong results on other neural network architectures, such as MLPs and Transformers. 


summary: This paper explores the integration of large language models (LLMs) with advanced hardware. It highlights the need for hardware that is not only powerful but also versatile and capable of managing the sophisticated demands of modern computation. The authors propose a general-purpose device designed for enhanced interaction with LLMs, addressing scalability, multimodal data processing, user interaction, and privacy concerns. 


summary: Keyword spotting (KWS) is a critical user interface for many mobile and edge applications, including phones, wearables, and cars.  As KWS systems are typically 'always on', maximizing both accuracy and power efficiency are central to their utility.  This work uses hardware aware training (HAT) to build new KWS neural networks based on the Legendre Memory Unit (LMU) that achieve state-of-the-art (SotA) accuracy and low parameter counts. This allows the neural network to run efficiently on standard hardware (212 µW).  We also characterize the power requirements of custom designed accelerator hardware that achieves SotA power efficiency of 8.79 µW, beating general purpose low power hardware (a microcontroller) by 24x and special purpose ASICs by 16x. 


summary: Designing hardware accelerators for deep neural networks (DNNs) has been highly sought after. However, most existing accelerators are built for either convolutional neural networks (CNNs) or recurrent neural networks (RNNs). The Transformer model is now replacing the RNN in natural language processing (NLP). However, due to the intensive matrix computations and complicated data flow involved, hardware design for the Transformer model has not been reported. This paper proposes the first hardware accelerator for two key components: the multi-head attention (MHA) ResBlock and the position-wise feed-forward network (FFN) ResBlock, the two most complex layers in the Transformer. The proposed design demonstrates a speed-up of 14.6× in the MHA ResBlock, and 3.4× in the FFN ResBlock, compared with the implementation on GPU with the same setting. This work lays a good foundation for building efficient hardware accelerators for multiple Transformer networks. 


summary: This paper presents a specialized RISC-V instruction set processor designed for edge AI applications. The processor targets the computational needs of large language models (LLMs), specifically focusing on vector dot product calculations. By introducing custom instructions for efficient vector dot product operations, the processor accelerates LLM inference and reduces energy consumption. The design, named Nanhu-vdot, is based on the open-source XiangShan Nanhu architecture and incorporates specialized units and pipeline logic for vector dot product calculations. FPGA testing demonstrates a speed improvement of over four times compared to scalar methods. Furthermore, hardware-software co-design for GPT-2 model inference resulted in a 30% speed increase with minimal additional hardware and power consumption. 


summary: The increasing popularity of Large Language Models (LLMs) has been hindered by their high hardware costs and the need for efficient hardware designs. Evaluating different hardware designs for LLM inference is becoming a bottleneck due to the extensive hardware required. 

LLMCompass, a hardware evaluation framework, addresses this issue by offering a fast, accurate, versatile tool for evaluating different hardware designs. LLMCompass features a mapper to find optimal performance mapping and scheduling, as well as an area-based cost model to guide design decisions. 

Compared to real-world hardware, LLMCompass' estimated latency boasts a 10.4% average error rate across operators with varying input sizes, and a 4.1% average error rate for LLM inference. Simulating a 4-NVIDIA A100 GPU node running GPT-3 175B inference with LLMCompass takes only 16 minutes on commodity hardware, including 26,400 rounds of the mapper's parameter search.

This research utilizes LLMCompass to explore cost-effective hardware designs, resulting in designs that achieve a 3.41x performance/cost improvement over NVIDIA A100 by reducing compute capability or replacing High Bandwidth Memory (HBM) with traditional DRAM. This makes these designs promising for democratizing LLMs. LLMCompass is planned to be fully open-source. 


summary: HW-NAS-Bench is a public dataset for HardWare-aware Neural Architecture Search (HW-NAS) research. It includes hardware performance (e.g., energy cost and latency) of networks from NAS-Bench-201 and FBNet on six hardware devices across different categories. This dataset aims to democratize HW-NAS research by making it more reproducible and accessible to non-hardware experts. The dataset provides insights for HW-NAS research and showcases the benefits of dedicated device-specific HW-NAS for achieving optimal accuracy-cost trade-offs. The codes and data are available at https://github.com/RICE-EIC/HW-NAS-Bench. 


summary: Large language models (LLMs) are incredibly powerful, but deploying them efficiently for various tasks requires careful hardware design.  This research presents GenZ, an analytical tool that examines the link between LLM inference performance and platform design parameters. The analysis provides insights into configuring platforms for different LLM workloads and use cases. GenZ quantifies the platform requirements for supporting advanced LLMs like LLaMA and GPT-4 across different serving settings. The study also projects the hardware capabilities needed for future LLMs with potentially hundreds of trillions of parameters. The trends and insights from GenZ can guide AI engineers deploying LLMs and computer architects designing next-generation hardware. This work highlights the platform design considerations needed to fully utilize the potential of large language models across diverse applications. The source code is available at [link]. 


summary: As artificial intelligence advances, the computational resources needed to support it increase exponentially. Neuromorphic hardware, inspired by the brain's information processing, offers energy-efficient AI computing. However, it has yet to be widely adopted in commercial AI data centers. This paper analyzes the reasons behind this, outlining requirements and guidelines to encourage neuromorphic systems for sustainable cloud computing.  It reviews existing neuromorphic hardware, highlights where it surpasses traditional AI processing on CPUs and GPUs, identifies applications and algorithms commonly used in data centers, and derives requirements for integrating neuromorphic systems into data centers. The aim is to raise awareness about the challenges of incorporating neuromorphic hardware and guide the community towards scalable, energy-efficient AI. 


summary: Attention-based neural networks are widely used in many AI tasks, but their computational and memory demands often hinder their hardware performance. This paper proposes FABNet, a hardware-friendly variant that utilizes a unified butterfly sparsity pattern to approximate both the attention mechanism and feedforward networks. A novel adaptable butterfly accelerator is designed to efficiently execute different butterfly layers using a single hardware engine. FABNet achieves comparable accuracy to vanilla Transformer while reducing computation by 10-66 times and parameters by 2-22 times. The FPGA-based butterfly accelerator delivers 14.2-23.2 times speedup over existing accelerators, demonstrating its efficiency. Compared to optimized CPU and GPU designs on Raspberry Pi 4 and Jetson Nano, the system is up to 273.8 and 15.1 times faster under the same power budget. 


summary: Automatic algorithm-hardware co-design for DNNs has shown promise in improving DNN performance on FPGAs. However, the search space of neural network architectures and hardware accelerator implementation makes this process challenging. Our work, HAO, incorporates integer programming into the search algorithm to prune the design space, unlike existing hardware-aware neural architecture search (NAS) algorithms that solely rely on expensive learning-based approaches. Given hardware resource constraints, our integer programming formulation directly outputs the optimal accelerator configuration for mapping a DNN subgraph that minimizes latency. We use an accuracy predictor for different DNN subgraphs with various quantization schemes and generate accuracy-latency pareto frontiers. Our algorithm achieves state-of-the-art accuracy and hardware performance on Xilinx Zynq (ZU3EG) FPGA for image classification on ImageNet dataset with low computational cost. The solution found by our algorithm achieves 72.5% top-1 accuracy on ImageNet at a framerate of 50, which is 60% faster than MnasNet and 135% faster than FBNet with comparable accuracy. 


summary: This paper introduces HSCoNAS, a novel multi-objective hardware-aware neural architecture search (NAS) framework. HSCoNAS automatically designs deep neural networks (DNNs) with high accuracy but low latency on target hardware. The framework uses a hardware performance modeling method to approximate runtime latency and incorporates dynamic channel scaling and progressive space shrinking to maximize accuracy under latency constraints and refine the search space towards target hardware. HSCoNAS uses an evolutionary algorithm for efficient architecture search and demonstrates superior performance on ImageNet compared to state-of-the-art approaches. 


summary: Realizing today's cloud-level artificial intelligence (AI) functionalities directly on devices distributed at the edge of the internet calls for edge hardware capable of processing multiple modalities of sensory data (e.g. video, audio) at unprecedented energy-efficiency.  Resistive random-access memory (RRAM) based compute-in-memory (CIM) architectures promise to bring orders of magnitude energy-efficiency improvement by performing computation directly within memory, using intrinsic physical properties of RRAM devices. However, conventional approaches to CIM hardware design limit its functional flexibility necessary for processing diverse AI workloads, and must overcome hardware imperfections that degrade inference accuracy.  By co-optimizing across all hierarchies of the design from algorithms and architecture to circuits and devices, we present NeuRRAM, the first multimodal edge AI chip using RRAM CIM to simultaneously deliver a high degree of versatility in reconfiguring a single chip for diverse model architectures, record energy-efficiency 5 -8 better than prior art across various computational bit-precisions, and inference accuracy comparable to software models with 4-bit weights on all measured standard AI benchmarks. This work paves a way towards building highly efficient and reconfigurable edge AI hardware platforms for the more demanding and heterogeneous AI applications of the future. 


summary: The unprecedented performance of deep neural networks (DNNs) has led to large strides in various Artificial Intelligence (AI) inference tasks, such as object and speech recognition. Nevertheless, deploying such AI models across commodity devices faces significant challenges: large computational cost, multiple performance objectives, hardware heterogeneity and a common need for high accuracy, together pose critical problems to the deployment of DNNs across the various embedded and mobile devices in the wild. As such, we have yet to witness the mainstream usage of state-of-the-art deep learning algorithms across consumer devices. In this paper, we provide preliminary answers to this potentially game-changing question by presenting an array of design techniques for efficient AI systems. We start by examining the major roadblocks when targeting both programmable processors and custom accelerators. Then, we present diverse methods for achieving real-time performance following a cross-stack approach. These span model-, system-and hardware-level techniques, and their combination. Our findings provide illustrative examples of AI systems that do not overburden mobile hardware, while also indicating how they can improve inference accuracy. Moreover, we showcase how custom ASIC-and FPGA-based accelerators can be an enabling factor for next-generation AI applications, such as multi-DNN systems. Collectively, these results highlight the critical need for further exploration as to how the various cross-stack solutions can be best combined in order to bring the latest advances in deep learning close to users, in a robust and efficient manner. 


summary: Pushing the boundaries of machine learning often requires exploring different hardware and software combinations. However, the freedom to experiment across different tooling stacks can be at odds with the drive for efficiency, which has produced increasingly specialized AI hardware and incentivized consolidation around a narrow set of ML frameworks. Exploratory research can be restricted if software and hardware are co-evolving, making it even harder to stray away from mainstream ideas that work well with popular tooling stacks. While this friction increasingly impacts the rate of innovation in machine learning, to our knowledge the lack of portability in tooling has not been quantified. In this work, we ask: How portable are popular ML software frameworks? We conduct a large-scale study of the portability of mainstream ML frameworks across different hardware types. Our findings paint an uncomfortable picture -frameworks can lose more than 40% of their key functions when ported to other hardware. Worse, even when functions are portable, the slowdown in their performance can be extreme and render performance untenable. Collectively, our results reveal how costly straying from a narrow set of hardware-software combinations can be -and suggest that specialization of hardware impedes innovation in machine learning research. 


summary: This paper introduces Fire-Flyer AI-HPC, a cost-effective software-hardware co-design framework for deep learning. Fire-Flyer AI-HPC combines the strengths of high-performance computing (HPC) and edge computing to achieve high-throughput inference while maintaining low cost. The framework consists of three key components: 1) a lightweight software stack that enables efficient communication and resource allocation, 2) a specialized hardware accelerator for inference tasks, and 3) an adaptive scheduling algorithm that dynamically adjusts the workload distribution based on real-time system conditions.  Extensive experiments on image classification, object detection, and natural language processing tasks demonstrate that Fire-Flyer AI-HPC significantly outperforms existing deep learning platforms in terms of both inference speed and cost effectiveness. 


summary: Neural architectures and hardware accelerators have been driving forces in deep learning progress. Previous works optimize either hardware for a fixed model or model for fixed hardware, often focusing on FPGAs. This paper targets optimization of hardware and software configurations on an industry-standard edge accelerator, systematically studying co-designing neural architectures and hardware accelerators. Three observations are made: 1) the software search space needs customization for the target hardware, 2) joint search for model and hardware architecture is needed for optimal results, and 3) different use cases result in different search outcomes. Experiments show that joint search outperforms platform-aware NAS, manually crafted models, and EfficientNet on ImageNet top-1 accuracy. Co-adapting model and hardware can reduce energy consumption by up to 2x under the same accuracy. 


summary: 3D integration offers significant benefits for improving system performance and efficiency, particularly in the context of the End-of-Scaling era. This technology enables the integration of heterogeneous system components and disparate technologies, eliminating off-chip communication constraints and reducing on-chip latency and power dissipation.  AIs, with their demand for increased computational power, larger GPU cache capacity, energy efficiency, and low power custom AI hardware integration, further drive the adoption of 3D integration.  While the advantages of 3D integration, such as enhanced interconnectivity and increased performance, have been demonstrated, the design of heterogeneous 3D systems poses significant challenges.  This study highlights the latest drivers for 3D integration and the need for hardware emulation frameworks. It then proposes a design to profile power, temperature, noise, inter-layer bandwidth, and lifetime reliability characterization, enabling the emulation of a wide range of stacking alternatives. This framework facilitates the control of activity levels at the macro-level and incorporates customized sensor infrastructure to characterize heat propagation, inter-layer noise, power delivery, reliability, and interconnectivity, as well as the interactions among critical design objectives. 


summary: DeepHammer is a hardware-based attack that exploits the rowhammer vulnerability to induce bit flips in the weights of quantized deep neural networks (DNNs). This targeted bit manipulation compromises the inference accuracy of the DNN, effectively "depleting its intelligence." DeepHammer uses a novel system-level approach to enable fast deployment of victim pages, memory-efficient rowhammering, and precise flipping of targeted bits. The researchers demonstrate the effectiveness of DeepHammer on 12 DNN architectures using 4 different datasets and application domains, showing that it can degrade the inference accuracy to the level of random guessing within minutes. The paper also discusses mitigation techniques to protect against such attacks. This research highlights the need for incorporating security mechanisms in future deep learning systems to enhance their robustness against hardware-based attacks. 


summary: Experience replay is a crucial component in deep reinforcement learning (DRL) that stores and generates experiences for the agent to learn. Prioritized experience replay (PER) is widely used in DRL agents but incurs significant latency overhead due to its frequent and irregular memory accesses. This paper proposes an associative memory (AM) based PER, AMPER, with an AM-friendly priority sampling operation. AMPER replaces the traditional tree-traversal-based priority sampling in PER while maintaining learning performance. We also design an in-memory computing hardware architecture based on AM to support AMPER. AMPER shows comparable learning performance while achieving 55× to 270× latency improvement compared to the state-of-the-art PER on GPU. 


summary: This paper presents Sunrise, a 3D AI chip with near-memory computing architecture designed to address the challenges of memory bandwidth, energy consumption, and memory capacity in conventional chip architectures. This distributed, near-memory computing architecture enables efficient data flow, leading to improved performance and energy efficiency. The authors project Sunrise to achieve more than ten times the energy efficiency, seven times the performance, and twenty times the memory capacity compared to current state-of-the-art AI chips. 


summary: Designing accurate and efficient convolutional neural architectures for a vast amount of hardware is challenging because hardware designs are complex and diverse. This paper addresses the hardware diversity challenge in Neural Architecture Search (NAS). Unlike previous approaches that apply search algorithms on a small, human-designed search space without considering hardware diversity, we propose HURRICANE that explores the automatic hardware-aware search over a much larger search space and a two-stage search algorithm, to efficiently generate tailored models for different types of hardware. Extensive experiments on ImageNet show that our algorithm consistently achieves a much lower inference latency with a similar or better accuracy than state-of-the-art NAS methods on three types of hardware. Remarkably, HURRICANE achieves a 76.67% top-1 accuracy on ImageNet with a inference latency of only 16.5 ms for DSP, which is a 3.47% higher accuracy and a 6.35X inference speedup than FBNet-iPhoneX, respectively. For VPU, HURRICANE achieves a 0.53% higher top-1 accuracy than Proxylessmobile with a 1.49X speedup. Even for well-studied mobile CPU, HURRICANE achieves a 1.63% higher top-1 accuracy than FBNet-iPhoneX with a comparable inference latency. HURRICANE also reduces the training time by 30.4% or even 54.7% (with less than

summary: This paper proposes a hardware design for the learning datapath of the Tsetlin machine algorithm, along with a latency analysis of the inference datapath. To achieve low energy consumption for applications like pervasive artificial intelligence, the authors employ asynchronous design techniques, including Petri nets, signal transition graphs, dual-rail, and bundled-data. The design builds upon previous inference hardware and includes a detailed breakdown of automaton feedback, probability generation, and Tsetlin automata. The results demonstrate the benefits of asynchronous design in applications where energy is limited and latency is crucial. The paper also addresses challenges related to static timing analysis in asynchronous circuits. 


summary: Recent advances in algorithm-hardware co-design for deep neural networks (DNNs) have demonstrated their potential in automatically designing neural architectures and hardware designs. Nevertheless, it is still a challenging optimization problem due to the expensive training cost and the time-consuming hardware implementation, which makes the exploration on the vast design space of neural architecture and hardware design intractable. In this paper, we demonstrate that our proposed approach is capable of locating designs on the Pareto frontier. This capability is enabled by a novel three-phase co-design framework, with the following new features: (a) decoupling DNN training from the design space exploration of hardware architecture and neural architecture, (b) providing a hardware-friendly neural architecture space by considering hardware characteristics in constructing the search cells, (c) adopting Gaussian process to predict accuracy, latency and power consumption to avoid time-consuming synthesis and place-and-route processes. In comparison with the manually-designed ResNet101, InceptionV2 and MobileNetV2, we can achieve up to 5% higher accuracy with up to 3× speed up on the ImageNet dataset. Compared with other state-of-the-art co-design frameworks, our found network and hardware configuration can achieve 2% ∼ 6% higher accuracy, 2×∼ 26× smaller latency and 8.5× higher energy efficiency. 


summary: This paper explores the problem of designing a single neural network that performs well on multiple hardware platforms. The authors propose a multi-hardware search space that is compatible with various hardware devices and introduce new metrics to evaluate the overall latency performance of models across these devices. The results demonstrate that models discovered via multi-hardware search perform on par or better than state-of-the-art models on each of the target accelerators and generalize well to other hardware platforms. Comparing to single-hardware searches, multi-hardware search provides a better trade-off between computation cost and model performance. 


summary: This paper proposes a framework for automatically generating hardware cores for Artificial Neural Network (ANN)-based chaotic oscillators. The framework trains an ANN to approximate a chaotic system, explores design space options for hardware implementation, and generates synthesizable code and testbenches. This process is designed to be faster and more efficient than manual hardware design, leading to lower cost and higher throughput. The framework primarily targets FPGAs. The source code is available on GitHub. 


summary: Deep neural networks (DNNs) have driven progress in speech recognition, but their size and complexity have led to high energy consumption. Here, we demonstrate an analog-AI chip for energy-efficient speech recognition and transcription, using a 14-nm CMOS technology that combines pulse-duration modulation, in-memory computing and a parallel 2D mesh network to enable low-latency and energy-efficient matrix multiplications. We demonstrate the chip’s efficacy by deploying a state-of-the-art recurrent neural-network transducer (RNNT) model on the chip and achieving a word error rate (WER) of 5.4% on the Librispeech clean dataset—on par with the WER of the model running on a digital processor. This work highlights the significant potential for analog computing in applications requiring high-performance AI tasks. 


summary: Neuromorphic spiking neural networks (SNNs) offer a low-power alternative to traditional deep neural networks (DNNs) for computer vision (CV) applications. However, existing SNNs require multiple time steps for inference, increasing energy consumption. This paper proposes an in-sensor computing framework for SNNs targeting image recognition tasks, aiming to reduce the bandwidth between sensing and processing. The approach utilizes direct encoding, customizes the pixel array and periphery to implement analog convolution, and employs knowledge distillation for bandwidth reduction. The proposed framework achieves a 12-96× reduction in bandwidth and a 2.32× reduction in total energy compared to traditional CV processing, with a 3.8% decrease in accuracy on ImageNet. 


summary: Neural Radiance Field (NeRF) is a promising alternative to traditional rendering methods, but its computational demands make it impractical for resource-constrained mobile devices like VR/AR.  CICERO, a new approach, addresses both algorithmic and architectural inefficiencies in NeRF. It introduces two algorithms: one to reduce the workload of any NeRF model and another to eliminate irregular DRAM accesses. CICERO also employs an on-chip data layout strategy to avoid SRAM bank conflicts.  On a mobile Volta GPU, CICERO achieves an 8x speed-up and 7.9x energy saving compared to a baseline implementation.  When paired with a dedicated DNN accelerator, the speed-up and energy reduction increase to 28.2x and 37.8x, respectively, with minimal quality degradation. 


summary: Hardware, systems and algorithms research communities have historically had different incentive structures and fluctuating motivation to engage with each other explicitly. This historical treatment is odd given that hardware and software have frequently determined which research ideas succeed (and fail). This essay introduces the term hardware lottery to describe when a research idea wins because it is suited to the available software and hard�ware and not because the idea is superior to alternative research directions. Examples from early computer science history illustrate how hardware lot�teries can delay research progress by casting successful ideas as failures. These lessons are particularly salient given the advent of domain special�ized hardware which make it increasingly costly to stray off of the beaten path of research ideas. This essay posits that the gains from progress in computing are likely to become even more uneven, with certain research directions moving into the fast-lane while progress on others is further ob�structed. 


summary: This paper introduces a new hardware-accelerated platform for real-time multidimensional video understanding. The platform combines artificial intelligence hardware with state-of-the-art machine vision networks, achieving a data processing speed of 1.2 Tb/s with hundreds of frequency bands and megapixel spatial resolution at video rates. This surpasses the speed of existing technologies by three to four orders of magnitude. The platform's performance was validated in video semantic segmentation and object understanding tasks, demonstrating its potential for real-time AI video understanding of multidimensional visual information. 


summary: This paper provides a comprehensive survey and comparison of hardware acceleration techniques for Large Language Models (LLMs). The paper covers key topics such as FPGAs, ASICs, and other specialized hardware for LLM acceleration. The authors also discuss the design considerations, trade-offs, and performance characteristics of different acceleration architectures. 


summary: As deep learning models scale, they become increasingly efficient but require more memory and computing power. Neuromorphic computing aims to improve efficiency by mimicking brain operations, such as spike-based information processing. This paper compares digital hardware acceleration techniques for ANNs and SNNs, finding that: (i) ANNs process static data more efficiently, (ii) SNNs are more suited for data from neuromorphic sensors, (iii) online and on-chip training for SNNs needs further research, and (iv) hybrid approaches combining ANNs and SNNs should be investigated. 


summary: This paper presents a new technique for searching for hardware architectures of accelerators optimized for end-to-end training of deep neural networks (DNNs). The approach addresses both single-device and distributed pipeline and tensor model parallel scenarios, optimizing accelerators for training metrics like throughput/TDP under area and power constraints. Unlike previous work, which focused on inference or specific layers, this method uses a heuristic-based critical path approach to determine the best use of resources for DNN workloads. It performs local search to find the architecture for each pipeline and tensor model stage, prioritizing critical operators. For distributed training, multiple designs per stage are selected and a global search identifies the optimal accelerator for training throughput across the stages. Evaluation on 11 DNN models shows that this method converges faster and achieves higher throughput than a recent inference-only approach, as well as a 12% throughput improvement over TPU architecture. 


summary: The scaling hypothesis suggests that increasing model size, even beyond trillions of parameters, can lead to better performance. However, training such large models efficiently with backpropagation becomes difficult due to communication bottlenecks. This paper argues for alternative training methods, such as Direct Feedback Alignment, which drastically reduce communication needs by enabling a parallelizable backward pass.  The authors present a photonic accelerator for Direct Feedback Alignment, capable of handling trillions of parameters, and demonstrate its effectiveness on benchmark tasks. This hardware, the first architecture-agnostic photonic co-processor for training neural networks, represents a significant step towards building scalable hardware that goes beyond backpropagation, opening new avenues for deep learning. 


summary: Recent advances in Neural Architecture Search (NAS) have led to the development of hardware-aware configurations, called "sub-networks", extracted from a hardware-agnostic "super-network".  This paper introduces a comprehensive system for efficiently finding optimized sub-networks from a pre-trained super-network, tailored to different performance metrics and hardware configurations. The proposed system leverages novel search tactics and algorithms along with intelligent predictors to significantly reduce the time needed to find optimal sub-networks. The system works seamlessly with existing state-of-the-art super-network training methods across various domains. Experiments demonstrate that this system can accelerate the search process for ResNet50, MobileNetV3, and Transformer, achieving an 8x faster search result compared to the WeakNAS approach. 


summary: Vector search is a fundamental technology used in large-scale information retrieval and machine learning systems. As the demand for performance in these systems grows, specialized hardware offers a promising solution. This paper introduces FANNS, a scalable vector search framework designed for FPGAs. FANNS automatically co-designs hardware and algorithm, generating an accelerator that can achieve high performance with minimal resource usage. The framework also supports scale-out by incorporating a hardware TCP/IP stack into the accelerator. FANNS demonstrates significant performance gains compared to FPGA and CPU baselines, and exhibits superior scalability compared to GPUs. 


summary: This paper explores the potential of Ising machines as a substrate for energy-based machine learning, particularly for training and inference using a Restricted Boltzmann Machine (RBM). The authors demonstrate that a slightly modified Ising machine can accelerate key parts of the algorithm, leading to significant speedup and efficiency gains. Moreover, a more substantial modification enables the machine to act as a self-sufficient gradient follower, enabling virtually complete training in hardware. This approach promises a 29x speedup and 1000x energy reduction compared to a TPU host. 


summary: Neural architecture search (NAS) should be hardware-aware to satisfy device-specific constraints (e.g., memory usage, latency, and energy consumption) for deployment. Existing hardware-aware NAS methods require a large number of samples from a target device to build a latency estimator, which is impractical for real-world scenarios with numerous devices. To address this, the authors propose HELP, a Hardware-adaptive Efficient Latency Predictor that formulates device-specific latency estimation as a meta-learning problem. HELP uses hardware embeddings to represent devices and meta-learns a hardware-adaptive latency predictor in a device-dependent manner. HELP achieves high estimation performance with only 10 measurement samples and significantly reduces the total time cost of the base NAS method in latency-constrained settings. 


summary: The widespread use of Deep Learning (DL) applications in science and industry has created a large demand for efficient inference systems. This has resulted in a rapid increase of available Hardware Accelerators (HWAs) making comparison challenging and laborious. To address this, several DL hardware benchmarks have been proposed aiming at a comprehensive comparison for many models, tasks, and hardware platforms. Here, we present our DL hardware benchmark which has been specifically developed for inference on embedded HWAs and tasks required for autonomous driving. In addition to previous benchmarks, we propose a new granularity level to evaluate common submodules of DL models, a twofold benchmark procedure that accounts for hardware and model optimizations done by HWA manufacturers, and an extended set of performance indicators that can help to identify a mismatch between a HWA and the DL models used in our benchmark. 


summary: This paper proposes an efficient FPGA-based hardware accelerator for the Swin Transformer, a neural network model achieving remarkable results in computer vision. The authors addressed the challenges of non-linear computations in Transformers by replacing Layer Normalization with Batch Normalization, employing approximate computation strategies for Softmax and GELU, and designing an efficient Matrix Multiplication Unit for linear computations. The accelerator achieved significant speedup and energy efficiency improvements compared to CPU and GPU implementations. Notably, the accelerator is reported to be the fastest FPGA-based accelerator for Swin Transformer to date. 


summary: This paper examines the performance of deep learning models on GPUs, emphasizing the importance of co-designing models with the target hardware. By optimizing model shapes through careful selection of hyperparameters, the authors achieve significant performance gains (up to 39% higher throughput) without sacrificing accuracy, compared to models with similar parameter counts but unoptimized shapes. The paper provides a set of guidelines for maximizing transformer model runtime performance on GPUs. 


summary: Artificial intelligence (AI) is playing an increasingly significant role in our everyday lives, especially with recent pushes to move more AI to the edge. One challenge with AI on edge devices (mobile phones, unmanned vehicles, sensors, etc.) is their associated size, weight, and power constraints. We consider the scenario where an AI system may need to operate at less-than-maximum accuracy to meet application-dependent energy requirements. We propose a simple function that divides the cost of using an AI system into the cost of the decision-making process and the cost of decision execution. For simple binary decision problems with convolutional neural networks, it is shown that minimizing the cost corresponds to using fewer than the maximum number of resources (e.g., convolutional neural network layers and filters). Finally, it is shown that the cost associated with energy can be significantly reduced by leveraging high-confidence predictions made in lower-level layers of the network. 


summary: DRAGON is an open-source, fast, and explainable hardware simulation and optimization toolchain that allows hardware architects to simulate and optimize hardware designs for efficient workload execution. It consists of three tools: Hardware Model Generator (DGen), Hardware Simulator (DSim), and Hardware Optimizer (DOpt). DSim simulates the execution of algorithms (represented as data-flow graphs) on hardware described by DGen, which provides a detailed hardware description with user-defined architecture and technology. DOpt uses gradient descent from the simulation to optimize the hardware model, providing directions for improving technology and design parameters. DRAGON's performance is significantly faster than previous works due to optimized code, mathematical formulas for common operations, efficient mapping algorithms, and optimized data structures for hardware state representation. It generates performance-optimized architectures for both AI and non-AI workloads, aiming for 100x-1000x improvement in computing systems. 


summary: In recent years, Deep Learning has achieved significant results in various practical problems like computer vision, natural language processing, and speech recognition. While research focused on improving model quality, practical applications often require real-time performance, making model latency crucial. Existing Neural Architecture Search (NAS) methods consider model complexity but struggle to design search spaces tailored to specific hardware. This paper proposes a new measure - matrix efficiency measure (MEM) - to assess the hardware efficiency of NAS search spaces. The authors also present a search space comprised of hardware-efficient operations, a latency-aware scaling method, and ISyNet, a set of architectures optimized for speed and accuracy on specialized neural processing units (NPUs). The study demonstrates the superiority of these architectures on ImageNet and their generalization capability for downstream classification and detection tasks. The research also highlights the NPU-efficient search space design with high MEM values. 


summary: Deep neural networks have become the standard approach for building reliable Natural Language Processing (NLP) applications.  However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time.  To address this issue, we propose a novel vector-vector-matrix architecture (VVMA) that takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations.  Our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four.  Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use. 


summary: Deep neural networks (DNNs) are vulnerable to adversarial attacks, where inputs are manipulated to induce misclassification. Existing defenses are mostly software-based, leading to high overheads. This paper presents HASI, a hardware-accelerated defense that uses stochastic inference to detect adversarial inputs. HASI injects noise into the model at inference time to differentiate adversarial inputs from benign ones. It achieves an average 87% detection rate, surpassing the state-of-the-art with significantly lower overhead. The software/hardware co-design reduces performance impact to 1.58×−2× compared to 14 × −20× overhead for software-only GPU implementations. 


summary: Deep learning and its hardware have garnered significant interest, leading to the emergence of numerous startups and investments. However, NVIDIA's TensorCore-based systems remain dominant, offering high performance, a comprehensive software stack, and support for diverse deep learning network architectures. This paper investigates the limitations of conventional hardware architectures for deep learning and presents a novel approach called UPCYCLE. UPCYCLE leverages SIMD/shortvector, caching, and synchronization within a multicore chip organization. The authors demonstrate that UPCYCLE achieves day-zero software maturity and outperforms NVIDIA's A100 in both inference and training, consuming less power. Notably, UPCYCLE requires no new compiler or software stack innovations, offering full deep learning architecture coverage and adaptability for different workloads. The paper emphasizes the importance of considering software maturity as a critical design constraint when developing new architectures for deep learning. 


summary: This paper proposes a novel optical subspace neural network (OSNN) architecture for deep learning, which utilizes a compact butterfly-style photonic-electronic neural chip. Compared to conventional GEMM-based ONNs, the OSNN architecture achieves a 7x reduction in trainable optical components, leading to lower area cost and energy consumption. The chip features a hardware-aware training framework that minimizes device programming precision and enhances noise robustness. Experimental results demonstrate the effectiveness of the neural chip in image recognition tasks, achieving a 94.16% accuracy in handwritten digit recognition with 3-bit weight programming precision. 


summary: This paper introduces Restructurable Activation Networks (RANs), a new paradigm for deep network design that optimizes for hardware efficiency by manipulating the amount of non-linearity in the model. RANs use a hardware-aware search space and semi-automatic search algorithm to replace inefficient blocks with hardware-aware blocks. They also offer a training-free model scaling method that links network topology to expressivity. RANs achieve state-of-the-art results on ImageNet at different scales for various hardware, outperforming existing methods in terms of accuracy and Frames-Per-Second (FPS). For instance, RAN-e achieves similar accuracy to EfficientNet-Lite-B0 while improving FPS by 1.5x on Arm micro-NPUs, while RAN-i achieves up to 2x reduction in #MACs over ConvNexts with similar or better accuracy. 


summary: This paper investigates the intersection of deep learning and neuromorphic hardware architectures, focusing on biologically plausible learning algorithms that are tolerant to hardware nonidealities.  The authors explore the impact of hardware imperfections and quantization on algorithm performance, as well as how network topologies and algorithm design choices can scale latency, energy, and area consumption. They compare the performance of traditional Backpropagation and Direct Feedback Alignment algorithms on Compute-In-Memory (CIM) hardware, demonstrating that Direct Feedback Alignment achieves significant speedups while Backpropagation delivers the highest accuracy. This work offers valuable insights into the design of efficient and robust learning systems for neuromorphic hardware. 


summary: Approximate computing methods have shown great potential for deep learning due to their reduced hardware costs, making them suitable for inference tasks on power-constrained devices. However, the lack of training methods has limited their full potential. This paper discusses training methods for approximate hardware, demonstrating the need for specialized training and proposing methods to speed up training by up to 18X. The authors address the limitations of existing approaches and propose techniques that combine error injection with accurate modeling to maintain model accuracy while reducing training time. 


summary: Optical computing has emerged as a potential solution to the escalating demands of AI/ML workloads. However, existing implementations face challenges in scalability, footprint, power consumption, and integration within existing datacenter architectures. This paper proposes a scalable optical AI accelerator utilizing a coherent photonic crossbar architecture with phase change material (PCM) for on-chip weight storage. The design incorporates all critical circuit blocks, modeling based on measured performance in a 45nm monolithic silicon photonic process. A system-level analysis for Resnet-50V1.5 is presented, accounting for memory, array size, photonic losses, and peripheral energy consumption.  The proposed 128×128 architecture achieves comparable inference per second (IPS) to Nvidia A100 GPU with significantly lower power and area consumption. 


summary: A "technology lottery" describes a research idea or technology succeeding over others because it is suited to the available software and hardware, not necessarily because it is superior to alternative directions. The nascent field of Self-Driving Laboratories (SDL), particularly those implemented as Materials Acceleration Platforms (MAPs), is at risk of an analogous pitfall: the next logical step for building MAPs is to take existing lab equipment and workflows and mix in some AI and automation. This whitepaper argues that the same simulation and AI tools that will accelerate the search for new materials, as part of the MAPs research program, also make possible the design of fundamentally new computing mediums. We need not be constrained by existing biases in science, mechatronics, and general-purpose computing, but rather we can pursue new vectors of engineering physics with advances in cyber-physical learning and closed-loop, self-optimizing systems. This paper outlines a simulation-based MAP program to design computers that use physics itself to solve optimization problems. Such systems mitigate the hardware-software-substrate-user information losses present in every other class of MAPs and they perfect alignment between computing problems and computing mediums eliminating any technology lottery. This paper offers concrete steps toward early Physical Computing (PC) -MAP advances and the longer term cyber-physical R&D, which is expected to introduce a new era of innovative collaboration between materials researchers and computer scientists. 


summary: This webpage does not include an abstract.  The content is primarily a website header and a reference to the Simons Foundation and member institutions. 


summary: Deep Q-Network (DQN) marked a major milestone for reinforcement learning, demonstrating for the first time that human-level control policies could be learned directly from raw visual inputs via reward maximization. Even years after its introduction, DQN remains highly relevant to the research community since many of its innovations have been adopted by successor methods. Nevertheless, despite significant hardware advances in the interim, DQN's original Atari 2600 experiments remain costly to replicate in full. This poses an immense barrier to researchers who cannot afford state-of-the-art hardware or lack access to large-scale cloud computing resources. To facilitate improved access to deep reinforcement learning research, we introduce a DQN implementation that leverages a novel concurrent and synchronized execution framework designed to maximally utilize a heterogeneous CPU-GPU desktop system. With just one NVIDIA GeForce GTX 1080 GPU, our implementation reduces the training time of a 200-million-frame Atari experiment from 25 hours to just 9 hours. The ideas introduced in our paper should be generalizable to a large number of off-policy deep reinforcement learning methods. 


summary: Egocentric, multi-modal data as available on future augmented reality (AR) devices provides unique challenges and opportunities for machine perception. These future devices will need to be all-day wearable in a socially acceptable form-factor to support always available, context-aware and personalized AI applications. Our team at Meta Reality Labs Research built the Project Aria device, an egocentric, multi-modal data recording and streaming device with the goal to foster and accelerate research in this area. In this paper, we describe the Project Aria device hardware including its sensor configuration and the corresponding software tools that enable recording and processing of such data. 


summary: Accelerating deep model training and inference is crucial in practice. Existing deep learning frameworks usually concentrate on optimizing training speed and pay fewer attentions to inference-specific optimizations. We propose a hardware-aware optimization framework, namely Woodpecker-DL (WPK), to accelerate inference by taking advantage of multiple joint optimizations from the perspectives of graph optimization, automated searches, domain-specific language (DSL) compiler techniques and system-level exploration. In WPK, we investigated two new automated search approaches based on genetic algorithm and reinforcement learning, respectively, to hunt the best operator code configurations targeting specific hardware. A customized DSL compiler is further attached to these search algorithms to generate efficient codes. To create an optimized inference plan, WPK systematically explores high-speed operator implementations from third-party libraries besides our automatically generated codes and singles out the best implementation per operator for use. Extensive experiments demonstrated that on a Tesla P100 GPU, we can achieve the maximum speedup of 5.40 over cuDNN and 1.63 over TVM on individual convolution operators, and run up to 1.18 times faster than TensorRT for end-to-end model inference. * Equal contributions. 


summary: This paper introduces a new type of continuous-variable thermodynamic computer called the stochastic processing unit (SPU), which is built using RLC circuits on a printed circuit board. The SPU can be used for both sampling and linear algebra tasks, and the paper demonstrates its use in Gaussian sampling and matrix inversion, the latter being the first thermodynamic linear algebra experiment. The paper also explores the potential of the SPU for uncertainty quantification in neural network classification. The authors believe that scaling up the SPU could significantly accelerate various probabilistic AI applications. 


summary: We propose SemifreddoNets, a system of fixed-topology neural networks with partially frozen weights, optimized for efficient hardware implementation. SemifreddoNets freeze some parameters at each layer, replacing multipliers with fixed scalers, reducing silicon area, logic delay, and memory requirements. This tradeoff between cost and flexibility allows for configurable weights at different scales and levels of abstraction. Although fixing the topology and some weights limits flexibility, the efficiency benefits outweigh the advantages of a fully configurable model for many use cases. SemifreddoNets use repeatable blocks, enabling adjustable model complexity without hardware changes. The hardware implementation achieves up to an order of magnitude reduction in silicon area and power consumption compared to general-purpose accelerator implementations. 


summary: This webpage does not contain the abstract of the paper "A Hardware–Software Blueprint for Flexible Deep Learning Specialization." The webpage is a standard IEEE Xplore page with links to user accounts, purchase details, profile information, and help resources. 


summary: Designing efficient hybrid networks for diverse edge devices is challenging due to varying hardware designs, supported operations, and compilation optimizations. Current NAS methods often rely on fixed architecture search spaces and hardware-agnostic latency proxies, leading to inaccurate latency predictions. SCAN-Edge addresses these limitations with a unified NAS framework that searches for optimal combinations of self-attention, convolution, and activation, accommodating various edge devices.  SCAN-Edge uses a hardware-aware evolutionary algorithm to accelerate the search process. Experiments show that the resulting hybrid networks achieve the latency of MobileNetV2 for 224x224 input on different edge devices. 


summary: This paper proposes a specialized hardware architecture for accelerating recommender systems. By analyzing Facebook's Deep Learning Recommendation Model (DLRM), the paper quantifies the impact of hardware parameters, such as memory system design, collective communication latency, and interconnect topology, on system throughput. The authors argue that this "scale-in" approach can significantly boost recommender system throughput for both inference and training compared to existing AI platforms. They also identify limitations of current AI accelerators and hardware platforms based on their analysis of the DLRM workload. 


summary: This paper proposes a memristive neuromorphic hardware implementation for the actor-critic algorithm in reinforcement learning (RL).  A two-fold training procedure is introduced, including ex-situ pre-training and in-situ re-training, which significantly reduces the number of weight updates, making it suitable for efficient in-situ learning implementations. The authors demonstrate the effectiveness of their approach in a case study involving balancing an inverted pendulum. This study highlights the potential of memristor-based hardware neural networks for handling complex tasks through in-situ reinforcement learning. 


summary: Research on efficient vision backbones is moving towards models that combine convolutions and transformer blocks. This paper focuses on hardware efficiency, analyzing common modules and architectural choices for backbones in terms of throughput and latency, instead of the traditional MACs metric. The analysis leads to a recipe for hardware-efficient macro design and a new, slimmed-down version of MultiHead Self-Attention. Combining these designs, the authors propose a new family of hardware-efficient backbone networks called LowFormer. LowFormer shows significant speedup in throughput and latency while maintaining or improving accuracy compared to existing efficient backbones. The paper evaluates LowFormer on various hardware platforms (GPU, mobile GPU, ARM CPU) and demonstrates its benefits for downstream tasks like object detection and semantic segmentation. Code and models are available online. 


summary: ## Abstract:

This paper presents a method for training keyword spotting models that are optimized for both general-purpose and specialized hardware.  The authors use a hardware-aware training approach to improve efficiency and reduce latency, making keyword spotting more suitable for real-time applications. The research specifically addresses the challenges of deploying keyword spotting on resource-constrained devices like mobile phones and embedded systems. 


summary: With the increasing computational demands of neural networks, many hardware accelerators for the neural networks have been proposed. Such existing neural network accelerators often focus on popular neural network types such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs); however, not much attention has been paid to attention mechanisms, an emerging neural network primitive that enables neural networks to retrieve most relevant information from a knowledge-base, external memory, or past states. The attention mechanism is widely adopted by many state-of-the-art neural networks for computer vision, natural language processing, and machine translation, and accounts for a large portion of total execution time. We observe today's practice of implementing this mechanism using matrix-vector multiplication is suboptimal as the attention mechanism is semantically a content-based search where a large portion of computations ends up not being used. Based on this observation, we design and architect A 3 , which accelerates attention mechanisms in neural networks with algorithmic approximation and hardware specialization. Our proposed accelerator achieves multiple orders of magnitude improvement in energy efficiency (performance/watt) as well as substantial speedup over the state-of-the-art conventional hardware. 


summary: The use of deep learning has grown at an exponential rate, leading to numerous specialized hardware and software systems. Prior work typically considers software optimizations and hardware architectures separately, limiting the exploration of the design space. This paper presents a hardware/software co-design approach using constrained Bayesian optimization to automatically identify desirable design points. The optimization framework leverages the constrained features of the design space, achieving energy-delay product improvements of up to 40% compared to hand-tuned state-of-the-art systems for various neural models. 


summary: Convolutional neural networks (CNN) are widely used in machine intelligence tasks, but are computationally intensive and energy consuming.  AdderNet, a novel minimalist hardware architecture, replaces convolution with adder kernels using only additions, significantly reducing complexity and energy consumption.  To further optimize energy efficiency, the paper explores low-bit quantization for AdderNet with a shared-scaling-factor method, and designs specific and general-purpose hardware accelerators.  Experimental results demonstrate high performance with int8/int16 quantization, leading to a significant reduction in resources.  Deployment on an FPGA platform shows notable improvements in speed, logic resource utilization, and power consumption compared to traditional CNNs.  AdderNet surpasses other competitors in performance, power consumption, hardware resource consumption, and network generalization capability, demonstrating its potential for high-performance, energy-efficient AI applications. 


summary: This paper proposes a hardware design for Bayesian deep networks, exploiting cycle-to-cycle variability in oxide-based Resistive Random Access Memories (RRAMs) for probabilistic sampling. Instead of viewing variability as a disadvantage, the authors leverage it to realize a probabilistic sampling function, reducing the need for costly CMOS Gaussian random number generators. The design integrates "In-Memory" crossbar architectures for energy-efficient hardware primitives, enabling a new paradigm for probabilistic Artificial Intelligence. While previous work has explored RRAM stochastic switching processes for neuromorphic algorithms, this paper specifically focuses on utilizing probability distributions derived from RRAM variability for deep learning applications, paving the way for more efficient and cost-effective probabilistic AI hardware. 


summary: This document is a metadata record for a research paper on Semantic Scholar. It provides the paper's title, authors, publication date, and citation count. However, it does not contain the paper's abstract. To access the abstract, you will need to find the full paper. 


summary: Multiplexed gradient descent (MGD) is a gradient descent framework designed to train analog or digital neural networks in hardware. MGD utilizes zero-order optimization techniques for online training of hardware neural networks. The authors demonstrate that MGD can train neural networks on modern machine learning datasets, including CIFAR-10 and Fashion-MNIST, and compare its performance to backpropagation. They conclude that these optimization techniques can train a network on emerging hardware platforms orders of magnitude faster than the wall-clock time of training via backpropagation on a standard GPU, even in the presence of imperfect weight updates or device-to-device variations in the hardware. They also discuss how MGD can be applied to existing hardware as part of chip-in-the-loop training, or integrated directly at the hardware level. 


summary: This paper explores how hardware selection, often overlooked in machine learning, can significantly impact fairness and model performance, especially in ML-as-a-service platforms where users lack control over hardware. The authors demonstrate that different hardware choices can worsen existing performance disparities across demographic groups due to variations in gradient flows and loss surfaces. The paper provides theoretical and empirical analyses to identify the underlying factors and proposes a strategy to mitigate hardware-induced performance imbalances. 


summary: Deep learning recommendation models (DLRMs) are used by Meta and are the single largest AI application in terms of infrastructure demand in its data-centers. This paper presents Neo, a software-hardware co-designed system for high-performance distributed training of large-scale DLRMs. Neo uses a novel 4D parallelism strategy that combines table-wise, row-wise, column-wise, and data parallelism for training massive embedding operators in DLRMs. Neo also enables extremely high-performance and memory-efficient embedding computations using hybrid kernel fusion, software-managed caching, and quality-preserving compression. Finally, Neo is paired with ZionEX, a new hardware platform co-designed with Neo's 4D parallelism for optimizing communications for large-scale DLRM training. Evaluation on 128 GPUs using 16 ZionEX nodes shows that Neo outperforms existing systems by up to 40× for training 12-trillion-parameter DLRM models deployed in production. 


summary: The number of parameters in deep neural networks (DNNs) is scaling at about 5× the rate of Moore's Law. To sustain this growth, photonic computing is a promising avenue, as it enables higher throughput in dominant general matrix-matrix multiplication (GEMM) operations in DNNs than their electrical counterpart. However, purely photonic systems face several challenges including lack of photonic memory and accumulation of noise. In this paper, we present an electrophotonic accelerator, ADEPT, which leverages a photonic computing unit for performing GEMM operations, a vectorized digital electronic ASIC for performing non-GEMM operations, and SRAM arrays for storing DNN parameters and activations. In contrast to prior works in photonic DNN accelerators, we adopt a system-level perspective and show that the gains while large are tempered relative to prior expectations. Our goal is to encourage architects to explore photonic technology in a more pragmatic way considering the system as a whole to understand its general applicability in accelerating today's DNNs. Our evaluation shows that ADEPT can provide, on average, 5.73× higher throughput per Watt compared to the traditional systolic arrays (SAs) in a full-system, and at least 6.8× and 2.5× better throughput per Watt, compared to state-of-the-art electronic and photonic accelerators, respectively. 


summary: Nonlinear model predictive control (NMPC) is a powerful control method, but it's computationally expensive. This paper presents hardware FPGA neural network controllers trained to mimic NMPC using supervised learning. These Neural Controllers (NCs) are implemented on low-cost embedded FPGA hardware, enabling high-frequency control of physical systems like a cartpole and an F1TENTH race car. The NCs achieve performance matching NMPC in simulation and outperform it in real-world scenarios due to faster control rates enabled by the quick FPGA NC inference. This research demonstrates kHz control rates for a physical cartpole and offloads control to the FPGA hardware on the F1TENTH car.  The code and hardware implementation are available at [link to GitHub repository]. 


summary: Computing platforms in autonomous vehicles rely heavily on fast, accurate, and reliable decision-making, requiring specialized hardware accelerators to meet the demands of perception and machine vision. This paper examines ML accelerators for autonomous driving, highlighting their architecture styles and application in machine vision use cases.  The authors discuss implications for AV development, offer recommendations for researchers and practitioners, and outline a trajectory for future research in this emerging field. 


summary: Analog neural network (NN) accelerators promise significant energy and time savings, but a major challenge is their susceptibility to static fabrication errors.  Current training methods for programmable photonic interferometer circuits, a leading analog NN platform, fail to produce networks that perform well in the presence of such errors. Existing hardware error correction techniques require individual retraining of each NN, place high demands on component quality, or introduce hardware overhead. This paper introduces a novel one-time error-aware training technique that addresses all three problems by producing robust NNs that match the performance of ideal hardware and can be directly transferred to any faulty photonic NN with hardware errors up to five times larger than current fabrication tolerances. This approach eliminates the need for individual training, reduces component quality requirements, and avoids adding hardware overhead, making it practical for large-scale deployment of analog NNs in edge settings. 


summary: Convolutional neural networks (CNNs) are typically trained using 16- or 32-bit floating-point (FP). Lower precision is often sufficient for inference, most commonly 8-bit integer values. However, recent research has shown that low-precision FP can be highly effective for inference. Low-precision FP can be implemented in hardware FPGA and ASIC accelerators, but existing processors do not, in general, support custom precision FP. We propose hardware optimized bit-sliced floating-point operators (HOBFLOPS), a method of generating efficient custom-precision emulated bitsliced software FP arithmetic. We generate custom-precision software FP routines using a hardware design flow. An FP unit generator creates high-level FP arithmetic hardware descriptions, and we use a standard hardware design flow to optimize and synthesize these designs into circuits. We provide standard cell libraries that match the bitwise operations on the target microprocessor architecture, and a custom code-generator to translate the resulting circuits to bitslice software equivalents. We exploit bitslice parallelism to create a very wide (32-512 element) vectorized CNN convolution. Experiments show that HOBFLOPS provides a fast approach to emulating custom, low-precision FP in software. We demonstrate implementing various widths of HOBFLOPS multiplier and adder in the multiply-accumulate (MAC) of a CNN convolution. The HOBFLOPS optimized C/C++ MAC performance of the convolution on Arm Neon, Intel AVX2, and AVX5

summary: This paper proposes a methodology to evaluate and compare the performance of efficient neural network building blocks for computer vision in a hardware-aware manner.  The comparison utilizes Pareto fronts based on randomly sampled networks from a design space to capture the accuracy/complexity trade-offs. This approach provides more insights into the relationship between hardware cost and accuracy compared to previous comparison methods.  The methodology is applied to analyze different building blocks and evaluate their performance on various embedded hardware platforms, highlighting the importance of benchmarking building blocks as a preselection step in neural network design. This approach demonstrates the potential for speeding up inference by up to a factor of 2× on specific hardware ML accelerators through the selection of appropriate building blocks. 


summary: Graph neural networks (GNNs) have shown impressive performance in handling non-Euclidean data. However, current GNN designs primarily focus on accuracy, neglecting resource constraints and real-time requirements of edge applications. This paper introduces HGNAS, a Hardware-aware Graph Neural Architecture Search framework designed for resource-limited edge devices. HGNAS leverages a multi-stage search strategy to explore optimal architectures within a few GPU hours, while achieving hardware awareness through a hardware performance predictor. This predictor balances GNN model accuracy and efficiency based on the targeted device's characteristics. Experimental results demonstrate that HGNAS achieves significant speedups (up to 10.6×) and peak memory reduction (88.2%) with minimal accuracy loss compared to DGCNN across various edge devices. 


summary: Convolutional neural networks (CNNs) have become the state-of-the-art in various AI tasks but their inference is computationally expensive. This work introduces HAPI, a new methodology for generating high-performance early-exit networks by co-optimising the placement of intermediate exits with the early-exit strategy during inference. HAPI proposes an efficient design space exploration algorithm to quickly traverse a large number of architectures and find the best design for a specific use case and hardware. Evaluations show that HAPI consistently outperforms other search methods and state-of-the-art early-exit schemes across latency budgets, achieving up to 5.11× speedup over lightweight models on embedded devices. 


summary: Modern consumer electronic devices often use deep neural networks for intelligent services.  This paper proposes an extension to the NNStreamer stream pipeline framework to support among-device AI, enabling the sharing of computing resources and hardware capabilities across devices of various vendors. The goal is to expand the use of on-device AI services, making them atomic, re-deployable, and shareable among connected devices. This work is part of the Linux Foundation (LF AI & Data) open source project and welcomes community contributions. 


summary: Deploying deep learning models in cloud clusters provides efficient and prompt inference services. These clusters usually have CPUs for input preprocessing and GPUs for forward computation. Recurrent neural networks (RNNs), essential for temporal inputs, have high inter-operator parallelism. Chrion optimizes RNN inference by collaboratively utilizing CPUs and GPUs. It formulates model deployment as an NP-hard scheduling problem on heterogeneous devices. Chrion preprocesses the model, partitions the graph to select execution devices, and performs forward computation in parallel on the CPU and GPU. Experiments show a 19.4% reduction in execution time (latency-optimal) and a 67.5% reduction in GPU memory footprint (memory-optimal) compared to GPU-only execution. 


summary: Customized hardware accelerators have been developed to provide improved performance and efficiency for DNN inference and training. However, the existing hardware accelerators may not always be suitable for handling various DNN models as their architecture paradigms and configuration tradeoffs are highly application-specific. It is important to benchmark the accelerator candidates in the earliest stage to gather comprehensive performance metrics and locate the potential bottlenecks. Further demands also emerge after benchmarking, which require adequate solutions to address the bottlenecks and improve the current designs for targeted workloads. To achieve these goals, in this paper, we leverage an automation tool called DNNExplorer [1] for benchmarking customized DNN hardware accelerators and exploring novel accelerator designs with improved performance and efficiency. Key features include (1) direct support to popular machine learning frameworks for DNN workload analysis and accurate analytical models for fast accelerator benchmarking; (2) a novel accelerator design paradigm with high-dimensional design space support and fine-grained adjustability to overcome the existing design drawbacks; and (3) a design space exploration (DSE) engine to generate optimized accelerators by considering targeted AI workloads and available hardware resources. Results show that accelerators adopting the proposed novel paradigm can deliver up to 4.2× higher throughput (GOP/s) than the state-of-the-art pipeline design in [2] and up to 2.0× improved efficiency than the recently published generic design in [3]) given the same DNN model and resource budgets. With DNNExplorer's benchmarking and exploration features, we

summary: DNN/Accelerator co-design has shown great potential in improving QoR and performance. Typical approaches separate the design flow into two-stage: (1) designing an application-specific DNN model with the highest accuracy; (2) building an accelerator considering the DNN specific characteristics. Though significant efforts have been dedicated to the improvement of DNN accuracy, it may fail in promising the highest composite score which combines the goals of accuracy and other hardware-related constraints (e.g., latency, energy efficiency) when building a specific neural network-based system. In this work, we present a single-stage automated framework, YOSO, aiming to generate the optimal solution of software-and-hardware that flexibly balances between the goal of accuracy, power, and QoS. YOSO jointly searches in the combined DNN and accelerator design spaces, which achieves a better composite score when facing a multiobjective design goal. As the search space is vast and it is costly to directly evaluate the accuracy and performance of the DNN and hardware architecture in design space search, we propose a cost-effective method to measure the accuracy and performance of solutions under consideration quickly. Compared with the two-stage method on the baseline systolic array accelerator and state-of-the-art dataset, we achieve 1.42x~2.29x energy reduction or 1.79x~3.07x latency reduction at the same level of precision, for different user-specified energy and

summary: The explosive growth of various types of big data and advances in AI technologies have catalyzed a new type of workload called multi-modal DNNs. These DNNs interpret and reason about information from multiple modalities, making them more applicable to real-world AI scenarios. Despite their importance, limited research has focused on understanding their characteristics and implications on current computing software/hardware platforms. Existing benchmarks either target uni-modal DNNs or focus on algorithm characteristics. To advance the understanding of multi-modal DNN workloads and facilitate related research, we present MM-Bench, an open-source, end-to-end benchmark suite consisting of real-world multi-modal DNN workloads with relevant performance metrics for evaluation. MM-Bench is used to conduct an in-depth analysis on the characteristics of multi-modal DNNs, demonstrating their unique characteristics of clear multi-stage execution, frequent synchronization, and high heterogeneity, which distinguish them from conventional uni-modal DNNs. This work aims to provide insights for future software/hardware design and optimization to underpin multi-modal DNNs on both cloud and edge computing platforms. 


summary: Widely popular transformer-based NLP models such as BERT and GPT have enormous capacity trending to billions of parameters. Current execution methods demand brute-force resources such as HBM devices and high-speed interconnectivity for data parallelism. In this paper, we introduce a new relay-style execution technique called L2L (layer-to-layer) where at any given moment, the device memory is primarily populated only with the executing layer(s)'s footprint. The model resides in the DRAM memory attached to either a CPU or an FPGA as an entity we call eager param-server (EPS). Unlike a traditional param-server, EPS transmits the model piecemeal to the devices thereby allowing it to perform other tasks in the background such as reduction and distributed optimization. To overcome the bandwidth issues of shuttling parameters to and from EPS, the model is executed a layer at a time across many micro-batches instead of the conventional method of minibatches over the whole model. In this paper, we explore a conservative version of L2L that is implemented on a modest Azure instance for BERT-Large running it with a batch size of 32 on a single V100 GPU using less than 8GB memory. Our results show a more stable learning curve, faster convergence, better accuracy, and 35% reduction in memory compared to the state-of-the-art baseline. Our method reproduces BERT results on any mid-level GPU that was hitherto not feasible. L2L

summary: This paper addresses the challenges of deploying machine learning workloads on edge embedded devices, which are characterized by limited compute and memory resources, tight power budgets, and real-time decision-making requirements. The authors propose a comprehensive design methodology for efficient DNN applications on embedded systems, including efficient DNN model designs, accelerator design and workload mapping technologies, and cross-stack optimization strategies. The paper discusses the challenges of deploying machine learning workloads on edge embedded devices, including the complexity of DNN models, the difficulty of mapping DNNs onto existing hardware, and the lack of efficient optimization strategies. 


summary: A key issue in system design is the lack of communication between hardware, software, and domain experts.  Recent research work shows progress in automatic HW/SW co-design flows of neural accelerators that seems to make this kind of communication obsolete. However, most real-world systems are a composition of multiple processing units, communication networks, and memories.  This position paper discusses possibilities for establishing a methodology for systems that include (reconfigurable) dedicated accelerators and outlines the central role that languages and tools play in the process.  


summary: ## Abstract:

Recently, a novel model named Kolmogorov-Arnold Networks (KAN) has been proposed with the potential to achieve the functionality of traditional deep neural networks (DNNs) using orders of magnitude fewer parameters by parameterized B-spline functions with trainable coefficients. However, the B-spline functions in KAN present new challenges for hardware acceleration. Evaluating the B-spline functions can be performed by using look-up tables (LUTs) to directly map the B-spline functions, thereby reducing computational resource requirements. However, this method still requires substantial circuit resources (LUTs, MUXs, decoders, etc.). For the first time, this paper employs an algorithm-hardware co-design methodology to accelerate KAN. The proposed algorithm-level techniques include Alignment-Symmetry and PowerGap KAN hardware aware quantization, KAN sparsity aware mapping strategy, and circuit-level techniques include N:1 Time Modulation Dynamic Voltage input generator with analog-CIM (ACIM) circuits. The impact of non-ideal effects, such as partial sum errors caused by the process variations, has been evaluated with the statistics measured from the TSMC 22nm RRAM-ACIM prototype chips. With the best searched hyperparameters of KAN and the optimized circuits implemented in 22 nm node, we can reduce hardware area by 41.78x, energy by 77.97x with 3.03% accuracy boost compared to the traditional DNN hardware. 


summary: Distributed deep learning training requires careful consideration of hardware architecture and device placement strategies. This paper introduces PHAZE, a novel approach that co-optimizes both aspects, achieving higher throughput for large language models compared to existing methods. PHAZE leverages tensor and vector units, memory configurations, and dynamic programming to find optimal operator scheduling and device placement strategies. It also considers microbatch size, recomputation, and activation stashing to balance memory usage and storage requirements. The entire source code for PHAZE is available on GitHub. 


summary: The paper presents RHNAS, a novel method for jointly optimizing neural network architecture and hardware accelerators. RHNAS combines reinforcement learning for hardware optimization with differentiable neural architecture search, resulting in realizable designs with improved latency and energy efficiency. The authors highlight the challenges of fully differentiable co-design methods, which fail to account for nonsynthesizable (invalid) designs in hardware search spaces. RHNAS addresses this limitation by enabling the exploration of configurable hardware accelerators with arbitrary neural network search spaces, ultimately achieving significant performance gains over default hardware accelerator designs. 


summary: Neural networks are powerful tools for analyzing big data, but traditional CPUs cannot achieve the desired performance or energy efficiency for these applications. While GPGPUs offer general purpose and high throughput, they lack the energy efficiency needed for data reuse. ASIC accelerators excel in performance and energy efficiency but have limited use cases. CISC accelerators attempt to achieve both general purpose and high energy efficiency by decomposing NN applications into simple instructions, but they fail to achieve the same level of data reuse optimization as ASIC accelerators.  This paper proposes RISC-NN, a novel many-core RISC-based NN accelerator that achieves high expressiveness, parallelism, programmability, and low control-hardware costs. It can implement all the necessary instructions of state-of-the-art CISC accelerators, while also achieving advanced optimization like multiple-level data reuse and support for Sparse NN applications. Experimental results show that RISC-NN outperforms Nvidia TITAN Xp GPGPU by an average of 11.88x, and outperforms CISC-based TPU by an average of 1.29x, 8.37x, and 21.71x for CNN, MLP, and LSTM applications, respectively. Additionally, RISC-NN achieves further performance improvements and energy reductions when applied to Sparse NN applications. 


summary: Embodied AI robots have the potential to significantly improve human life and manufacturing, and progress in using large language models (LLMs) to control robots depends heavily on efficient computing systems. Current computing systems for embodied AI robots are designed with a frame-based approach, resulting in high latency and energy consumption. Corki is a novel algorithm-architecture co-design framework for real-time embodied AI robot control that aims to address this issue. Corki decouples LLM inference, robotic control, and data communication to predict trajectories for the near future, thus reducing LLM inference frequency. This is coupled with hardware that accelerates trajectory conversion into torque signals for robot control and a parallel execution pipeline for data communication and computation.  As a result, Corki significantly reduces LLM inference frequency (up to 8.0x), leading to a speed-up of up to 3.6x and a success rate improvement of up to 17.3%. The code for re-implementation is available. 


summary: This paper proposes STAR, a new architecture for attention models using RRAM crossbars. STAR addresses the efficiency bottleneck of frequent softmax operations by utilizing a dedicated RRAM-based softmax engine and a fine-grained global pipeline.  The paper demonstrates that this approach leads to significant performance improvements over both GPUs and existing RRAM-based attention accelerators, achieving up to 30.63x and 1.31x efficiency gains, respectively. 


