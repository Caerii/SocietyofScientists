summary: This paper presents a computer vision-based model for detecting road obstacles in construction zones, a crucial element for safe autonomous driving. The model, built on the YOLO framework, achieves impressive accuracy (over 94% mean average precision) and operates with a quick inference time of 1.6 milliseconds. This suggests that the system can effectively identify obstacles under varying conditions, improving road safety for autonomous vehicles. 


summary: This paper proposes a system for optimizing Tilapia feeding using computer vision and IoT technologies. It employs real-time sensors to monitor water quality and computer vision to analyze fish size, determining the optimal feed amount. YOLOv8 is used for keypoint detection to measure fish weight, achieving 94% precision on a dataset of 3,500 images. This method, incorporating data collection mirroring inference conditions, significantly improves feeding accuracy and could potentially increase production up to 58 times compared to traditional farms. 


summary: This paper proposes a novel fast architecture for real-time semantic segmentation named DuFNet. DuFNet proposes a novel Semantic Information Flow (SIF) structure for context information and a novel Fringe Information Flow (FIF) structure for spatial information. The SIF encodes the input stage by stage in the ResNet18 backbone and provides context information for the feature fusion module. The FIF consists of a pooling layer and an upsampling operator followed by projection convolution layer.  DuFNet achieved faster speed and comparable performance with 72.34% mIoU accuracy and 78 FPS on Cityscapes Dataset based on the ResNet18 backbone. 


summary: This research focuses on automating stem detection and classifying xylem wetness using a Scholander Pressure Chamber.  The goal is to improve SWP (stem water potential) measurement, which is crucial for efficient irrigation management in agriculture.  The study involved collecting video data and applying computer vision and machine learning techniques to identify stems and classify water emergence at the xylem.  The best performing model combination for this process was YOLOv8n for stem detection and ResNet50 for classification, achieving a Top-1 accuracy of 80.98%. 


summary: This study provides a detailed analysis of the YOLOv8 object detection model, examining its architecture, training methods, and performance improvements over prior versions like YOLOv5. Key advancements include the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the shift to an anchor-free approach. The paper evaluates YOLOv8's performance on benchmarks such as Microsoft COCO and Roboflow 100, showcasing its high accuracy and real-time capabilities across diverse hardware platforms. The study also explores YOLOv8's developer-friendly enhancements, including its unified Python package and CLI, which simplify model training and deployment. This research establishes YOLOv8 as a leading solution in the evolving object detection domain. 


summary: This paper analyzes 30 years of computer vision research papers and patents, revealing a pipeline where computer vision research directly fuels surveillance technologies. The analysis of over 40,000 documents shows that a vast majority of these papers and patents explicitly state their technology is capable of extracting human data, particularly about the body and its parts. The paper highlights how leading universities and tech corporations play a key role in this pipeline, with their research being cited in thousands of surveillance patents. The study counters the narrative that only a few entities are responsible for surveillance, demonstrating how the entire field contributes to it. The number of papers with downstream surveillance patents has increased fivefold since the 1990s, with computer vision research now used in over 11,000 surveillance patents. The paper further reveals how language used in these documents obfuscates the extent of surveillance. 


summary: This paper presents a mobile imaging system for assessing road safety in urban areas. The system uses computer vision techniques to identify irregularities like missing street lights, damaged roads, and traffic violations.  The system was tested on 2000 km of unconstrained roads in an entire city, generating city-level safety maps. The authors investigated the performance of these techniques under various conditions, including different road types, lighting, and weather.  This system offers a cost-effective and scalable solution for road safety monitoring. 


summary: This paper introduces a reconfigurable CMOS image sensor (CIS) system that improves energy efficiency in computer vision applications. It achieves this by selectively skipping uneventful regions or rows during the sensor's readout and analog-to-digital conversion (ADC) phases. A novel masking algorithm directs this skipping process in real-time, optimizing both the front-end sensor and back-end neural networks for applications such as autonomous driving and augmented/virtual reality (AR/VR). The system can operate in standard mode without skipping, depending on application needs. Evaluations on object detection and gaze estimation datasets demonstrate up to 53% reduction in front-end sensor energy while maintaining state-of-the-art accuracy. 


summary: This paper describes the research experience of a computer engineering undergraduate student working with computer vision and robotics. The student explored the use of optical flow to detect moving objects, detailing the challenges faced and solutions used. The paper highlights the development of both technical skills and interpersonal skills related to teamwork and diversity. It also emphasizes the importance of learning process, problem-solving and creative thinking. 


summary: This paper proposes a vision-based system for automatic grocery tracking in smart homes. It addresses the challenge of tracking groceries in a home environment by combining retail shelving data and fruit datasets with real-time 360-degree views of home grocery storage. By integrating this object detection system with supply chain and user food interest prediction systems, the authors aim to achieve complete automation of grocery ordering. 


summary: This report focuses on the problem of quickly annotating video frames with bounding boxes for a novel object. The authors present a user interface and workflow specifically designed to make this process fast, even for unfamiliar objects. 


summary: Existing object detection methods rely on neural networks and deep learning for applications such as autonomous driving and aerial photography. These methods are vulnerable to factors such as illumination, occlusion, and viewing angle. This study uses convolutional neural networks (CNN) to recognize objects, leveraging their advantages of end-to-end learning, sparse relations, and weight sharing. The study focuses on classifying objects based on their detected bounding box position, exploring recognition accuracy under varying distances. Results show that accuracy is influenced by the object proportion and number of samples: smaller objects and fewer samples lead to higher accuracy. The study emphasizes the need for cost-effective object recognition systems, especially in the context of the global economic impact of the pandemic. The authors propose a custom dataset and utilize the Yolov2 model for training and testing, demonstrating the effectiveness of neural networks in improving object recognition rates. 


summary: This paper explores the effectiveness of simple visual sensors, like single photoreceptors, in solving computer vision tasks. The researchers demonstrate that even with resolutions as low as one-by-one pixel, these sensors can achieve performance comparable to high-resolution cameras in tasks like visual navigation and continuous control. They highlight the importance of sensor design and present a computational optimization algorithm to find well-performing designs. The paper also includes a human survey to evaluate the effectiveness of intuitive designs. 


summary: Object detection is a well-studied problem in autonomous driving, particularly for pedestrian detection. However, object detection using fisheye cameras for near-field sensing has been less explored. The standard bounding box representation struggles with fisheye cameras due to radial distortion. This paper investigates alternative representations for object detection, including rotated bounding boxes, ellipses, polygons, and polar arc/angle representations. The proposed FisheyeDetNet model, using polygons, outperforms other approaches, achieving a mAP score of 49.5% on the Valeo fisheye surround-view dataset. This dataset comprises 60,000 images captured from four surround-view cameras across different regions. This research is the first detailed study on object detection using fisheye cameras for autonomous driving. 


summary: This paper introduces an enhanced self-checkout system for retail stores that leverages an improved version of the YOLOv10 network. The system aims to boost checkout efficiency and reduce labor costs. It achieves this by implementing targeted optimizations to the YOLOv10 model, including the incorporation of the detection head structure from YOLOv8, which results in improved product recognition accuracy. Additionally, a specialized post-processing algorithm tailored for self-checkout scenarios is developed to further enhance the system's performance. The paper claims that experimental results indicate that the system surpasses existing methods in terms of product recognition accuracy and checkout speed. This research provides a novel technical solution for retail automation and offers insights into optimizing deep learning models for real-world applications. 


summary: Early-stage identification of fruit flowers in both opened and unopened conditions in an orchard environment is crucial for crop load management operations such as flower thinning and pollination using automated and robotic platforms. This paper proposes a vision system that detects early-stage flowers in an unstructured orchard environment using the YOLOv5 object detection algorithm. The centroid of individual flowers (both open and unopen) is identified and associated with flower clusters via K-means clustering. The system achieves an accuracy of up to 81.9% mAP in detecting opened and unopened flowers in commercial orchard images. 


summary: DroneVis is a new Python library for automating computer vision algorithms on Parrot drones. It offers a variety of features and tasks, including various computer vision models. The library is well-documented, easy to customize, and available on Github. 


summary: This paper describes two computer vision systems designed to automatically count crustacean larvae in industrial ponds. The first system uses an iPhone 11 camera and a specially designed bucket to capture images of larvae in controlled conditions. The second system uses a DSLR Nikon D510 camera to capture images of larvae in outdoor conditions. Both systems employ a YOLOv5 CNN model to count the larvae, overcoming the challenge of their small size. The paper also details the development of a growth function for Macrobrachium Rosenberg's larvae, using data from 11 growth stages over 19 days.  The results show promising accuracy in both systems, with the first achieving an 88.4% accuracy and the second reaching 86%. 


summary: This paper explores the potential of computer vision in security and surveillance, presenting a novel approach to track motion in videos.  By categorizing motion into Arcs, Lanes, Converging/Diverging, and Random/Block motions using Motion Information Images and Blockwise dominant motion data, the paper examines different optical flow techniques, CNN models, and machine learning models. The results can train anomaly-detection models, provide behavioral insights based on motion, and enhance scene comprehension. 


summary: This study presents a novel driver drowsiness detection system that combines deep learning techniques with the OpenCV framework. The system utilizes facial landmarks extracted from the driver's face as input to Convolutional Neural Networks trained to recognize drowsiness patterns. The integration of OpenCV enables real-time video processing, making the system suitable for practical implementation. Extensive experiments on a diverse dataset demonstrate high accuracy, sensitivity, and specificity in detecting drowsiness. The proposed system has the potential to enhance road safety by providing timely alerts to prevent accidents caused by driver fatigue. This research contributes to advancing real-time driver monitoring systems and has implications for automotive safety and intelligent transportation systems. The successful application of deep learning techniques in this context opens up new avenues for future research in driver monitoring and vehicle safety. The implementation code for the paper is available at this https URL. 


summary: This paper proposes a low-cost alternative to graphics tablets for online educators called "Do-It-Yourself Graphics Tab" or "DIY Graphics Tab".  It uses computer vision to capture images of a person writing on paper with a webcam and generates an output screen containing the written content.  This solution addresses the high cost of graphics tablets for many instructors and requires only a pen, paper, and a webcam. The paper outlines challenges faced by the system, such as hand occlusion, paper movement, and lighting conditions, and describes a pipeline that uses instance segmentation and preprocessing to overcome these obstacles. User experience evaluations from teachers and students are also discussed. 


summary: This paper investigates the YOLOv5 model for cattle identification in yards. The current solution using RFID tags has limitations with lost or damaged tags. This research utilizes YOLOv5 with eight different backbones and investigates the impact of mosaic augmentation on the model's performance. The results show that YOLOv5 with a transformer backbone achieved the highest accuracy, with a mean Average Precision (mAP) of 0.995. Mosaic augmentation significantly improved the model's accuracy across all backbones. The study concludes that YOLOv5 holds excellent potential for automatic cattle identification. 


summary: This paper proposes a novel reCAPTCHA system called arXiv reCAPTCHA, designed to be more robust to adversarial attacks. The system leverages the unique characteristics of the arXiv preprint repository, such as the abundance of scientific text and the high quality of submissions, to create a more challenging and less exploitable CAPTCHA. It utilizes a combination of natural language processing and computer vision techniques to analyze text and image content, ensuring that only genuine users can pass the challenge. The paper details the system's architecture, training process, and evaluation results, showcasing its improved security and user experience compared to traditional CAPTCHA approaches. 


summary: For distant iris recognition, a long focal length lens is generally used to ensure the resolution of iris images, which reduces the depth of field and leads to potential defocus blur. To accommodate users at different distances, it is necessary to control focus quickly and accurately. While for users in motion, it is expected to maintain the correct focus on the iris area continuously. In this paper, we introduced a novel rapid autofocus camera for active refocusing of the iris area of the moving objects using a focus-tunable lens. Our end-to-end computational algorithm can predict the best focus position from one single blurred image and generate a lens diopter control signal automatically. This scene-based active manipulation method enables real-time focus tracking of the iris area of a moving object. We built a testing bench to collect real-world focal stacks for evaluation of the autofocus methods. Our camera has reached an autofocus speed of over 50 fps. The results demonstrate the advantages of our proposed camera for biometric perception in static and dynamic scenes. The code is available at https://github.com/Debatrix/AquulaCam. 


summary: It is common practice to think of a video as a sequence of images (frames), and re-use deep neural network models that are trained only on images for similar analytics tasks on videos. This paper shows that this "leap of faith" that deep learning models that work well on images will also work well on videos is actually flawed. Even when a video camera is viewing a scene that is not changing in any human-perceptible way, the accuracy of video analytics application fluctuates noticeably. These fluctuations occur because successive frames produced by the video camera may look similar visually, but are perceived quite differently by the video analytics applications. The root cause for these fluctuations is the dynamic camera parameter changes that a video camera automatically makes in order to capture and produce a visually pleasing video. The camera inadvertently acts as an "unintentional adversary" because these slight changes in the image pixel values in consecutive frames have a noticeably adverse impact on the accuracy of insights from video analytics tasks that re-use image-trained deep learning models. To address this inadvertent adversarial effect from the camera, the authors explore the use of transfer learning techniques to improve learning in video analytics tasks through the transfer of knowledge from learning on image analytics tasks. Experiments with a number of different cameras, and a variety of different video analytics tasks, show that the inadvertent adversarial effect from the camera can be noticeably offset by quickly re-training the deep learning models using transfer learning.  


summary: This paper presents a real-time helmet violation detection system that uses a unique data processing strategy called "few-shot data sampling" and the YOLOv8 object detection model. The system was designed to operate in real-time and achieve high accuracy even with limited training data. The proposed method achieved a mean Average Precision (mAP) score of 0.5861 on experimental validation data, demonstrating its effectiveness and robustness. The code for the few-shot data sampling technique is available on GitHub. 


summary: Convolutional Neural Networks (CNNs) are often used for computer vision tasks, but they are compute-intensive and difficult to deploy on low-power devices. This paper proposes a method to improve efficiency by identifying and excluding irrelevant pixels from the input image. The authors found that 48% of pixels in three popular computer vision datasets (COCO, MOT Challenge, and PASCAL VOC) are irrelevant to the task. They developed a "focused convolution" to modify the CNN's convolutional layers to exclude these irrelevant pixels, resulting in a 45% reduction in inference latency, energy consumption, and multiply-add count on an embedded device without loss of accuracy. 


summary: This paper proposes a computer vision-based solution for automated crop field surveillance to address the problem of wild animal trespassing. The system aims to reduce crop loss and automate field security. The paper discusses the existing challenges faced by farmers due to wildlife damage, including high costs associated with traditional solutions like fences and the inefficacy of scare tactics. 


summary: Retail checkout systems typically rely on barcode scanners or QR codes, which are time-consuming, require human supervision, and result in long queues. This paper proposes ARC, a vision-based system that aims to make checkout faster, autonomous, and more convenient. ARC utilizes a convolutional neural network to identify objects placed beneath a webcam. The system was evaluated on a dataset of 100 local retail items and achieved reasonable accuracy. The project code and dataset are publicly available. 


summary: This study compares five YOLOv5 variants (YOLOv5n6s, YOLOv5s6s, YOLOv5m6s, YOLOv5l6s, and YOLOv5x6s) for vehicle detection in different environments. The research evaluates their performance in detecting various vehicle types (Car, Bus, Truck, Bicycle, and Motorcycle) under varying conditions (lighting, occlusion, and weather). Performance metrics like precision, recall, F1-score, and mean Average Precision are used to assess each model's accuracy and reliability. YOLOv5n6s shows a good balance between precision and recall, especially for detecting Cars. YOLOv5s6s and YOLOv5m6s improve recall, detecting more objects. YOLOv5l6s, with its larger capacity, performs well for Cars but struggles with Motorcycles and Bicycles. YOLOv5x6s is effective with Buses and Cars but has difficulties with Motorcycles. 


summary: Facial analysis is a popular area of research in computer vision. This paper presents a system-level design for a real-time facial analysis system. The system utilizes a collection of deep neural networks to perform various tasks including age, gender, and facial expression recognition, as well as facial similarity analysis. The paper investigates the parallelization and interaction between individual tasks to achieve high accuracy and real-time performance. The system's accuracy is comparable to state-of-the-art methods, and its recognition speed meets real-time requirements. Additionally, a multitask network is proposed for joint prediction of age, gender, and facial expression. The source code and trained models are available online. 


summary: ## Abstract:

Given the growing urban population and the resulting traffic congestion, the development of smart parking systems has become increasingly important. Smart parking solutions use cameras, sensors, and algorithms like computer vision to identify available parking spaces. This approach improves parking spot recognition, reduces traffic and pollution, and optimizes travel time. Recent years have seen a rise in the use of computer vision-based methods. However, many existing studies rely on manually labeled parking spots, which poses challenges in terms of cost and practical implementation. 

To address this, we propose a novel approach called PakLoc, which automatically localizes parking spots. Additionally, we introduce the PakSke module, which automatically adjusts the rotation and size of detected bounding boxes. Our methodology, evaluated on the PKLot dataset, results in a significant reduction in human labor by 94.25%. 

Another crucial aspect of a smart parking system is its ability to accurately determine and indicate the state of parking spots within a parking lot. The traditional approach involves using classification techniques to predict the status of parking spots based on bounding boxes derived from manually labeled grids. In this study, we present a novel approach called PakSta for automatically identifying the state of parking spots. Our method utilizes object detection from PakLoc to simultaneously determine the occupancy status of all parking spots within a video frame. PakSta demonstrates competitive performance on the PKLot dataset compared to other classification methods. 


summary: This paper examines the occlusion problem in cattle detection using unmanned aerial vehicles (UAVs).  The authors compare three cutting-edge object detection algorithms - YOLOv7, RetinaNet with ResNet50 backbone, RetinaNet with EfficientNet, and Mask RCNN - to identify hidden cattle in drone-captured datasets.  Experimental results demonstrate that YOLOv7 outperforms the other algorithms, achieving a precision of 0.612. The study focuses on improving cattle detection accuracy, particularly in challenging scenarios where animals are partially obscured. 


summary: With advancements in technology, computers now have the processing power to handle real-time images. Face recognition is commonly used for security and commercial applications, and AI has boosted its usage across many fields, including education. This study utilizes deep learning for object detection in images, specifically to track student attendance at an educational institution. The system captures images from a camera and analyzes them with image processing algorithms to record student attendance. This application is currently being tested at a school for the 2022-2023 academic year. 


summary: This research explores the application of deep learning in autonomous driving computer vision technology. Using advanced technologies such as convolutional neural networks (CNN), multi-task joint learning methods, and deep reinforcement learning, the article analyzes the application of deep learning in image recognition, real-time target tracking and classification, environment perception and decision support, and path planning and navigation. The proposed system achieves over 98% accuracy in image recognition, target tracking, and classification, and demonstrates efficient performance in environmental perception and decision support, path planning, and navigation. The conclusion highlights that deep learning significantly improves the accuracy and real-time response capabilities of autonomous driving systems. While challenges in environmental perception and decision support remain, the advancement of technology is expected to lead to wider applications and greater potential in the future. 


summary: This project explores how machine learning and computer vision can be used to improve accessibility for people with visual impairments. The paper proposes a mobile application that uses audio and haptic feedback to help blind people orient themselves in space.  The application will have three main features: 1) scanning text from a camera and reading it to the user, 2) detecting objects and providing audio feedback about their location, and 3) currency detection, which provides the user with the total amount of currency value through the camera. 


summary: The paper "YOLO OBJECT DETECTION USING OPENCV" focuses on the challenges of object detection in computer vision. The goal is to create algorithms that can recognize objects and their location within images. While object detection is a complex task, the paper aims to address these challenges by utilizing the YOLO (You Only Look Once) object detection model with OpenCV.  The authors propose an approach that leverages a trained YOLO model to perform object detection and presents a detailed breakdown of the technical implementation.  This paper aims to overcome limitations of existing systems and improve the accuracy, up-to-dateness, and efficiency of object detection. 


summary: This research paper focuses on the detection and tracking of pedestrians from video sequences. It explores how computer vision and pattern recognition can be combined to achieve this, emphasizing the importance of this technology in various applications like target recognition and behavior understanding.  The paper proposes a binary function based on importance and uses space representation to minimize noise impact on the tracking algorithm. The color band learning method is then implemented to update the target template online and account for changes in pedestrian appearance. Experimental results demonstrate the system's effectiveness in tracking even with significant changes in appearance and posture. 


summary: This paper proposes a novel method for intelligent streetlight management using a smart CCTV camera and semantic segmentation. The system automatically adjusts streetlight brightness based on the presence of pedestrians or vehicles detected in the CCTV footage, dimming the lights when no activity is present. This approach also differentiates between day and night, enabling automated ON/OFF switching for energy conservation. The model is trained using a U-net architecture with ResNet-34 as its backbone, and its effectiveness is validated through assessment metrics. This solution is presented as a cost-effective, energy-efficient, and resilient alternative to conventional streetlight management systems. 


summary: Image classification models, including convolutional neural networks (CNNs), perform well on a variety of classification tasks but struggle under conditions of partial occlusion, i.e., conditions in which objects are partially covered from the view of a camera. Methods to improve performance under occlusion, including data augmentation, part-based clustering, and more inherently robust architectures, including Vision Transformer (ViT) models, have, to some extent, been evaluated on their ability to classify objects under partial occlusion. However, evaluations of these methods have largely relied on images containing artificial occlusion, which are typically computer-generated and therefore inexpensive to label. Additionally, methods are rarely compared against each other, and many methods are compared against early, now outdated, deep learning models. We contribute the Image Recognition Under Occlusion (IRUO) dataset, based on the recently developed Occluded Video Instance Segmentation (OVIS) dataset (arXiv:2102.01558). IRUO utilizes real-world and artificially occluded images to test and benchmark leading methods' robustness to partial occlusion in visual recognition tasks. In addition, we contribute the design and results of a human study using images from IRUO that evaluates human classification performance at multiple levels and types of occlusion. We find that modern CNN-based models show improved recognition accuracy on occluded images compared to earlier CNN-based models, and ViT-based models are more accurate than CNN-based models on occluded images, performing only modestly worse than human accuracy. We

summary: The DifuzCam design replaces a traditional camera lens with a mask and uses a pre-trained diffusion model for image reconstruction. This approach allows for a significantly smaller and lighter camera while maintaining high image quality. The model can leverage textual descriptions of the captured scene to further improve image reconstruction. 


summary: This paper proposes a method for analyzing classroom teaching behavior using intelligent image recognition. It utilizes a fast target detection method based on FFmpeg CODEC, extracts MHI-HOG joint features from the detected foreground target area, and employs a BP neural network support vector machine joint classifier to recognize behavior. The method achieves high accuracy in motion detection (95%) and foreground detection (90% time savings), while the MHI-HOG joint feature-based behavior classification and recognition system has a comprehensive recognition rate of 95%. The built-in BP neural network support vector machine demonstrates 97% accuracy in extracting, classifying, and recognizing single sample characteristics. The study aims to identify and analyze classroom behavior and validate the effectiveness of the proposed collaborative classifiers for building an intelligent classroom. 


summary: This paper focuses on designing a Raspberry Pi system with a camera that can detect and count objects within a target area. The system utilizes Python programming due to its compatibility with Raspberry Pi and its ease of use for rapid application development. The results show that the system successfully detects and counts different objects in an image, achieving an average efficiency of 90.206%, demonstrating high reliability. 


summary: Open-vocabulary detection (OVD) aims to detect objects beyond a predefined set of categories. YOLO-World, a pioneering model incorporating the YOLO series into OVD, prioritizes speed and efficiency. However, its performance is limited by its neck feature fusion mechanism. Mamba-YOLO-World, a novel YOLO-based OVD model, addresses these limitations by using the proposed MambaFusion Path Aggregation Network (MambaFusion-PAN) as its neck architecture. This new architecture utilizes a State Space Model-based feature fusion mechanism with linear complexity and globally guided receptive fields, outperforming YOLO-World on COCO and LVIS benchmarks while maintaining comparable parameters and FLOPs. Mamba-YOLO-World also surpasses existing state-of-the-art OVD methods with fewer parameters and FLOPs. 


summary: Precision agriculture aims to use technological tools for the agro-food sector to increase productivity, cut labor costs, and reduce the use of resources. This work takes inspiration from bees' vision to design a remote sensing system tailored to incorporate UV-reflectance into a flower detector. We demonstrate how this approach can provide feature-rich images for deep learning strawberry flower detection and we apply it to a scalable, yet cost effective aerial monitoring robotic system in the field. We also compare the performance of our UV-G-B image detector with a similar work that utilizes RGB images. 


summary: Real-time object detection in indoor settings is challenging due to variable lighting and complex backgrounds. This paper addresses this by evaluating existing datasets and models, creating a refined dataset focused on 32 relevant indoor categories derived from OpenImages v7. It then presents an adaptation of a CNN detection model with an attention mechanism to improve feature identification in cluttered scenes. The study demonstrates that this approach outperforms existing models in both accuracy and speed, opening new avenues for research and applications in real-time indoor object detection. 


summary: This paper introduces FA-YOLO, a novel object detection model that improves upon YOLOv9 by incorporating two new modules: the Fine-grained Multi-scale Dynamic Selection Module (FMDS) and the Adaptive Gated Multi-branch Focus Fusion Module (AGMF).  The FMDS module enhances detection accuracy for various object sizes by dynamically selecting and fusing multi-scale features. The AGMF module further refines feature fusion through parallel branches, incorporating information from gated units, the FMDS module, and TripletAttention. Experimental results on the PASCAL VOC 2007 dataset show FA-YOLO achieves a 1.0% improvement in mean Average Precision (mAP) compared to YOLOv9, reaching 66.1%. This improvement is particularly notable for small, medium, and large targets, with FA-YOLO demonstrating gains of 2.0%, 3.1%, and 0.9% respectively. 


summary: This paper proposes a novel design and implementation method for real-time multi-area target individual detection on video, where the detection areas are defined by freehand drawing on the video display. The drawn areas are represented as polylines with color changes indicating the drawing or detection stage. The shape of the drawn areas is customizable, and they operate independently. The detection results are presented via a Tkinter-based GUI. While the object recognition model is currently based on YOLOv5, the core design is model-independent. Using PIL, OpenCV, and Tkinter, the drawing and detection processes are real-time and efficient. The proposed design is basic and can be extended for various monitoring and detection applications. 


summary: Current parking area perception algorithms rely on limited range detection and error-prone homographic projection, but Advanced Driver Assistance Systems (ADAS) need comprehensive and intelligent Human-Machine Interfaces (HMIs) for user interaction. This paper introduces Multi-Task Fisheye Cross View Transformers (MT F-CVT) that uses a four-camera fisheye Surround-view Camera System (SVCS) to create a detailed Bird-Eye View (BEV) grid feature map. This map is processed by both a segmentation decoder and a Polygon-Yolo based object detection decoder for parking slots and vehicles. Trained on LiDAR labeled data, MT F-CVT accurately positions objects in real open-road scenes, achieving an F-1 score of 0.89. The smaller model operates at 16 fps on an Nvidia Jetson Orin board with similar detection results, demonstrating robust generalization. A demo video is available at the provided link. 


summary: Automating fruit and vegetable detection using computer vision is crucial for modernizing agriculture and improving efficiency. This paper presents an end-to-end pipeline for detecting and localizing fruits and vegetables in real-world scenarios. To achieve this, they curated a dataset named FRUVEG67 with images of 67 fruit and vegetable classes captured in unconstrained scenarios. They also developed a semi-supervised data annotation algorithm (SSDA) to generate bounding boxes for objects in the non-annotated images. For detection, they introduced the Fruit and Vegetable Detection Network (FVDNet), an ensemble version of YOLOv7 featuring three distinct grid configurations. Their experimental results highlight the superiority of FVDNet compared to previous versions of YOLO, showcasing remarkable improvements in detection and localization performance. They achieved an impressive mean average precision (mAP) score of 0.78 across all classes. 


summary: In static monitoring cameras, useful contextual information can stretch far beyond the few seconds typical video understanding models might see: subjects may exhibit similar behavior over multiple days, and background objects remain static. Due to power and storage constraints, sampling frequencies are low, often no faster than one frame per second, and sometimes are irregular due to the use of a motion trigger. In order to perform well in this setting, models must be robust to irregular sampling rates. In this paper we propose a method that leverages temporal context from the unlabeled frames of a novel camera to improve performance at that camera. Specifically, we propose an attention-based approach that allows our model, Context R-CNN, to index into a long term memory bank constructed on a per-camera basis and aggregate contextual features from other frames to boost object detection performance on the current frame. We apply Context R-CNN to two settings: (1) species detection using camera traps, and (2) vehicle detection in traffic cameras, showing in both settings that Context R-CNN leads to performance gains over strong baselines. Moreover, we show that increasing the contextual time horizon leads to improved results. When applied to camera trap data from the Snapshot Serengeti dataset, Context R-CNN with context from up to a month of images outperforms a single-frame baseline by 17.9% mAP, and outperforms S3D (a 3d convolution based baseline) by 11.2% mAP. 


summary: This paper proposes a novel video-based, two-stream deep neural network approach for automatic pain state recognition in dogs. The approach extracts and preprocesses body keypoints, and computes features from both keypoints and the RGB representation over the video. It also addresses self-occlusions and missing keypoints. The paper introduces a unique video-based dog behavior dataset, collected and annotated by veterinary professionals, and reports good classification results with the proposed approach. This study is one of the first works on machine learning based estimation of dog pain state. 


summary: This paper proposes a computer vision-based framework for detecting road traffic crashes (RTCs) using surveillance cameras. The system, comprised of five modules, utilizes YOLO for vehicle detection, MOSSE tracker for vehicle tracking, and a novel collision estimation method for accident detection. Violence flow descriptor (ViF) and SVM classifier are employed for crash prediction. Finally, a GPS and GSM module sends a notification to emergency services with the location, time, and date of the accident. The goal is to achieve high accuracy with minimal false alarms using a pipelined approach. 


summary: This study evaluated the performance of YOLOv8 model configurations for instance segmentation of strawberries into ripe and unripe stages in an open field environment. The YOLOv8n model demonstrated superior segmentation accuracy with a mean Average Precision (mAP) of 80.9%, outperforming other YOLOv8 configurations. It processed images at 12.9 milliseconds, while the least-performing model, YOLOv8s, processed at 22.2 milliseconds. Overall, YOLOv8n achieved the fastest inference speed of 24.2 milliseconds, outperforming YOLOv8s, YOLOv8m, YOLOv8l, and YOLOv8x. These results highlight the potential of advanced object segmentation algorithms to address complex visual recognition tasks in open-field agriculture effectively. 


summary: Object detection is essential for traffic management, autonomous vehicles, and smart cities. However, detecting small objects in images captured by distant cameras is challenging due to their size, distance, and cluttered backgrounds. To overcome these challenges, this paper introduces SOD-YOLOv8, a model specifically designed for small object detection in traffic scenes. SOD-YOLOv8 improves multi-path fusion within YOLOv8 to integrate features across different levels, enhances feature extraction with an Efficient Multi-Scale Attention Module, and introduces a new loss function called Powerful-IoU. The result is a model that significantly outperforms existing methods in small object detection, achieving higher recall, precision, and mAP without compromising computational efficiency. 


summary: This paper explores the application of computer vision in intelligent computing. It utilizes face recognition as a case study, applying image recognition, feature point extraction using the K-means clustering algorithm, and histogram-based classification modeling. The study focuses on extracting data for vision recognition, even in the presence of obstacles. Results highlight computer vision's potential as a key development area in artificial intelligence, enabling machines to perceive, recognize, track, and measure objects in a manner analogous to human vision. 


summary: This paper proposes a new methodology for estimating the geolocation of target vehicles in a GPS coordinate system using a monocular camera mounted on a moving vehicle. The methodology combines deep learning, image processing, and geometric computation to address the observed-vehicle localization problem. Real-world traffic data was used to evaluate the proposed algorithms, which effectively estimated the observed vehicle's latitude and longitude dynamically. 


summary: The MINSU (Mobile Inventory and Scanning Unit) algorithm uses computer vision to estimate the amount of material in a cabinet. The process involves five steps: object detection, foreground subtraction, K-means clustering, percentage estimation, and counting. Object detection identifies the cabinet's position, while foreground subtraction isolates the cabinet from the background. K-means clustering simplifies the image for analysis, and percentage estimation determines the proportion of the cabinet filled with material. Finally, the algorithm uses the percentage to estimate the quantity of materials inside the cabinet. 


summary: This paper proposes a new method for generating region proposals in object detection by using an event camera. The authors draw an analogy between the human eye's rod cells and the event camera, both of which detect changes in light intensity. They argue that the event camera can act as a region proposal network (RPN) in deep learning, similar to how the rods act as an RPN in human vision. By replacing the traditional RPN in Mask R-CNN with an event camera, the authors achieve faster object detection with comparable accuracy, making it suitable for fast applications. 


summary: This paper investigates real-time pedestrian recognition on small physical-size computers with low computational resources for faster speed. Three methods are presented: improved Local Binary Pattern (LBP) features and Adaboost classifier, optimized Histogram of Oriented Gradients (HOG) and Support Vector Machine, and fast Convolutional Neural Networks (CNNs). Results show that these methods achieve real-time pedestrian recognition with over 95% accuracy and over 5 fps on a small platform with a 1.8 GHz Intel i5 CPU. These methods are easily adaptable to small mobile devices, offering high compatibility and generality. 


summary: By analyzing the motion of people and other objects in a scene, we demonstrate how to infer depth, occlusion, lighting, and shadow information from video taken from a single camera viewpoint. This information is then used to composite new objects into the same scene with a high degree of automation and realism. In particular, when a user places a new object (2D cut-out) in the image, it is automatically rescaled, relit, occluded properly, and casts realistic shadows in the correct direction relative to the sun, and which conform properly to scene geometry. We demonstrate results (best viewed in supplementary video) on a range of scenes and compare to alternative methods for depth estimation and shadow compositing. 


summary: Accurate crop row detection is often challenged by varying field conditions. Traditional color-based segmentation struggles with these variations, and a lack of comprehensive datasets hinders the development of robust models. This paper presents a dataset for crop row detection with 11 field variations from Sugar Beet and Maize crops. A novel crop row detection algorithm is introduced that can detect crop rows under varying conditions, including curved rows, weeds, discontinuities, growth stages, tramlines, shadows, and light levels. The algorithm uses only RGB images from a front-mounted camera on a Husky robot and outperforms traditional color-based methods. Dense weed presence and discontinuities in crop rows were the most challenging conditions. The method can detect the end of a crop row and navigate the robot towards the headland area. 


summary: The paper explores the use of computer vision techniques to automate bee counting, which can help monitor bee colony health, analyze blooming periods, and investigate the effects of agricultural spraying. The authors compare three methods and find that a ResNet-50 convolutional neural network classifier achieves the best performance, reaching 87% accuracy on the BUT1 dataset and 93% accuracy on the BUT2 dataset. 


summary: Online exams via video conference software like Zoom have been adopted in many schools due to COVID-19. While it is convenient, it is challenging for teachers to supervise online exams from simultaneously displayed student Zoom windows. In this paper, we propose iExam, an intelligent online exam monitoring and analysis system that can not only use face detection to assist invigilators in real-time student identification, but also be able to detect common abnormal behaviors (including face disappearing, rotating faces, and replacing with a different person during the exams) via a face recognition-based post-exam video analysis. To build such a novel system in its first kind, we overcome three challenges. First, we discover a lightweight approach to capturing exam video streams and analyzing them in real time. Second, we utilize the left-corner names that are displayed on each student's Zoom window and propose an improved OCR (optical character recognition) technique to automatically gather the ground truth for the student faces with dynamic positions. Third, we perform several experimental comparisons and optimizations to efficiently shorten the training and testing time required on teachers' PC. Our evaluation shows that iExam achieves high accuracy, 90.4% for real-time face detection and 98.4% for postexam face recognition, while maintaining acceptable runtime performance. We have made iExam's source code available at https://github.com/VPRLab/iExam. 


summary: Current computer vision (CV) systems use an image signal processing (ISP) unit to convert raw images to RGB images.  However, this process can be computationally expensive, especially for low-power devices.  Recent works have proposed in-sensor and in-pixel computing approaches to bypass the ISP, but this can lead to accuracy degradation due to the difference in covariance between raw and processed images.  To address this issue, we propose to invert the ISP pipeline to enable training on raw images, which leads to a 7.1% increase in test accuracy.  We also propose an energy-efficient form of analog in-pixel demosaicing to further improve accuracy and reduce energy consumption, resulting in an 8.1% increase in mAP.  Finally, we demonstrate a 20.5% increase in mAP using few-shot learning on raw images from the PASCALRAW dataset. 


summary: This work studies the dynamic between research in the computer vision industry and academia. The results are demonstrated on a set of top-5 vision conferences. The study quantifies the share of industry-sponsored research, showing an increasing proportion of papers published by industry-affiliated researchers and more academics joining or collaborating with companies. It explores the impact of industry presence on research topics and citation patterns, finding similar topic distributions but a strong preference towards citing industry papers. Finally, it investigates possible reasons for citation bias, such as code availability and influence. 


summary: Visual recognition is crucial for harvesting robots, but unstructured environments pose challenges to detection accuracy. This paper proposes an improved YOLO v4 model, called YOLO v4+, to address these challenges.  YOLO v4+ incorporates an attention mechanism for feature refinement, a multi-scale feature fusion module, and a modified focal loss function. Experiments show that YOLO v4+ achieves an average precision of 94.25% and an F1 score of 93%, outperforming the original YOLO v4.  The model demonstrates high comprehensive and generalization abilities and can be applied to harvesting robots for enhanced robustness. 


summary: This paper proposes an object detection model for smart surveillance systems (3s) utilizing the YOLO v3 deep learning architecture. The model utilizes a transfer learning approach and the MS COCO dataset for training, achieving a high accuracy of 99.71% and an improved mean Average Precision (mAP) of 61.5.  This approach aims to address the need for efficient object detection in real-time surveillance systems, contributing to the advancement of global security measures. 


summary: This paper proposes a new obstacle detection and recognition model based on a computer-expanded convolutional neural network (CNN).  The model uses dilated convolutions to learn features from the original image without needing preprocessing, and it can achieve accurate recognition of obstacle types. The proposed model is highly accurate, has good generalization capability, and can be used in real-time applications. The model was built by combining a hierarchical expanded CNN structure with a Region of Interest (ROI) algorithm. The model was able to learn the features of various obstacle types and extract global features with characterization significance.  This enables real-time obstacle detection and high-accuracy type recognition. 


summary: Teaching Computer Science (CS) by having students write programs by hand on paper offers several advantages, including focused learning, careful thinking, and reduced cognitive load for beginners. However, the lack of tools for working with handwritten programs poses a challenge. The paper presents two innovative methods for handwritten code recognition:

1. **Combined OCR, indentation recognition, and a language model for post-OCR error correction:** This method surpasses existing systems, reducing error from 30% to 5% with minimal hallucinations.

2. **Multimodal language model for end-to-end recognition:** This method directly recognizes handwritten programs without relying on OCR.

The authors believe their contributions can stimulate further research in CS education and promote accessibility. A dataset of handwritten programs and code is released to support future work. 


summary: This paper introduces CCTVCV, a computer vision model capable of accurately detecting CCTV cameras in images and video frames. The model was trained using 8387 manually annotated images containing 10419 CCTV camera instances, achieving an accuracy of up to 98.7%. The paper outlines the challenges associated with this research, presents a comprehensive comparison of model performance, and discusses potential privacy, safety, and security applications. The authors release the data and code as open-source for further validation and expansion. 


summary: Activity recognition computer vision algorithms can be used to detect the presence of autism-related behaviors, including what are termed "restricted and repetitive behaviors", or stimming, by diagnostic instruments. The limited data that exist in this domain are usually recorded with a handheld camera which can be shaky or even moving, posing a challenge for traditional feature representation approaches for activity detection which mistakenly capture the camera's motion as a feature. To address these issues, we first document the advantages and limitations of current feature representation techniques for activity recognition when applied to head banging detection. We then propose a feature representation consisting exclusively of head pose keypoints. We create a computer vision classifier for detecting head banging in home videos using a time-distributed convolutional neural network (CNN) in which a single CNN extracts features from each frame in the input sequence, and these extracted features are fed as input to a long short-term memory (LSTM) network. On the binary task of predicting head banging and no head banging within videos from the Self Stimulatory Behaviour Dataset (SSBD), we reach a mean F1-score of 90.77% using 3-fold cross validation (with individual fold F1-scores of 83.3%, 89.0%, and 100.0%) when ensuring that no child who appeared in the train set was in the test set for all folds. This work documents a successful technique for training a computer vision classifier which can detect human motion with few training examples

summary: This paper benchmarks various deep learning techniques for violence recognition from video data. The study uses a complex dataset and then tests the impact of increasing the dataset size from 500 to 1,600 videos. The results show an average accuracy improvement of 6% across four models. 


summary: Hermit crabs are important for coastal ecosystems, acting as indicators of marine health. Traditional survey methods are inefficient, so this study proposes a new approach using drones and deep learning to monitor them.  Super-Resolution Reconstruction (SRR) improves image quality, while a modified YOLOv8s network (CRAB-YOLO) increases detection accuracy. The system achieved a mean average precision (mAP) of 69.5%, showing promise for cost-effective and automated hermit crab monitoring. 


summary: This paper proposes a YOLOv8-based framework for layout hotspot detection, aiming to improve the efficiency of design rule checking (DRC). The method utilizes PCA to extract auxiliary information from the layout image and incorporates this information as an additional color channel, improving the accuracy of multi-hotspot detection while reducing false alarms.  Evaluations on ICCAD-2019 benchmark datasets show a precision of 83% and recall of 86%, with a false alarm rate below 7.4%. The augmentation approach also improved detection of "never-seen-before" hotspots by approximately 10%. 


summary: Shortage of labor in fruit crop production has become a significant challenge. To address this, automated machines are being developed to perform tasks like harvesting, pruning, and thinning. This study proposes a machine vision system to estimate crop load in apple orchards using an RGB-D sensor and a YOLOv8-based instance segmentation technique. This system identifies trunks and branches, estimates branch diameter and orientation using Principal Component Analysis, and calculates the limb cross-sectional area (LCSA) which is used for crop-load estimation. The system achieved an RMSE of 2.08 mm for branch diameter estimation and 3.95 for crop-load estimation, demonstrating the potential for automated decision-making in fruit yield management. 


summary: The lack of tamper-proof cattle identification technology in Bangladesh was a significant barrier for livestock insurance, leading to financial hardship for farmers. This paper presents a novel muzzle-based cattle identification system using AI to address this issue. The system utilizes the uniqueness of cattle muzzles, which are akin to human fingerprints. The researchers collected 32,374 images of 826 cattle and employed image processing techniques, including CLAHE and sharpening filters, to enhance the image quality. YOLO algorithm was used for cattle muzzle detection, and FaceNet architecture was used for feature extraction, resulting in a system with an accuracy of 96.489%, F1 score of 97.334%, and a remarkably low false positive rate of 0.098%. This system promises to significantly advance livestock insurance and precision farming. 


summary: Roads in medium-sized Indian towns often have lots of traffic but no (or disregarded) traffic stops, making it difficult for the blind to cross safely. This paper introduces INDRA, a dataset of 104 videos (26,000 frames) recorded from a pedestrian's perspective on Indian roads, annotated with road crossing safety labels and vehicle bounding boxes.  The authors train various classifiers on this data, including convolutional neural networks (CNNs), and develop a novel single-image architecture, DilatedRoadCrossNet, for deployment on the Nvidia Jetson Nano.  This model achieves 79% recall at 90% precision on unseen images. The paper concludes with a description of a wearable road crossing assistant running DilatedRoadCrossNet that can help the blind cross Indian roads in real-time. 


summary: This paper presents a fall detection system using the YOLOv5mu model, achieving a mean average precision (mAP) of 0.995. The system utilizes advanced data augmentation techniques for robustness and adaptability, offering precise, real-time fall detection within smart homes. The authors plan to further enhance the system by incorporating contextual data and exploring multi-sensor approaches for improved performance in diverse environments. 


summary: This research presents a resource-efficient framework for anomaly recognition in surveillance videos. The proposed Temporal based Anomaly Recognizer (TAR) combines a partial shift strategy with a 2D convolutional architecture-based model (MobileNetV2). Experiments on the UCF Crime dataset show an accuracy of 88%, exceeding current state-of-the-art performance. The TAR framework also achieves 52.7% accuracy for multiclass anomaly recognition on the UCF Crime2Local dataset.  The model can handle six camera streams simultaneously in real-time settings without requiring additional resources. 


summary: This paper presents a deep learning framework for real-time vehicle and pedestrian detection. The authors trained and evaluated different versions of YOLOv8 and RT-DETR models on a dataset representing complex urban settings.  YOLOv8 Large emerged as the most effective model, particularly for pedestrian recognition, achieving high precision and robustness. The results, including Mean Average Precision and recall rates, demonstrate the model's potential to significantly improve traffic monitoring and safety. 


summary: Accurately detecting student behavior in classroom videos can aid in analyzing their classroom performance and improving teaching effectiveness. However, the current accuracy rate in behavior detection is low. To address this challenge, we propose the Student Classroom Behavior Detection method, based on improved YOLOv7. First, we created the Student Classroom Behavior dataset (SCB-Dataset), which includes 18.4k labels and 4.2k images, covering three behaviors: hand raising, reading, and writing. To improve detection accuracy in crowded scenes, we integrated the biformer attention module and Wise-IoU into the YOLOv7 network. Finally, experiments were conducted on the SCB-Dataset, and the model achieved an mAP@0.5 of 79%, resulting in a 1.8% improvement over previous results. The SCB-Dataset and code are available for download at: https://github.com/Whiffe/SCB-dataset. 


summary: This paper presents a visual geo-localization system that determines the geographic locations of places (buildings and road intersections) from images without relying on GPS data. The approach combines Scale-Invariant Feature Transform (SIFT) for place recognition, traditional image processing for identifying road junction types, and deep learning using the VGG16 model for classifying road junctions. The most effective techniques have been integrated into an offline mobile application, improving accessibility for users in GPS-denied environments. 


summary: This study evaluated the performance of different configurations of YOLOv8, YOLOv9, and YOLOv10 object detection algorithms for fruitlet detection in commercial orchards. The research compared 17 configurations across the three YOLO versions, with YOLOv9 achieving the highest mAP@50 (0.935) and YOLOv10x demonstrating superior precision and recall. While YOLOv10x had the highest precision (0.908), YOLOv10s had the highest recall (0.872) within the YOLOv10 family. Additionally, YOLOv10b, YOLOv10l, and YOLOv10x exhibited faster post-processing speeds compared to other configurations. 


summary: Autofocus is an important task for digital cameras, yet current approaches often exhibit poor performance. We propose a learning-based approach to this problem, and provide a realistic dataset of sufficient size for effective learning. Our dataset is labeled with per-pixel depths obtained from multi-view stereo, following [10]. Using this dataset, we apply modern deep classification models and an ordinal regression loss to obtain an efficient learning-based autofocus technique. We demonstrate that our approach provides a significant improvement compared with previous learned and non-learned methods: our model reduces the mean absolute error by a factor of 3.6 over the best comparable baseline algorithm. Our dataset and code are publicly available. 


summary: Robotic apple harvesting has become increasingly important due to labor shortages and rising costs. This paper presents DeepApple, a novel deep learning-based apple detection framework. DeepApple utilizes a suppression Mask R-CNN, which adds a suppression branch to suppress non-apple features. The framework was tested on a comprehensive dataset of 'Gala' and 'Blondee' apples under various lighting conditions, achieving a higher F1-score of 0.905 and a detection time of 0.25 seconds per frame. 


summary: This paper tackles the challenge of real-time human action recognition (HAR) on embedded platforms. It identifies Optical Flow (OF) extraction as the bottleneck in HAR pipelines and proposes a novel, efficient motion feature extractor called Integrated Motion Feature Extractor (IMFE). IMFE significantly reduces latency while maintaining high recognition accuracy, enabling a real-time HAR system called RT-HARE. RT-HARE achieves a video frame rate of 30 frames per second on an Nvidia Jetson Xavier NX platform. 


summary: This paper introduces an efficient and layout-independent Automatic License Plate Recognition (ALPR) system based on the YOLO object detector. The system uses a unified approach for license plate detection and layout classification, with post-processing rules to improve recognition results. The system was trained using images from multiple datasets with data augmentation techniques to improve robustness under different conditions. It achieved an average end-to-end recognition rate of 96.8% across eight public datasets, outperforming previous works and commercial systems in some cases. The system also achieves impressive frame-per-second rates on a high-end GPU, enabling real-time performance even with four vehicles in the scene. As a contribution, the authors have manually labeled 38,351 bounding boxes on 6,239 images from public datasets and made the annotations available to the research community. 


summary: This paper introduces a novel approach to evaluating and understanding streaming image understanding, where the goal is to perceive and react to a constantly changing environment.  The authors address the discrepancy between offline evaluation and real-time applications, where the world changes during processing. They introduce a "streaming accuracy" metric that jointly evaluates the entire perception stack at every time instant, accounting for data ignored during computation. This leads to a meta-benchmark that converts any image understanding task into a streaming task. They apply this to object detection and instance segmentation in urban video streams, presenting a dataset with dense annotations. Their analysis reveals an optimal "sweet spot" for latency-accuracy trade-off, the emergence of asynchronous tracking and future forecasting, and the surprising benefit of dynamic scheduling, where "doing nothing" can sometimes minimize latency. 


summary: This paper presents a hybrid method for intelligent identification of moving objects in natural environments, addressing challenges posed by factors like wind, sunlight, and lighting changes. The method combines Gaussian Mixture Model (GMM) for background modeling, background subtraction for foreground extraction, HSV color model and morphological operations for shadow removal, and a back propagation neural network (BPNN) for object recognition. The algorithm effectively eliminates the influence of natural conditions, adapts to dynamic backgrounds, and achieves accurate detection regardless of body pose. Experimental results demonstrate its robustness and real-time performance. 


summary: VisBuddy is a voice-based assistant for the visually challenged that uses image captioning, optical character recognition (OCR), object detection, and web scraping to help with everyday tasks. It combines deep learning and the Internet of Things to offer a cost-effective, all-in-one solution for navigation, object recognition, reading, and more. VisBuddy addresses the limitations of existing assistive technologies like white canes and guide dogs by providing a more comprehensive and user-friendly experience. 


summary: This paper surveys recent advances in low-power and energy-efficient DNN implementations for deploying deep neural networks (DNNs) on resource-constrained Internet-of-Things (IoT) devices. While DNNs are highly effective for computer vision tasks, their computational demands make them challenging to run on IoT devices. This paper explores three major categories of techniques for addressing this challenge: neural network compression, network architecture search and design, and compiler and graph optimizations. The paper reviews both low-power techniques for convolutional and transformer DNNs, highlighting their advantages, disadvantages, and open research problems.  


summary: Gun violence is a serious security issue, and the need for effective gun detection algorithms is paramount. This paper introduces a benchmark called CCTV-Gun, specifically designed to tackle the challenges of detecting handguns in real-world CCTV footage. The benchmark addresses the difficulties of small size, occlusion, and lack of salient features. It includes a cross-dataset evaluation protocol and offers a comprehensive evaluation of various object detection algorithms. The benchmark aims to encourage research and development in this crucial area, ultimately enhancing security. Code, annotations, and trained models are available at https://github.com/srikarym/CCTV-Gun. 


summary: This paper proposes a novel approach to unsupervised camera pose estimation using a compositional re-estimation process. The method first estimates a depth map from an input video sequence and then iteratively estimates camera motion based on the depth map. This approach significantly improves the predicted camera motion, both quantitatively and visually, and resolves the issue of out-of-boundaries pixels in a simple and novel way. It also adapts to other camera pose estimation approaches. Experiments on the KITTI benchmark dataset demonstrate that the proposed method surpasses existing state-of-the-art methods in unsupervised camera ego-motion estimation. 


summary: This paper presents a lightweight pipeline for video-based delivery detection that can run on resource-constrained doorbell cameras. The pipeline uses motion cues to generate activity proposals, followed by classification with a 3DCNN network.  A novel semisupervised attention module is used during training to improve the network's ability to learn robust spatio-temporal features. The paper also introduces an evidence-based optimization objective that allows for quantifying the uncertainty of predictions.  Experimental results on a curated delivery dataset show that this pipeline outperforms existing methods, with significant inference-time performance gains. 


summary: Deep learning has shown great promise in camera localization, but existing single-image techniques are often lacking in robustness, leading to significant errors. This work introduces AtLoc, a novel method using attention mechanisms to focus on geometrically stable objects and features, achieving state-of-the-art performance even when using only a single image. AtLoc excels in common benchmarks, demonstrating its ability to reject dynamic objects and illumination changes. The network's ability to learn to ignore irrelevant features is visualized through saliency maps, illustrating its superior performance in global camera pose regression. The source code is available at https://github.com/BingCS/AtLoc. 


summary: This paper proposes a deep learning model that reconstructs dark visual scenes to clear scenes like daylight and recognizes visual actions for autonomous vehicles. The proposed model consists of two parts, a generative adversarial network (GAN) for scene reconstruction and an object detection module for action recognition. The model achieved 87.3 percent accuracy for scene reconstruction and 89.2 percent accuracy for scene understanding and detection tasks. 


summary: Current computer vision (CV) systems use an image signal processing (ISP) unit to convert raw images to RGB images. This paper proposes a method to invert the ISP pipeline to train CV models on raw images. The paper also proposes an energy-efficient form of analog in-pixel demosaicing that can be coupled with in-pixel CNN computations, and demonstrates the benefits of few-shot learning for ISP-less CV.  This approach results in significant improvements in test accuracy and reduces bandwidth and energy consumption. 


