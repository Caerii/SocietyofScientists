summary: This paper introduces a method for reconstructing the dynamics of computational systems from neural data using recurrent neural networks (RNNs). The authors demonstrate the effectiveness of their approach by applying it to a simulated neural network, and then to experimental data from a rat performing a decision-making task. Their findings suggest that RNNs can accurately capture the dynamics of complex computational systems, offering a powerful tool for studying the neural basis of cognition. 


summary: This paper argues that computational modeling alone is insufficient for understanding consciousness and proposes a framework for grounding computational descriptions in an ontological substrate. This framework allows for estimating the difference in qualitative experience between two systems and has broad applications to computational theories of consciousness. 


summary: The "point-neuron" doctrine, prevalent in both neuroscience and artificial intelligence, simplifies neurons as simple summing units. However, recent evidence suggests that subcellular compartments, particularly dendritic spines, exhibit complex computational abilities. This paper presents a GPU-based computational framework that bridges neuron simulation and artificial intelligence, enabling researchers to study the biophysical properties of neurons and their implications for AI algorithms. The framework utilizes detailed multi-compartment models, incorporating both passive and active biophysical mechanisms, to simulate the behavior of neurons with realistic dendritic morphologies. By combining high-performance computing with biologically realistic models, this framework provides a powerful tool for exploring the computational capabilities of neurons and developing novel AI algorithms inspired by the human brain. 


summary: This paper defines and examines a new kind of computation better adapted to biological systems, called biological computation, a natural adaptation of mechanistic physical computation. The paper focuses on the edge cases of biological computing, such as hearts and flytraps, to illustrate ways that classical computability theory can miss complexities of biology.  It also discusses how the disconnect between human and machine learning can be resolved by reframing computation. 


summary: The original computers were people using algorithms to get mathematical results such as rocket trajectories. After the invention of the digital computer, brains have been widely understood through analogies with computers and now artificial neural networks, which have strengths and drawbacks. This paper defines and examines a new kind of computation better adapted to biological systems, called biological computation, a natural adaptation of mechanistic physical computation. Nervous systems are of course biological computers, and this paper focuses on some edge cases of biological computing, hearts and flytraps. The heart has about the computing power of a slug, and much of its computing happens outside of its forty thousand neurons. The flytrap has about the computing power of a lobster ganglion. This account advances fundamental debates in neuroscience by illustrating ways that classical computability theory can miss complexities of biology. By this reframing of computation, this paper makes way for resolving the disconnect between human and machine learning. 


summary: This paper presents a hardware architecture for efficiently computing large-scale networks of Hodgkin-Huxley (HH) neurons. The architecture is based on the neuron machine hardware architecture, but with multiple computation nodes (hardware neurons) for linear speedup. The proposed architecture supports axonal conduction delay of spikes, short- and long-term plasticity synapses, and floating-point precision HH neurons. The design was implemented on a field-programmable gate array (FPGA) chip and can compute a network with up to 12 million HH neurons and 600 million synapses. 


summary: The connectivity of a neural circuit, together with knowledge of its computational task, enables accurate predictions of the role played by individual neurons in the circuit in the computational task. Using a differentiable model neural network with connectivity based on connectome measurements and unknown single-neuron and single-synapse parameters, we optimized the unknown parameters using deep learning techniques. We applied this approach to the fly visual system and found that our model could accurately predict the neural activity of individual neurons in the system, even when the model was not directly trained on the specific neural activity data. This suggests that connectome data can be used to predict neural activity in a brain region, even when other biological details are unknown, and provides a framework for understanding the relationship between circuit structure and function. 


summary: The diversity and specificity of animal behaviors, as well as their neural correlates, has received attention from diverse areas of study. Recently, machine learning has provided key insights into the mechanics of solving complex behaviors. However, AI frameworks do not capture the emergence of innate behaviors, as conventional models require extensive update rules and training examples to achieve desired fitness on a task. The brain’s innate solutions have long inspired AI techniques, from convolutional neural networks to reinforcement learning, yet neuroevolutionary innovation has not been successfully recapitulated for the systematic discovery of powerful architectures. This study incorporates neurodevelopmental priors into machine learning architectures by utilizing the Genetic Connectome Model (GCM), which provides a network formalization for how interactions between expressed proteins seed synapse formation. The GCM has previously predicted the wiring rules that code for the *C. elegans* gap junction connectome, and similar formulations have been used to infer synaptic interaction rules in *C. elegans* and *Drosophila*, providing mechanistic support to the GCM’s quantitative formulation.  


summary: The human brain, despite being only 2% of our body weight, consumes 20% of the oxygen we take in. This energy fuels our ability to interact with the world and our internal thoughts and feelings. This paper argues that understanding the brain is one of the most challenging tasks of the 21st century. 


summary: This paper presents a novel computational framework for understanding how assemblies of neurons in the brain perform complex cognitive tasks. It introduces the concept of an "Assembly Calculus" which describes how assemblies of neurons interact and operate on information. The paper explains the various operations of this calculus, including "Repeat, Project, and Connect" (RP&amp;C), "projection," "association," "pattern completion," and "merge." It also demonstrates how these operations are enabled by Hebbian plasticity, a fundamental learning mechanism in the brain.  The paper argues that these operations provide a basis for understanding a wide range of cognitive functions, including memory, learning, and reasoning. 


summary: ## Abstract:

The diversity and specificity of animal behaviors, as well as their neural correlates, has received attention from diverse areas of study. While machine learning has provided key insights into solving complex behaviors, current AI frameworks struggle to capture the emergence of innate behaviors. This is because they require extensive training and updates, unlike the hard-coded behaviors present in the developing nervous systems of animals. To tackle this challenge, this study utilizes the Genetic Connectome Model (GCM) to unravel the genomic bottleneck—the ability of development to translate genetic information into specific neural circuits. The GCM aims to reproduce the selection process behind innate behaviors by modeling mechanisms specific to the genetic wiring of neuronal circuits. The findings suggest that developmental priors can guide complex computation, which can be leveraged for building more efficient AI systems that learn from minimal data. 


summary: Even as machine learning surpasses human performance in many tasks, the brain's learning abilities remain unmatched in terms of generality, robustness, and speed. This paper explores the role of sequences in the brain's computational capabilities, building upon a previous model of neural activity (NEMO). It shows how NEMO can naturally capture time as precedence through synaptic weights and plasticity, enabling computations on sequences of neuron assemblies. This allows for memorization of stimulus sequences, creation of scaffolded representations for efficient memorization and recall, and the learning of any finite state machine through appropriate sequence patterns. The model can potentially achieve universal computation through an extension of this mechanism. These findings offer a concrete hypothesis for the basis of the brain's remarkable computational and learning abilities, highlighting the crucial role of sequences in these processes. 


summary: Significant experimental, computational, and theoretical work has identified rich structure within the coordinated activity of interconnected neural populations. An emerging challenge now is to uncover the nature of the associated computations, how they are implemented, and what role they play in driving behavior. We term this computation through neural population dynamics. If successful, this framework will reveal general motifs of neural population activity and quantitatively describe how neural population dynamics implement computations necessary for driving goal-directed behavior. Here, we start with a mathematical primer on dynamical systems theory and analytical tools necessary to apply this perspective to experimental data. Next, we highlight some recent discoveries resulting from successful application of dynamical systems. We focus on studies spanning motor control, timing, decision-making, and working memory. Finally, we briefly discuss promising recent lines of investigation and future directions for the computation through neural population dynamics framework. 


summary: Even as machine learning surpasses human performance in many areas, the brain's learning abilities remain unmatched in their generality, robustness, and speed. This paper presents a model of neural activity capable of capturing time as precedence through synaptic weights and plasticity. This allows the model to perform computations on sequences of stimuli, including memorization, recognition, and the creation of scaffolded representations for more efficient recall. The model can learn any finite state machine and, through an extension of this mechanism, is capable of universal computation. Experiments demonstrate the model's learning capacity, providing a concrete hypothesis for the brain's remarkable abilities to compute and learn. 


summary: This paper proposes a new approach to mapping spike activities with multiplicity, adaptability, and plasticity into bio-plausible spiking neural networks (SNNs). The proposed method, called MAP-SNN, utilizes a novel mapping scheme to convert the continuous spike trains from traditional artificial neural networks (ANNs) into discrete spike trains that can be processed by SNNs. The authors demonstrate that MAP-SNN achieves significant improvements in accuracy and efficiency compared to existing methods, while also maintaining biological plausibility. The proposed approach offers a promising solution for bridging the gap between ANNs and SNNs, paving the way for the development of more powerful and efficient neuromorphic computing systems. 


summary: The paper argues that the next generation of AI, one that is more human-like in its capabilities, requires a greater understanding of how the brain computes. Drawing inspiration from neuroscience, the authors propose a research agenda in "NeuroAI" which aims to integrate insights from neuroscience to develop more robust and adaptable AI systems. The authors point to historical examples of how insights from neuroscience, such as the work of Hubel and Wiesel on visual processing, have fueled breakthroughs in AI. However, they also acknowledge that current AI systems are still far from achieving human-level intelligence, and they believe that major advancements in NeuroAI are crucial to bridge this gap. 


summary: This webpage is not the actual research paper. It only provides navigation links for the IEEE website.  The abstract you are looking for is not included on this page. 


summary: This paper introduces SoftHebb, a new algorithm for training deep neural networks without any feedback, target, or error signals. SoftHebb leverages recent theory for Hebbian learning in soft winner-take-all networks and achieves efficiency by avoiding weight transport, non-local plasticity, time-locking of layer updates, iterative equilibria, and (self-) supervisory or other feedback signals. This approach improves accuracy compared to state-of-the-art bioplausible learning, achieving accuracies of up to 99.4% on MNIST, 80.3% on CIFAR-10, 76.2% on STL-10, and 27.3% on ImageNet. SoftHebb demonstrates that deep learning over few layers may be plausible in the brain and increases the accuracy of bio-plausible machine learning. 


summary: This paper advocates for a concerted effort to reverse engineer the entire nervous system of *Caenorhabditis elegans* using modern machine learning techniques. The authors argue that this endeavor holds significant potential for understanding brain function, advancing artificial intelligence, and developing novel therapeutic approaches. They propose to create a comprehensive model of the worm's nervous system by mapping the input-output functions of each neuron and muscle cell, allowing for the simulation and analysis of brain states and behaviors. This approach offers a unique opportunity to study the workings of a complete nervous system at a level of detail not previously possible. 


summary: Recent experimental observations have shown that the reactivation of hippocampal place cells (PC) during sleep or wakeful immobility depicts trajectories that can go around barriers and can flexibly adapt to a changing maze layout. However, existing computational models of replay fall short of generating such layout-conforming replay, restricting their usage to simple environments, like linear tracks or open fields. In this paper, we propose a computational model that generates layout-conforming replay and explains how such replay drives the learning of flexible navigation in a maze. First, we propose a Hebbian-like rule to learn the inter-PC synaptic strength during exploration. Then we use a continuous attractor network (CAN) with feedback inhibition to model the interaction among place cells and hippocampal interneurons. The activity bump of place cells drifts along paths in the maze, which models layout-conforming replay. During replay in sleep, the synaptic strengths from place cells to striatal medium spiny neurons (MSN) are learned by a novel dopamine-modulated three-factor rule to store place-reward associations. During goal-directed navigation, the CAN periodically generates replay trajectories from the animal's location for path planning, and the trajectory leading to a maximal MSN activity is followed by the animal. We have implemented our model into a high-fidelity virtual rat in the MuJoCo physics simulator. Extensive experiments have demonstrated that its superior flexibility during navigation in a maze is due to a continuous re-learning of inter-PC and PC-MSN

summary: This paper explores a complex-valued neural network (cv-NN) with linear, time-delayed interactions. The cv-NN exhibits complex spatiotemporal dynamics, including "chimera" states, which can be harnessed for computation using a nonlinear readout. The cv-NN demonstrates the ability to implement dynamics-based logic gates, encode short-term memories, and facilitate secure message passing through interactions and time delays. The paper presents a closed-form mathematical expression describing these computations, and it shows that the cv-NN's computations are decodable by living biological neurons using recordings from neocortical slices. These findings suggest that complex-valued linear systems can perform sophisticated computations while being exactly solvable, paving the way for bio-hybrid computing systems seamlessly integrating with other neural networks. 


summary: Utilizing recent advances in machine learning, we introduce a systematic approach to characterize neurons’ input/output (I/O) mapping complexity. Deep neural networks (DNNs) were trained to faithfully replicate the I/O function of various biophysical models of cortical neurons at millisecond (spiking) resolution. A temporally convolutional DNN with five to eight layers was required to capture the I/O mapping of a realistic model of a layer 5 cortical pyramidal cell (L5PC). This DNN generalized well when presented with inputs widely outside the training distribution. When NMDA receptors were removed, a much simpler network (fully connected neural network with one hidden layer) was sufficient to fit the model. Analysis of the DNNs’ weight matrices revealed that synaptic integration in dendritic branches could be conceptualized as pattern matching from a set of spatiotemporal templates. This study provides a unified characterization of the computational complexity of single neurons and suggests that cortical networks therefore have a unique architecture, potentially supporting their computational power. 


summary: This paper demonstrates that neuromorphic computing, a non-von Neumann computing paradigm that emulates the human brain, is Turing-complete. This means that it is capable of performing any computation that a Turing machine can perform, including general-purpose computing.  The authors achieve this by creating a simple model of neuromorphic computing with only two neuron parameters and two synaptic parameters. They then devise neuromorphic circuits for all the µ-recursive functions and operators, which are known to be equivalent to Turing machine computations. 


summary: Understanding the brain's computational basis is critical for advancing computing and treating neurological disorders. Despite advances in artificial intelligence (AI), biological intelligence remains unmatched in its energy efficiency and cognitive capabilities. This comment discusses how theoretical computing frameworks that bridge top-down algorithm and bottom-up physics approaches can guide the development of neural computing technologies. This balanced perspective is also necessary to incorporate the neurobiological details crucial for describing the computational disruptions underlying mental health and neurological disorders. 


summary: ## Abstract

While long-term potentiation and depression are often implicated in memory encoding, recent research suggests that memory formation can occur without persistent changes in synaptic strength. This paper proposes a computational passage-of-time model of the cerebellar Purkinje cell, the sole output neuron of the cerebellar cortex, in eyeblink conditioning. This model focuses on the Purkinje cell's ability to learn a temporally precise pause response that mirrors the conditioned response (CR) observed in eyeblink conditioning. The model incorporates the known physiological properties of Purkinje cells, including their high tonic firing rates, their ability to receive input from a vast number of parallel fibers (CS input) and a single climbing fiber (US input), and their role in inhibiting a motor pathway that controls eyelid movements. The model suggests that the Purkinje cell's ability to learn a temporally precise pause response arises from its internal pacemaker mechanism and its unique synaptic connectivity, allowing it to represent the temporal relationship between the CS and the US. The model provides a novel explanation for how the cerebellum contributes to the learning and storage of temporally precise memories. 


summary: Can the powerful 'backpropagation of error' (backprop) reinforcement learning algorithm be formulated in a manner suitable for implementation in neural circuitry? The primary challenge is to ensure that any candidate formulation uses only local information, rather than relying on global (error) signals, as in orthodox backprop. Recently several algorithms for approximating backprop using only local signals, such as predictive coding and equilibrium-prop, have been proposed. However, these algorithms typically impose other requirements which challenge biological plausibility: for example, requiring complex and precise connectivity schemes (predictive coding), or multiple sequential backwards phases with information being stored across phases (equilibrium-prop). Here, we propose a novel local algorithm, Activation Relaxation (AR), which is motivated by constructing the backpropagation gradient as the equilibrium point of a dynamical system. Our algorithm converges robustly and exactly to the correct backpropagation gradients, requires only a single type of neuron, utilises only a single backwards phase, and can perform credit assignment on arbitrary computation graphs. We illustrate these properties by training deep neural networks on visual classification tasks, and we describe simplifications to the algorithm which remove further obstacles to neurobiological implementation (for example, the weight-transport problem, and the use of nonlinear derivatives), while preserving performance. 


summary: ## Abstract:

Deep learning models have been highly successful in solving complex problems, but often require significant memory and are difficult to interpret. This paper discusses how bio-inspired architectures, taking inspiration from biological nervous systems like the visual cortex and insect brains, can offer an alternative to traditional deep learning models. These bio-inspired architectures can achieve high performance while significantly reducing memory requirements and potentially leading to more interpretable models. Examples are given of how these bio-inspired approaches have been used in applications like image processing, machine translation, and data generation. 


summary: This research explores the development of a full-scale scaffold model of the human hippocampus CA1 area, a key region for memory and learning. The lack of sufficient cellular data on human brain structure and function has been a significant barrier to building accurate models. The paper addresses this challenge by proposing a novel approach that leverages existing data and advanced simulation techniques to create a detailed and computationally efficient representation of the CA1 region. The model incorporates detailed neuronal morphologies, electrophysiological properties, and synaptic characteristics, all derived from existing experimental data. This approach, coupled with innovative co-simulation technologies, allows for the exploration of human brain activity at a cellular resolution, paving the way for a deeper understanding of human brain function and its potential dysfunctions. 


summary: The Neural Engineering Framework (NEF) is a method for implementing high-level algorithms that are constrained by low-level neurobiological details. This paper expands on the NEF's core principles by incorporating more biological details and applying it to new tasks. The authors demonstrate how to extend the NEF to include complex spatiotemporal tuning curves and use this approach to produce functional computational models of grid cells, time cells, path integration, sparse representations, probabilistic representations, and symbolic representations in the brain. 


summary: Simulations of neural networks can be used to study the direct effect of internal or external changes on brain dynamics. However, some changes are not immediate but occur on the timescale of weeks, months, or years. This paper models a coupled network of human brain regions with a modified Wilson-Cowan model representing dynamics for each region and with synaptic plasticity adjusting connection weights within and between regions. Using strategies ranging from different models for plasticity, vectorization and a different differential equation solver setup, the authors achieved one second runtime for one second biological time. 


summary: This paper explores the convergence of brain sciences and computing. It highlights the exponential growth of data related to brain research, the availability of vast computational power, and the success of machine learning models inspired by neural architectures. The paper argues that these advancements create unprecedented opportunities to forge connections between these two fields, potentially leading to a deeper understanding of the brain's structure and function. 


summary: This article aims to provide a computational understanding of the brain, which is a complex and powerful system. While research in computational neuroscience is rapidly advancing, it can be challenging for scientists from other fields to engage in this research due to the abundance of facts and models. This paper offers background information, up-to-date references on data and models in neuroscience, and open problems that could provide opportunities for computer scientists to explore the field of brain computation. 


summary: The paper argues that the brain can be considered a literal computer, not just metaphorically. It proposes empirical criteria for what constitutes a physical computational system, going beyond theoretical computer science.  These criteria are used to show how the brain, likely an analog computer, can be considered a computer, making this claim both informative and falsifiable. The paper highlights the distinction between analog and digital computation, emphasizing the importance of understanding these differences for neuroscientists, cognitive scientists, and theoretical computer scientists. 


summary: Spiking neural networks (SNN) are energy-efficient alternatives to traditional neural networks, but training them is difficult due to the hard-thresholding and discontinuities at spike times. Existing work often assumes exact gradients for SNNs do not exist and focuses on approximations. This paper demonstrates that exact gradients for SNNs do exist, and proposes a novel training algorithm called forward propagation (FP) that computes these gradients. FP exploits the causality structure between spikes, allowing for parallelized computation and offering insights into why other methods like Hebbian learning and surrogate gradients may perform well. 


summary: Brain-inspired computing, leveraging neuroscientific principles for solving cognitive tasks, is emerging as a potential solution for the challenges faced by deep learning. This paper argues that employing probabilistic neuromorphic systems for temporal information encoding could address these challenges. The authors propose using superparamagnetic tunnel junctions to create a new generation of brain-inspired computing that combines insights from computational neuroscience.  Hardware-algorithm co-design analysis demonstrates 97.41% accuracy of a state-compressed 3-layer spintronics enabled stochastic spiking network on the MNIST dataset with high spiking sparsity due to temporal information encoding. 


summary: This paper investigates the possibility of synthetic neuromorphic computing in living cells. It highlights limitations of current digital and analog computing paradigms in synthetic biology, particularly in scaling and dealing with biological noise. The authors propose that a neuromorphic approach, inspired by the nonlinearity and robustness of natural biological systems, could provide an alternative for building complex, scalable genetic networks in living cells. 


summary: This paper explores the diverse timescales of neural activity and their implications for brain function. It synthesizes three computational approaches to study these timescales: 1) data analysis methods to capture different timescales from various recording modalities, 2) computational models to explain the emergence of these timescales, and 3) task-optimized machine learning models to uncover their functional relevance. The authors argue that combining these computational approaches with empirical findings will provide a holistic understanding of the relationship between brain structure, dynamics, and behavior. 


summary: Recent work suggests goal-driven training of neural networks can be used to model neural activity in the brain. While response properties of neurons in artificial neural networks bear similarities to those in the brain, the network architectures are often constrained to be different. Here we ask if a neural network can recover both neural representations and, if the architecture is unconstrained and optimized, the anatomical properties of neural circuits. We demonstrate this in a system where the connectivity and the functional organization have been characterized, namely, the head direction circuits of the rodent and fruit fly. We trained recurrent neural networks (RNNs) to estimate head direction through integration of angular velocity. We found that the two distinct classes of neurons observed in the head direction system, the Ring neurons and the Shifter neurons, emerged naturally in artificial neural networks as a result of training. Furthermore, connectivity analysis and in-silico neurophysiology revealed structural and mechanistic similarities between artificial networks and the head direction system. Overall, our results show that optimization of RNNs in a goal-driven task can recapitulate the structure and function of biological circuits, suggesting that artificial neural networks can be used to study the brain at the level of both neural activity and anatomical organization. 


summary: This paper extends previous work by the authors, arguing that the brain may act like a quantum computer. They propose that the difference in voltage between two axons provides an environment for ions to undergo spatial superposition. This allows for the encoding of information during processing by the interaction of the quantum state of the ions with the controlling potential.  Upon decoherence (measurement), the final spatial state of the ions is determined and reset by the next impulse initiation time.  The synchronized action of multiple tracts leads to a complete quantum computing circuit. The authors estimate that up to 15 million quantum states could be prepared and evolved every millisecond in the corpus callosum, exceeding the capabilities of current quantum computers. 


summary: Obtaining computational models for brain function allows us to understand brain activity in detail, leading to better treatments for neurological illnesses. This study used the Brian simulator to create a cortical model in Python, focusing on connection weights and their impact on cortical neuron connectivity. The paper examines synchronization between neuronal groups connected across layers and investigates the influence of input noise on the model's internal neuronal networks and their synchronization. This research highlights the importance of studying synchronization in simulated cortical models for understanding neuronal behavior and testing hypotheses. 


summary: Dragonflies are adept aerial predators capable of selectively attending to a single target within a swarm. This behavior likely relies on small target motion detector (STMD) neurons, which show enhanced responses to targets moving on continuous paths, known as facilitation.  This study combines detailed morphological data with computational modeling to investigate whether dendritic morphology and nonlinear properties of NMDA receptors contribute to this facilitation. A hybrid model of dragonfly optic lobe neurons successfully reproduced the observed facilitation, including a "spotlight" of sensitivity near the target's last known location. However, the model did not replicate a traveling wave of facilitation. The study highlights the importance of dendritic density in enhancing non-linear facilitation and supports a potential role for NMDA receptors in target tracking. 


summary: Spiking neural networks (SNNs) are increasingly used in neuroscience research and artificial intelligence. Simulating large-scale SNNs poses a challenge due to their computational complexity. This paper presents a novel framework for parameter calibration and real-time simulation of large-scale SNNs using the GeNN and NEST simulators. The framework facilitates rapid exploration of different network architectures and parameter configurations, enabling efficient tuning for desired computational properties. The authors demonstrate the framework by simulating a biologically realistic model of the hippocampus, highlighting the potential for applying this approach to a range of neuroscience and machine learning applications. 


summary: This paper proposes a novel computational model for life-long learning in artificial neural networks. The model is inspired by the biological processes in the hippocampus, particularly the dopamine-mediated novelty detection and lateral inhibitory plasticity. The authors argue that the model can mitigate catastrophic forgetting, a common problem in artificial neural networks where learning new information leads to forgetting old information. The model incorporates a novelty signal, similar to the dopamine signal in the hippocampus, which modulates the learning process and protects existing memories. This approach, they claim, is a biologically plausible mechanism for achieving life-long learning capabilities in artificial neural networks. 


summary: This paper explores how neural activity represents probability distributions.  It first establishes a common language for defining various hypotheses about this representation. Then, it compares three prominent proposals: Probabilistic Population Codes (PPCs), Distributed Distributional Codes (DDCs), and Neural Sampling Codes (NSCs), highlighting their similarities and differences. The paper reviews empirical evidence supporting these proposals, considering whether alternative theories could also explain the data. Finally, it addresses challenges in resolving the debate and proposes potential directions for future research combining theory and experiments. 


summary: Multiple studies have shown that dendrites enable some neurons to perform linearly non-separable computations. This research focuses on cells with an extended dendritic arbor where voltage can vary independently, turning dendritic branches into local non-linear subunits. However, many neurons have modest dendritic trees and are electrically compact. This paper demonstrates that even a single layer neuron with interacting synapses has more computational capacity than without, meaning all neurons can potentially implement linearly non-separable computations. 


summary: This document doesn't contain an abstract.  It seems to be an introduction to a paper on the computational power of the human brain.  It goes into great detail about the differences between analog and digital computation, and how the human brain may use a combination of both. 


summary: Sleep is essential for restoring optimal computational regimes in cortical networks. Here, we show that sleep deprivation disrupts the balance of excitation and inhibition (E/I) in the cortex. This leads to a decrease in neuronal firing rates and an increase in synaptic plasticity. The restoration of E/I balance during sleep is critical for memory consolidation and other cognitive functions. Our findings provide new insights into the role of sleep in brain function and highlight the importance of maintaining sufficient sleep for optimal cognitive performance. 


summary: ## Abstract:

It has been observed that hippocampal place cells (PCs) exhibit spontaneous and sequential firing activity during rest, resembling the patterns observed during movement. Studies have primarily focused on simple environments like linear tracks or open fields. Recent research, however, has shown that replay trajectories in a reconfigurable maze conform to the spatial layout and flexibly adapt to new configurations. This layout-conforming replay is crucial for understanding how place cell activity supports flexible navigation learning in mazes, a longstanding open question in computational neuroscience.

Existing models of hippocampal replay rely on continuous attractor networks (CAN) with pre-configured synaptic strengths based on Euclidean distances, leading to replay trajectories that disregard maze layouts. Additionally, these models lack mechanisms for integrating replay activity with downstream circuits like the striatum for higher-level functions. 

This paper presents a computational model that generates layout-conforming replay and explains how it facilitates flexible navigation learning. The model incorporates a firing field function based on shortest path distances, a Hebbian-like rule for learning inter-PC synaptic strength, and a CAN with feedback inhibition from hippocampal GABAergic interneurons. The model successfully reproduces layout-conforming replay during both rest and goal-directed navigation. Furthermore, a novel three-factor learning rule is proposed for learning synaptic strengths from place cells to striatal medium spiny neurons (MSNs), enabling reward-based learning and navigation planning. 


summary: This paper presents a large-scale biophysically detailed model of the somatosensory thalamocortical circuits in NetPyNE.  The model, based on the Blue Brain Project's rat S1 model, incorporates over 31,000 neurons of 55 layer-specific morphological and 207 morpho-electrical neuron subtypes, and 37 million synapses capturing layer- and cell type-specific connectivity patterns and synaptic dynamics. The authors implemented the original model in NetPyNE, a python package that provides a high-level interface to the NEURON simulator, to make it more accessible, easier to scale, modify, and extend. This new implementation allows for easier connection to other models developed within the NetPyNE platform, such as the authors' previous primary motor cortex model.  The model can help researchers investigate the neural coding underlying touch perception and how disruptions in these circuits contribute to neurological disorders.


summary: This paper explores the challenges of understanding the brain's computational processes, highlighting the limitations of both top-down algorithmic and bottom-up hardware-centric approaches. The authors advocate for a balanced perspective that integrates both approaches, arguing that such a framework is crucial for developing effective neural computing technologies and addressing the computational disruptions underlying neurological disorders. 


summary: ## Abstract:

Connectomes, complete wiring diagrams of neural circuits, are increasingly available, but it remains unclear how to extract useful information from them. Here, we show that connectivity inferred from neural activity in recurrent networks can exhibit systematic errors that depend on the firing rates of the neurons. We demonstrate this effect both in theoretical networks and in real neural data. Furthermore, we show that errors in connectivity inferred from activity may be used to learn about hidden network structure. This approach, which we call **connectivity inference**, allows us to identify previously unknown connections and to predict the presence of specific neuron types. We conclude that connectivity inference can be a powerful tool for studying neural circuits and that systematic errors in inferred connectivity provide insights into the underlying network structure.


summary: This paper presents a new framework for modelling auditory sensory cells and synapses using convolutional neural networks (DNNs).  The DNN model architecture is designed to mimic the biophysical properties of analytical models, but with significantly faster computation times. The authors demonstrate that their model can be used to simulate a variety of existing analytical models and can be applied to other neuron and synapse types, accelerating the development of large-scale brain networks and DNN-based treatments for neurological disorders. 


summary: This paper proposes recurrent spiking neural networks trained to perform multiple cognitive tasks. The authors argue that viewing neurocognitive activity as computational processes through dynamics allows for a more comprehensive understanding of the brain. The networks are trained by input-output examples and then reverse engineered to identify the dynamic mechanisms underlying their performance. The paper explores the relationship between multitasking and spiking in a single system, offering insights into the principles of neural computation. 


summary: The paper develops a computational model for grid map representations in neural populations based on neural field theory. This model simulates the spatial firing patterns of grid cells observed in the hippocampus of rodents and is inspired by the spatial representation properties of a topological map. The model successfully replicates the key features of grid cells, including their hexagonal firing patterns, multiple spatial scales, and phase precession during locomotion. The model also exhibits a robust response to environmental changes and noise. 


summary: This work examines the computational potential of a network relying solely on synapse modulation, called the multi-plasticity network (MPN), for information processing. The MPN, unlike traditional recurrent neural networks (RNNs), has no recurrent connections, allowing for a study of computational capabilities driven solely by synaptic modulations. The paper explores the neural population dynamics of the MPN trained on integration-based tasks and finds its dynamic structure differs significantly from RNNs, leading to improved performance on several neuroscience-relevant tests.  The MPN’s computational capabilities in various neuroscience tasks are comparable to networks with recurrent connections, demonstrating the potential of computing with synaptic modulations and providing insights for identifying such computations in brain-like systems. 


summary: Normative models of neural computation offer simplified yet lucid mathematical descriptions of murky biological phenomena. Previously, online Principal Component Analysis (PCA) was used to model a network of single-compartment neurons accounting for weighted summation of upstream neural activity in the soma and Hebbian/anti-Hebbian synaptic learning rules. However, synaptic plasticity in biological neurons often depends on the integration of synaptic currents over a dendritic compartment rather than total current in the soma. Motivated by this observation, we model a pyramidal neuronal network using online Canonical Correlation Analysis (CCA). Given two related datasets represented by distal and proximal dendritic inputs, CCA projects them onto the subspace which maximizes the correlation between their projections. First, adopting a normative approach and starting from a single-channel CCA objective function, we derive an online gradient-based optimization algorithm whose steps can be interpreted as the operation of a pyramidal neuron. To model networks of pyramidal neurons, we introduce a novel multi-channel CCA objective function, and derive from it an online gradient-based optimization algorithm whose steps can be interpreted as the operation of a pyramidal neuron network including its architecture, dynamics, and synaptic learning rules. Next, we model a neuron with more than two dendritic compartments by deriving its operation from a known objective function for multi-view CCA. Finally, we confirm the functionality of our networks via numerical simulations. Overall, our work presents a simplified but informative abstraction of learning in a pyramidal neuron network, and demonstrates how such networks can integrate multiple sources of inputs. 


summary: ## Abstract: 

Deep learning has achieved remarkable success in artificial intelligence, and is starting to be applied to neuroscience. Deep learning models have the potential to explain how brains perform complex cognitive tasks, and can be used to predict neural activity and behavior. This article outlines key challenges and opportunities for the application of deep learning to neuroscience, and discusses the potential impact of this nascent field. 


summary: The error-backpropagation (backprop) algorithm is the most common solution to the credit assignment problem in artificial neural networks. However, it is unclear whether the brain could use a similar strategy to modify its synapses. This paper introduces a new model, Bursting Cortico-Cortical Networks (BurstCCN), that addresses this gap. BurstCCN uses burst multiplexing and short-term plasticity to propagate backprop-like error signals within deep cortical networks. The model can effectively backpropagate errors through multiple layers using a single-phase learning process and is capable of learning complex image classification tasks. 


summary: Synaptic plasticity enables cortical circuits to learn new tasks and adapt to changing environments. This paper explores how cortical circuits use plasticity to acquire functions like decision-making or working memory. The authors describe how to train recurrent neural networks in tasks similar to those used to train animals in neuroscience laboratories and how computations emerge in the trained networks. Surprisingly, artificial networks and real brains can use similar computational strategies. The paper discusses ways to build neural network models to investigate how the brain could solve cognitive tasks, focusing on the biological plausibility of the models and techniques used to formulate them. 


summary: Assemblies are large populations of neurons believed to imprint memories, concepts, words, and other cognitive information. This paper identifies a repertoire of operations on assemblies, which correspond to properties of assemblies observed in experiments, and can be shown, analytically and through simulations, to be realizable by generic, randomly connected populations of neurons with Hebbian plasticity and inhibition. Assemblies and their operations constitute a computational model of the brain, which the authors call the Assembly Calculus, occupying a level of detail intermediate between the level of spiking neurons and synapses and that of the whole brain. The resulting computational system can be shown, under assumptions, to be, in principle, capable of carrying out arbitrary computations. The authors hypothesize that something like the Assembly Calculus may underlie higher human cognitive functions such as reasoning, planning, and language. In particular, they propose a plausible brain architecture based on assemblies for implementing the syntactic processing of language in cortex, which is consistent with recent experimental results. 


summary: To understand the function of the neocortex, which is a hierarchical distributed network, it is useful giving meaning to the signals transmitted between these areas from the computational viewpoint. This paper investigates the types of signals exchanged between neocortical areas, taking into account biological constraints, and employing theories such as predictive coding, reinforcement learning, representation emulation theory, and BDI logic as theoretical starting points. Based on the anatomical knowledge of the neocortex and thalamus, the pathways connecting the areas are organized and summarized as three corticocortical pathways and two thalamocortical pathways. This paper proposes a hypothesis that gives meaning to each type of signal transmitted in the different pathways in the neocortex, from the viewpoint of their functions. 


summary: This paper explores the symbiotic relationship between neuroscience and artificial intelligence, highlighting how these seemingly disparate fields can collaborate to drive new discoveries. Neuroscientists can leverage artificial intelligence to test hypotheses that are challenging or impossible to test through traditional methods, while AI experts can gain insights into information processing by studying brain function.  The paper specifically mentions the study of the tripartite synapse between neurons and astrocytes as an example of this collaborative approach. 


summary: This paper investigates how basic membrane characteristics and synaptic delay affect information transfer and representation in a spiking neural network. The authors build upon the Brendel model, which teaches a spiking network to follow and decode arbitrary noise patterns, after training and connection fixation. By analyzing a simple system with minimal non-linearities, they aim to capture extra-synaptic effects on information flow. The study examines how variations in membrane capacitance, resistance, and synaptic delay influence signal transfer and the ability of the network to represent information. 


summary: This paper presents a computational model that simulates the dynamics of human brain states, specifically wakefulness and slow-wave sleep. Using a network of adaptive exponential integrate-and-fire neurons, constrained by the human connectome and informed by local circuit parameters, the model captures essential characteristics of both states. The model relies on previous work at the mesoscale level, where biologically-relevant activity states were modeled in networks of spiking neurons.  The authors derive mean-field models from these networks to take into account second-order statistics and capture macroscopic dynamics. The model demonstrates that variations in spike-frequency adaptation, when coupled with the human connectome, can produce global dynamics mimicking human wakefulness and slow-wave sleep. 


summary: This paper presents CerebelluMorphic, a large-scale cerebellar network model for supervised learning, with a corresponding neuromorphic architecture. The model, with approximately 3.5 million neurons, incorporates 3411k granule cells, significantly scaling up state-of-the-art neuromorphic designs. This large-scale model mimics biological divergence/convergence ratios more realistically and is validated by replicating cerebellar dynamics during the optokinetic response on a reconfigurable neuromorphic system. The architecture is also used to analyze Purkinje cell synchronization, demonstrating the impact of mossy fiber firing rates on their resonance dynamics. Real-time operation is achieved with a high synaptic event rate and significantly improved throughput. The work provides theoretical and engineering insights for brain-inspired computing and cerebellar learning research. 


summary: Neural systems, from biological brains to artificial neural networks, are powerful computers that leverage unique computational abilities. However, the lack of a concrete, low-level neural machine code hinders our understanding of their relationship with conventional silicon computers. This paper introduces a framework for a neural machine code and programming based on the Reservoir Computer (RC), a recurrent neural network (RNN) capable of memory storage, manipulation, prediction, and control. The framework allows for the implementation of basic computing functions within the RC, paving the way for harnessing the full potential of neural computation. 


summary: Several studies attempt to address the biological implausibility of backpropagation (BP) for neural networks, but their validity remains controversial due to unsolved issues. This paper proposes a hypothetical framework, consisting of a new microcircuit architecture and Hebbian learning rules, that realizes random backpropagation solely based on neuroscientific mechanisms. The framework involves three types of cells and two types of synaptic connectivity, enabling error signal propagation through local feedback connections and training multi-layered spiking neural networks with a globally defined spiking error function. Hebbian learning in local compartments updates synaptic weights, achieving supervised learning in a biologically plausible manner. The framework is interpreted from an optimization perspective, showing its equivalence to sign-concordant feedback alignment. Benchmarked on MNIST and CIFAR10 datasets, the proposed framework demonstrates BP-comparable accuracy. 


summary: ## Abstract:

Spiking neural networks (SNNs) are powerful tools for modeling brain function, but their large-scale implementation remains challenging. We present an automated method for customizing SNN models to match neuronal population activity recorded in vivo. Our approach utilizes a genetic algorithm to optimize network parameters, such as connectivity, synaptic strengths, and neuron properties, while minimizing the discrepancy between simulated and observed activity patterns. We demonstrate this method on a model of the primary visual cortex (V1), successfully adapting the SNN to reproduce the firing rates and correlations of V1 neurons in response to natural stimuli. This automated approach opens new avenues for studying large-scale SNNs and bridging the gap between theoretical models and real-world neural activity. 


summary: To date, most dendritic studies have focused on the apical zone of pyramidal two-point neurons (TPNs) receiving only feedback (FB) connections from higher perceptual layers and using them for learning. Recent studies suggest that apical input (context), coming from feedback and lateral connections, is far more diverse and has greater implications for ongoing learning and processing in the brain. This context, which includes signals from neighboring cells, other parts of the brain, and overall coherent information across the network, can amplify or suppress the transmission of feedforward (FF) signals, depending on coherence. Context-sensitive (CS)-TPNs flexibly integrate this context with the FF somatic current, amplifying the signal when FF and context are coherent and attenuating it otherwise. This flexible integration enables the propagation of more coherent signals, making learning faster and requiring fewer neurons. Similar behavior is observed in artificial networks, suggesting a previously overlooked role of context-sensitive dendrites in efficient information processing. 


summary: This paper presents the backpropagation-based recollection hypothesis, which proposes that weak, fast-fading action potentials traveling backward from post to pre-synaptic neurons mediate memory recall. This mechanism is suggested to underlie explicit cue-based memory retrieval, imagination, language understanding, and associating names with stimuli. The hypothesis argues that highly invariant neurons uniquely responding to specific stimuli (e.g., an image of a cat) generate these backward signals, reactivating the same neuron populations involved in perception, thus recreating an experience "offline." The paper reviews literature supporting the hypothesis' assumptions, including the existence of backpropagating signals, and presents simulations using spiking neural networks demonstrating the computational feasibility of the proposed mechanism. The authors believe this hypothesis represents a paradigm shift, offering new interpretations of language acquisition, memory encoding and retrieval, and reconciling seemingly opposed views on sparse coding and distributed representations. 


summary: This paper proposes a new type of model for understanding neural computation called parallel, recurrent cascade models. These models represent individual neurons as cascades of parallel linear and non-linear operations, similar to multi-layer recurrent artificial neural networks. The authors argue that these models can capture sub-cellular phenomena, like the interaction between sodium, calcium, and NMDA spikes, which traditional cascade models cannot.  Moreover, due to their mathematical tractability, these models can be integrated into multi-layered artificial neural networks for complex tasks. The paper explores potential implications for artificial intelligence and concludes that parallel, recurrent cascade models offer a unifying framework for understanding single-cell computation. 


summary: Artificial Neural Networks (ANNs) inspired by biology are increasingly used to model behavioral and neural data, an approach called neuroconnectionism. While lauded as the best models of brain information processing, ANNs are also criticized for failing to account for basic cognitive functions. This paper argues that assessing the promise of neuroconnectionism based on current ANNs is the wrong approach. Instead, it draws inspiration from the philosophy of science, particularly Lakatos' work on scientific research programs, which suggests that the core of these programs is not directly falsifiable and should be assessed by its ability to generate novel insights. The paper proposes neuroconnectionism as a large-scale research program focused on ANNs as a computational language for expressing falsifiable theories about brain computation. It describes the program's core, its computational framework and tools for testing neuroscientific hypotheses, and reviews past and present neuroconnectionist projects, arguing that the program is highly progressive in generating new insights into brain function. 


summary: This paper compares biologically inspired learning algorithms for training spiking neural networks (SNNs) to perform motor control in the CartPole reinforcement learning problem. The authors evaluate the efficiency of two learning approaches: spike-timing dependent reinforcement learning (STDP-RL) and a novel variation of evolutionary strategies (EVOL) adapted from non-spiking neural networks.  The study explores the dynamics of these algorithms and compares their performance in terms of training efficiency and network dynamics. The authors demonstrate that SNNs with realistic neuron models, trained using these biologically inspired learning mechanisms, can effectively solve RL problems and offer insights into neural circuit dynamics. 


summary: This study proposes a simplified, adaptive, power-efficient BCI spike sorting VLSI architecture, "Zydeco-Style," for BCI Implants empowered by a low-power BLE technology for wireless communication. The proposed architecture is accurate according to the industrial standards and follows all the human body implant limitations. 


summary: This paper investigates the potential of using astrocytes, a type of glial cell, for self-repair in neuromorphic hardware systems based on Spiking Neural Networks (SNNs).  The work examines the role of astrocytes in regulating synaptic strength and explores how this regulation can be leveraged to mitigate hardware faults. The authors present a bio-morphic model of astrocytic regulation and demonstrate its effectiveness in self-repairing faults in neuromorphic hardware. Results show improved accuracy and repair convergence for unsupervised learning tasks on the MNIST and F-MNIST datasets. The implementation source code and trained models are available on GitHub. 


summary: Neuromorphic computing is a key future technology for the computing industry, but it has yet to achieve its promise. This article presents a strategy, framed by market and policy pressures, for overcoming the current technological and cultural hurdles to realizing its full impact across technology. Achieving the full potential of brain-derived algorithms as well as post-complementary metal-oxide-semiconductor (CMOS) scaling neuromorphic hardware requires appropriately balancing the near-term opportunities of deep learning applications with the long-term potential of less understood opportunities in neural computing. 


summary: ## Abstract

Understanding the neural basis of human consciousness is a major challenge in neuroscience. Consciousness is supported by dynamic brain activity, but how this arises from the brain's fixed network of connections (the connectome) remains unclear. Existing studies typically rely on correlation measures, neglecting the asymmetric relationship between structure and function and the multi-scale nature of both. This study uses connectome harmonic decomposition (CHD), which generalizes the Fourier transform to the brain's network structure, to explore this relationship across multiple scales. CHD reveals distributed patterns of structure-function dependence that change dramatically with altered states of consciousness. These findings suggest that the interplay of structure and function across scales, captured by CHD, is crucial for consciousness and its disruption. 


summary: This paper presents a hybrid machine learning and computational neuroscience approach to transform analytical sensory neuron and synapse models into artificial neural network (ANN) units. The ANN architecture offers a simulation run-time improvement factor of 70 on CPUs and 280 on GPUs, while maintaining the biophysical properties of the original analytical models. The authors focus on auditory sensory neurons and synapses, demonstrating the ANN model's generalization to various existing analytical models. This approach can be adapted for other neuron and synapse types, facilitating the development of large-scale brain networks and ANN-based treatments of pathological systems. 


summary: Computational neuroscience aims to develop methods for manipulating brain states through pharmacological or electromagnetic interventions. However, a clear definition of brain states and a thorough understanding of their underlying dynamics remain elusive. This review presents a framework for defining brain states based on their functional hierarchical organization and explores recent progress in computational models that incorporate neuronal and neurotransmitter interactions. These models allow for in silico prediction and design of interventions to rebalance brain states in disease. 


summary: Sophisticated machine learning struggles to transition onto battery-operated devices due to the high-power consumption of neural networks. Researchers have turned to neuromorphic engineering, inspired by biological neural networks, for more efficient solutions. While previous research focused on artificial neurons and synapses, an essential component has been overlooked: dendrites. Dendrites transmit inputs from synapses to the neuron's soma, applying both passive and active transformations. However, neuromorphic circuits replace these sophisticated computational channels with metallic interconnects. In this study, we introduce a versatile circuit that emulates a segment of a dendrite which exhibits gain, introduces delays, and performs integration. We show how sound localisation - a biological example of dendritic computation - is not possible with the existing passive dendrite circuits but can be achieved using this proposed circuit. We also find that dendrites can form bursting neurons. This significant discovery suggests the potential to fabricate neural networks solely comprised of dendrite circuits. 


summary: Cybernetical neuroscience is a new field that uses methods from cybernetics, the science of control and communication, to study mathematical models used in computational neuroscience. The field also explores the practical applications of the results obtained from studying these models. This paper outlines the main tasks, methods, and some results of cybernetic neuroscience. 


summary: We introduce a novel, biologically plausible local learning rule that provably increases the robustness of neural dynamics to noise in nonlinear recurrent neural networks with homogeneous nonlinearities. Our learning rule achieves higher noise robustness without sacrificing performance on the task and without requiring any knowledge of the particular task. The plasticity dynamics—an integrable dynamical system operating on the weights of the network—maintains a multiplicity of conserved quantities, most notably the network's entire temporal map of input to output trajectories. The outcome of our learning rule is a synaptic balancing between the incoming and outgoing synapses of every neuron. This synaptic balancing rule is consistent with many known aspects of experimentally observed heterosynaptic plasticity, and moreover makes new experimentally testable predictions relating plasticity at the incoming and outgoing synapses of individual neurons. Overall, this work provides a novel, practical local learning rule that exactly preserves overall network function and, in doing so, provides new conceptual bridges between the disparate worlds of the neurobiology of heterosynaptic plasticity, the engineering of regularized noise-robust networks, and the mathematics of integrable Lax dynamical systems. 


summary: This paper describes the authors' experience using Google Compute Platform (GCP) with Slurm to run large-scale simulations of a detailed model of the brain motor cortex circuits, simulating over 10,000 biophysically detailed neurons and 30 million synaptic connections. The authors outline best practices and solutions to issues encountered during the process, and present preliminary results from simulations run on GCP. 


summary: The field of basal cognition investigates how adaptive, context-specific behavior occurs in non-neural biological systems. This paper focuses on the idea that non-neural bioelectric networks can support computation, which is crucial for understanding embryogenesis and regeneration. The authors model a minimal Bio-Electric Network (BEN) that utilizes the general principles of bioelectricity (electrodiffusion and gating) to show it can compute. They characterize BEN behaviors ranging from elementary logic gates to pattern detectors, demonstrating that logic can manifest in bidirectional, continuous, and relatively slow bioelectrical systems. This research suggests novel bioengineering approaches for regenerative medicine and synthetic biology, as well as new machine learning architectures. 


summary: This paper presents a brain-inspired computational model for action recognition.  The model incorporates novel, biologically plausible mechanisms for spiking neural networks, focusing on learning spatio-temporal patterns.  This approach aims to bridge the gap between the brain's capabilities and action recognition tasks by integrating key biological principles into the computational framework. The model was tested against other models on the DVS-128 Gesture dataset and showed superior performance compared to previous biologically plausible models, even competing with deep supervised models. 


summary: The membrane potential of individual neurons depends on a large number of interacting biophysical processes operating on spatial-temporal scales spanning several orders of magnitude. The multi-scale nature of these processes dictates that accurate prediction of membrane potentials in specific neurons requires utilization of detailed simulations. However, constraining parameters within biologically detailed neuron models can be difficult, leading to poor model fits. This obstacle can be overcome partially by numerical optimization or detailed exploration of parameter space. However, these processes, which currently rely on central processing unit (CPU) computation, often incur exponential increases in computing time for marginal improvements in model behavior. As a result, model quality is often compromised to accommodate compute resources. Here, we present a simulation environment, NeuroGPU, that takes advantage of the inherent parallelized structure of graphics processing unit (GPU) to accelerate neuronal simulation. NeuroGPU can simulate most of biologically detailed models 800x faster than traditional simulators when using multiple GPU cores, and even 10-200 times faster when implemented on relatively inexpensive GPU systems. We demonstrate the power of NeuroGPU through large-scale parameter exploration to reveal the response landscape of a neuron. Finally, we accelerate numerical optimization of biophysically detailed neuron models to achieve highly accurate fitting of models to simulation and experimental data. Thus, NeuroGPU enables the rapid simulation of multi-compartment, biophysically detailed neuron models on commonly used computing systems accessible by many scientists. 


summary: ## Abstract

Neurons are complex computational devices, especially in their dendrites. While biophysical models capture these processes directly, cascade models offer another approach by representing neurons as a series of linear and non-linear operations, similar to artificial neural networks. Cascade models effectively capture single-cell computation, but struggle with certain dendritic phenomena. 

This research proposes **parallel, recurrent cascade models** to address this limitation. These models represent neurons as a cascade of parallel linear and non-linear operations that can be connected recurrently, resembling multi-layer, recurrent, artificial neural networks. 

The study suggests that these parallel, recurrent cascade models can capture additional sub-cellular phenomena like the interplay between sodium, calcium, and NMDA spikes.  The authors highlight potential applications of these models in artificial intelligence and argue that they offer a unifying tool for understanding single-cell computation and its algorithmic implications. 


summary: Predictive coding (PC) has become a popular model for neural computation, but its biological plausibility has been debated. Current implementations often use separate value and error neurons and require symmetric forward and backward weights, features not observed experimentally.  This paper introduces a constrained version of PC in the linear regime that addresses these limitations. By applying a disentangling-inspired constraint on hidden-layer neural activity, the authors derive an upper bound for the PC objective. Optimization of this upper bound leads to a network that achieves the same performance as the original PC objective, but with a biologically plausible structure. The units in this network can be interpreted as multi-compartmental neurons with non-Hebbian learning rules, aligning with recent experimental findings. This model also eliminates the need for one-to-one connectivity and signal multiplexing, features previously thought necessary. The normative nature of the algorithm allows for analytical understanding of the computational roles of different network components, and the parameters have natural interpretations as physiological quantities, providing a concrete link between PC and experimental measurements in the cortex. 


summary: This paper introduces a novel Recurrent Spiking Neural Network (RSNN) architecture that incorporates neuronal plasticity and reward propagation to enhance its learning and performance. The model addresses the limitations of traditional Deep Neural Networks (DNNs) by incorporating biologically-inspired mechanisms, such as spiking neurons and reward-based learning. The proposed RSNN overcomes the issues of high synaptic parameter count, slow backpropagation, non-differentiable spiking signals, and the separation of spatial and temporal information. It demonstrates significant improvements in various tasks, including image classification, recognition, memory association, and prediction. 


summary: The human brain's immense learning capabilities, energy efficiency, and scale are unmatched by any artificial system. This paper proposes a brain-derived neuromorphic computing approach using artificial electronic, ionic, and photonic materials, devices, and circuits to mimic the brain's molecular, neuro/synaptic, and hierarchical structures. The authors argue that bio-plausible local learning algorithms are crucial for capturing the brain's flexible and adaptive learning mechanisms. The resulting system could potentially realize an "artificial brain" prototype with general self-learning capabilities, while also allowing us to understand the link between neuronal and network-level properties with system-level functioning. 


summary: This paper introduces a novel set of methods and analyses for studying the dynamics of the *Caenorhabditis elegans* worm connectome. The research focuses on the dynamic implications of the spatial separation of neurons, an understudied area in *C. elegans* research. The paper investigates how the concurrent activity of independent neuronal elements, along axons, ultimately gives rise to a rich behavioral repertoire. The research addresses the challenge posed by experimental findings that many *C. elegans* neurons do not communicate using traditional action potentials. 


summary: The brain processes and integrates multiple timescales of information into a meaningful whole. Recent evidence suggests that the brain exhibits a complex multiscale temporal organization, with different regions displaying different timescales known as intrinsic neural timescales (INT). This paper reviews recent literature on INT and proposes that they are crucial for input processing. Specifically, INT are shared across different species, indicating their role in encoding inputs by matching the input's stochasticity with the brain's ongoing temporal statistics. The paper further highlights input integration versus segregation and input sampling as key temporal mechanisms for input processing, providing insights into the brain's environmental and evolutionary context. These findings have implications for understanding mental features, psychiatric disorders, and integrating timescales into artificial intelligence. 


summary: ## Abstract:

Working memory is the ability to preserve information for further processing. While persistent neuronal activity has been considered the primary mechanism for working memory, recent studies suggest that memory can also be retained in the "activity silence" neural state via synaptic changes. This paper proposes a unified framework for working memory that incorporates both persistent activity and synaptic changes, relying on spike-timing-dependent plasticity (STDP) to facilitate memory formation and retrieval. This model provides a comprehensive understanding of working memory by integrating different experimental findings and proposing a deeper mechanism for information storage and retrieval in the brain. 


summary: This review explores computational psychiatry through the lens of explaining mental illness through pathophysiology.  It uses the concept of a generative model to understand both sentient processing in the brain and the scientific process in psychiatry. The paper examines the brain as an organ of inference and prediction, exploring how dysconnections in neuronal networks lead to aberrant belief updating and false inference. The review then investigates how these theories are tested empirically, emphasizing the computational modeling of neuronal circuits and synaptic gain control. It also explores the possibilities of computational neuropsychology, computational phenotyping, and a computational nosology for psychiatry. The aim is to review a theoretical narrative emerging across subdisciplines within psychiatry, ranging from epilepsy to neurodegenerative disorders, post-traumatic stress disorder, chronic pain, schizophrenia, and functional medical symptoms. 


summary: This paper explores the use of Quantum Computing and Neuromorphic Computing to improve the safety, reliability, and explainability of Multi-Agent Reinforcement Learning (MARL) for optimal control in autonomous robotics. Quantum Approximate Optimization Algorithm (QAOA) is used to efficiently search for solutions to complex MARL problems, while Neuromorphic Computing, inspired by the human brain, provides parallel and distributed processing for intelligent and adaptive systems. This research aims to enhance the performance of autonomous robots by leveraging the power of these cutting-edge technologies. 


summary: The synaptic organization in the *Caenorhabditis elegans* neural network suggests significant local compartmentalized computations. Analysis of the *Caenorhabditis elegans* neural network reveals a surprising discrete clustered organization of the synapses. This organization can support local compartmentalized activities where several computations may be performed in parallel along a single neurite. This capacity of simply structured neurons can greatly enhance the computational power of compact neural networks. 


summary: This paper proposes a novel model of neuroplasticity called the "horizontal-vertical integration model." It combines a network of neurons with adaptive transmission links (horizontal plane) with an internal system of parameters controlling the external membrane-expressed parameters (vertical plane). This vertical system includes external parameters at the membrane layer, internal parameters in the sub-membrane zone and cytoplasm, and core parameters in the nucleus. This model allows for each neuron to have its own internal memory, separating neural transmission and information storage. The authors argue that this approach is a significant advancement over synaptic weight models and discuss how it could be applied to artificial intelligence. 


summary: Grand efforts in neuroscience are working toward mapping the connectomes of many new species, including the near completion of the Drosophila melanogaster. It is important to ask whether these models could benefit artificial intelligence. In this work we ask two fundamental questions: (1) where and when biological connectomes can provide use in machine learning, (2) which design principles are necessary for extracting a good representation of the connectome. Toward this end, we translate the motor circuit of the C. Elegans nematode into artificial neural networks at varying levels of biophysical realism and evaluate the outcome of training these networks on motor and non-motor behavioral tasks. We demonstrate that biophysical realism need not be upheld to attain the advantages of using biological circuits. We also establish that, even if the exact wiring diagram is not retained, the architectural statistics provide a valuable prior. Finally, we show that while the C. Elegans locomotion circuit provides a powerful inductive bias on locomotion problems, its structure may hinder performance on tasks unrelated to locomotion such as visual classification problems. 


summary: Deep learning has redefined AI thanks to the rise of artificial neural networks, which are inspired by neuronal networks in the brain. Neural networks use an efficient implementation of reverse differentiation, called backpropagation (BP). However, BP is often criticized for its biological implausibility. Biologically plausible learning methods that rely on predictive coding (PC) are increasingly studied. Recent works prove that these methods can approximate BP up to a certain margin on multilayer perceptrons (MLPs) and asymptotically on any other complex model. However, there is no biologically plausible method yet that can exactly replicate the weight update of BP on complex models. This paper generalizes PC and Z-IL by directly defining it on computational graphs and shows that it can perform exact reverse differentiation. This results in the first PC (and so biologically plausible) algorithm that is equivalent to BP in the way of updating parameters on any neural network, providing a bridge between the interdisciplinary research of neuroscience and deep learning. Furthermore, these results immediately provide a novel local and parallel implementation of BP. 


summary: In neural computation, information is typically encoded in neurons' spiking configurations, activation values, or dynamics. Synapses and their plasticity mechanisms mainly process this information and implement learning. This paper proposes a novel Turing complete paradigm of neural computation where information is encoded into discrete synaptic states, updated via synaptic plasticity mechanisms. It proves that any 2-counter machine, and therefore any Turing machine, can be simulated by a rational-weighted recurrent neural network using spike-timing-dependent plasticity (STDP) rules. The computational states and counter values are encoded into discrete synaptic strengths, transitioning through STDP. This suggests that a Turing complete synaptic-based paradigm of neural computation is theoretically possible and potentially exploitable, highlighting synapses' potential in encoding essential information beyond processing and learning. This approach represents a paradigm shift in neural computation. 


