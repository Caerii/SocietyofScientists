summary: ## Reconstructing computational system dynamics from neural data with recurrent neural networks

This study proposes a novel approach for reconstructing computational system dynamics from neural data using recurrent neural networks (RNNs). The method utilizes RNNs to learn the underlying computational mechanisms of a system by leveraging the temporal dependencies in neural recordings.  The authors demonstrate their approach's effectiveness in reconstructing the computational dynamics of a simulated neural network and a real-world auditory task. The results highlight the potential of RNNs as a powerful tool for understanding the complex computational processes within neural systems. 


summary: Computational modeling is a valuable tool for understanding consciousness, but is it sufficient on its own? This paper argues for the need for an ontological basis of consciousness and introduces a formal framework for grounding computational descriptions within an ontological substrate. Using this approach, a method for estimating the difference in qualitative experience between two systems is presented. This framework has broad applicability to computational theories of consciousness. 


summary: The abstract of this paper is not included in the provided text. Please provide the full text of the paper, including the abstract, so I can summarize it. 


summary: This paper introduces the concept of "biological computation," a new type of computation better adapted to biological systems. It goes beyond traditional computational theory by incorporating the interplay of computational and non-computational functions within biological systems. The paper uses the examples of hearts and flytraps, which are biological computers with computing power comparable to a slug and lobster ganglion respectively, to illustrate how classical computability theory may overlook the complexities of biological processes. This new understanding of computation aims to bridge the gap between human and machine learning by recognizing the unique capabilities of biological systems. 


summary: The original computers were people using algorithms to get mathematical results such as rocket trajectories. After the invention of the digital computer, brains have been widely understood through analogies with computers and now artificial neural networks, which have strengths and drawbacks. This article defines and examines a new kind of computation better adapted to biological systems, called biological computation, a natural adaptation of mechanistic physical computation. Nervous systems are of course biological computers, and the article focuses on some edge cases of biological computing, hearts and flytraps. The heart has about the computing power of a slug, and much of its computing happens outside of its forty thousand neurons. The flytrap has about the computing power of a lobster ganglion. This account advances fundamental debates in neuroscience by illustrating ways that classical computability theory can miss complexities of biology. By this reframing of computation, we make way for resolving the disconnect between human and machine learning. 


summary: This paper proposes a hardware architecture for efficiently computing a large-scale network of Hodgkin-Huxley (HH) neurons. The architecture is based on the neuron machine hardware architecture, but with multiple computation nodes (hardware neurons) to achieve linear speedup. The paper describes the design of a digital circuit for large-scale HH neuron networks, which supports axonal conduction delay, short- and long-term plasticity synapses, and floating-point precision HH neurons. This design is implemented on an FPGA chip, achieving near real-time computation for a network of one million HH neurons. The system can handle up to 12 million HH neurons and 600 million synapses. This design method can facilitate the implementation of complex neuron models on reconfigurable FPGA chips. 


summary: ## Abstract:

The connectome, the complete map of neural connections, offers a unique window into brain function. However, the extent to which connectivity alone can predict neural activity remains unclear. We addressed this question by developing a differentiable neural network model constrained by the connectome of the fly visual system.  We demonstrate that, given the computational task of visual motion detection, our model can accurately predict the activity of individual neurons in response to visual stimuli. Our findings suggest that, combined with knowledge of the computational task, connectomes can provide a powerful tool for understanding the functional roles of neurons in the brain. 


summary: The diversity and specificity of animal behaviors are often hard-coded into the developing nervous system, which represents a significant challenge for artificial intelligence. This paper explores the "genomic bottleneck" - the process by which a genome produces task-relevant neural circuits during development. Using the Genetic Connectome Model (GCM), the authors incorporate neurodevelopmental priors into machine learning architectures, aiming to reproduce the selection process behind innate behaviors. This model has been shown to predict wiring rules in the <i>C. elegans</i> gap junction connectome, and similar formulations have been applied to other organisms. The GCM's potential to promote structured connectivity, including feed-forward and scale-free networks, suggests its value in understanding the development of complex computational capabilities. 


summary: The human brain is a complex organ responsible for our behaviors, thoughts, emotions, and consciousness. Despite its small size (2% of body weight), it consumes a significant amount of energy (20% of oxygen). The brain is constantly active, even during sleep, carrying out functions like synaptic homeostasis and memory formation. The challenge of understanding how the brain works is one of the major scientific endeavors of the 21st century. 


summary: This article presents a computational framework for understanding how assemblies of neurons in the brain can perform complex computations. The framework, called Assembly Calculus, is based on the idea that assemblies of neurons, which are groups of neurons that fire together, can be combined and manipulated to represent and process information. The article describes how assemblies can be created, projected, associated, merged, and used for pattern completion.  The authors show that these operations can be implemented using biologically plausible mechanisms, such as Hebbian plasticity and recurrent synaptic connections. The article also discusses the potential applications of Assembly Calculus for understanding cognitive processes, such as memory, learning, and decision-making. 


summary: ## Abstract: 

The diversity and specificity of animal behaviors, as well as their neural correlates, has received attention from diverse areas of study. Recently, machine learning has provided key insights into the mechanics of solving complex behaviors. However, AI frameworks do not capture the emergence of innate behaviors, as conventional models require extensive update rules and training examples to achieve desired fitness on a task. The brain’s innate solutions have long inspired AI techniques, from convolutional neural networks to reinforcement learning, yet neuroevolutionary innovation has not been successfully recapitulated for the systematic discovery of powerful architectures. In order to reproduce the selection process behind innate behaviors, we must first confront the mystery of the “genomic bottleneck”: development’s uncanny ability to unpack a genome in order to produce specific, task-relevant neural circuits. To address this, we utilize the Genetic Connectome Model (GCM), which embeds genomic constraints into the development of a network through a set of evolutionary rules. These rules guide the emergence of functional circuits, which can be trained to solve different tasks. This model enables the exploration of neurodevelopmental evolution, by showing that the process of development can be recapitulated in a simplified artificial system that maps the space of possible brain architectures.  


summary: Even as machine learning surpasses human performance in many tasks, the brain's learning capabilities remain unmatched in their generality, robustness, and speed. This paper investigates how the brain can represent and manipulate sequences of stimuli, a crucial ability underlying many intelligent behaviors. The authors show that a previously proposed model of neural activity, NEMO, can naturally capture temporal order through synaptic weights and plasticity. This allows NEMO to learn and recall sequences of stimuli, enabling computations like sequence recognition, memory formation, and the creation of scaffolded representations. Moreover, the model can learn any finite state machine and, through an extension, achieve universal computation. The paper provides both theoretical analysis and experimental evidence, suggesting that sequences play a vital role in the brain's computational and learning capabilities. 


summary: Significant research has identified complex structures within the coordinated activity of interconnected neural populations. This paper explores the nature of the associated computations, how they are implemented, and their role in driving behavior. It introduces the concept of "computation through neural population dynamics" as a framework for understanding how neural activity implements computations essential for goal-directed behavior. The paper delves into mathematical tools for applying this framework to experimental data and highlights recent discoveries in areas like motor control, timing, decision-making, and working memory. Finally, it discusses promising future directions for this research. 


summary: Even though machine learning surpasses human performance in many areas, the brain’s learning abilities are still unmatched in terms of generality, robustness, and speed.  This paper demonstrates that a formal model of neural activity can naturally capture time as precedence through synaptic weights and plasticity, allowing for computations on sequences of assemblies.  The model can memorize sequences through repeated presentation, and when two brain areas receive the same sequence simultaneously, a scaffolded representation is created that enhances memorization and recall. The model can also learn any finite state machine through the presentation of appropriate sequence patterns, ultimately demonstrating the capability for universal computation. The paper supports its analysis with experiments exploring the learning limits of the model, providing a concrete hypothesis for the basis of the brain’s computational and learning capabilities.  


summary: This paper explores the intrinsic limitations of two widely used discrete Leaky-Integrate-and-Fire (LIF) models, the soft-reset model and the hard-reset model. It then redefines the applicable conditions and underlying assumptions for their implementation by mathematically re-deriving the numerical solution process of the LIF neuron model. The authors obtain an accurate discretization model, which they refer to as the standard numerical model. By adding constraints, the standard discrete model is simplified to obtain approximate models under various assumptions and their potential errors. The paper also discusses state-of-the-art bio-inspired SNNs and their potential for improved performance. 


summary: The article discusses the importance of “NeuroAI” – research combining neuroscience and artificial intelligence – to accelerate progress in AI. While AI has achieved impressive results in specific tasks, such as game playing, it still lags behind human capabilities in areas like adaptability and physical interaction with the world. The authors argue that current approaches to AI may not be sufficient to overcome these limitations and call for greater investment in NeuroAI to develop more human-like artificial intelligence. 


summary: This webpage does not contain the full text of the paper. It appears to be the IEEE Xplore website which contains links to purchase the full paper. 


summary: Recent approximations to backpropagation (BP) have mitigated many of BP's computational inefficiencies and incompatibilities with biology, but important limitations still remain. Moreover, the approximations significantly decrease accuracy in benchmarks, suggesting that an entirely different approach may be more fruitful. Here, grounded on recent theory for Hebbian learning in soft winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm that trains deep neural networks, without any feedback, target, or error signals. As a result, it achieves efficiency by avoiding weight transport, non-local plasticity, time-locking of layer updates, iterative equilibria, and (self-) supervisory or other feedback signals -which were necessary in other approaches. Its increased efficiency and biological compatibility do not trade off accuracy compared to state-of-the-art bioplausible learning, but rather improve it. With up to five hidden layers and an added linear classifier, accuracies on MNIST, CIFAR-10, STL-10, and ImageNet, respectively reach 99.4%, 80.3%, 76.2%, and 27.3%. In conclusion, SoftHebb shows with a radically different approach from BP that Deep Learning over few layers may be plausible in the brain and increases the accuracy of bio-plausible machine learning. Code is available at https://github.com/NeuromorphicComputing/SoftHebb. 


summary: This paper argues that the time is ripe to reverse engineer a smaller nervous system, specifically Caenorhabditis elegans, using established optophysiology techniques. The authors propose that this model system, with its conserved nervous system and well-defined anatomy, is ideal for capturing and controlling neuronal activity across hundreds of thousands of experiments. By combining data across populations and behaviors, modern machine learning can be used to create a simulation of C. elegans' brain states and behaviors. This, they contend, will be beneficial for designing artificial intelligence systems and advancing our understanding of how brains work. 


summary: Recent studies have shown that replay of hippocampal place cell activity during sleep or rest can reflect paths that go around barriers and adapt to changes in maze layout. However, existing computational models of replay struggle to generate such layout-conforming replay, limiting their application to simple environments. This paper introduces a computational model that generates layout-conforming replay and explains how it drives the learning of flexible navigation in a maze. The model incorporates a Hebbian-like learning rule for inter-place cell synaptic strengths, a continuous attractor network (CAN) with feedback inhibition to model place cell and interneuron interactions, and a dopamine-modulated three-factor rule to learn place-reward associations. During replay, the model generates trajectories that conform to the maze layout, and during navigation, the CAN periodically generates replay trajectories for path planning, with the animal choosing the path leading to maximum reward-related activity. Simulations in a virtual rat environment demonstrate the model's ability to learn flexible navigation in a maze through continuous re-learning of synaptic strengths. 


summary: This paper explores a complex-valued neural network (cv-NN) with linear, time-delayed interactions. The cv-NN exhibits complex spatiotemporal dynamics, including "chimera" states, which are used for computation. The paper demonstrates that the cv-NN can perform logic operations, store short-term memories, and securely transmit messages through a combination of interactions and time delays. Importantly, the computations can be precisely described by a mathematical formula. The authors further show that these computations can be decoded by biological neurons using intracellular recordings. The study suggests that complex-valued linear systems are capable of sophisticated computations while being mathematically solvable. This opens the door for designing bio-hybrid computing systems that can seamlessly integrate with other neural networks. 


summary: Utilizing recent advances in machine learning, we introduce a systematic approach to characterize neurons’ input/output (I/O) mapping complexity. Deep neural networks (DNNs) were trained to faithfully replicate the I/O function of various biophysical models of cortical neurons at millisecond (spiking) resolution. A temporally convolutional DNN with five to eight layers was required to capture the I/O mapping of a realistic model of a layer 5 cortical pyramidal cell (L5PC). This DNN generalized well when presented with inputs widely outside the training distribution. When NMDA receptors were removed, a much simpler network (fully connected neural network with one hidden layer) was sufficient to fit the model. Analysis of the DNNs’ weight matrices revealed that synaptic integration in dendritic branches could be conceptualized as pattern matching from a set of spatiotemporal templates. This study provides a unified characterization of the computational complexity of single neurons and suggests that cortical networks therefore have a unique architecture, potentially supporting their computational power. 


summary: Neuromorphic computing, inspired by the human brain, offers high energy efficiency and potential for diverse applications like autonomous vehicles and IoT. While mainly used for spike-based machine learning, its applicability extends to other areas like graph theory and differential equations. This paper proves that neuromorphic computing is Turing-complete, meaning it is capable of general-purpose computation. The authors present a neuromorphic model with two neuron parameters and two synaptic parameters, and demonstrate the computation of µ-recursive functions and operators, which are equivalent to Turing machine computations. This establishes the theoretical foundation for neuromorphic computing's potential in broader computing scenarios. 


summary: This paper explores the challenge of understanding the brain's computation, noting that different disciplines employ distinct approaches, hindering a unified understanding. The authors argue that emerging theoretical computing frameworks bridging top-down algorithmic and bottom-up physical perspectives are ideal for guiding the development of neural computing technologies like neuromorphic hardware and artificial intelligence. They further suggest that this balanced perspective is crucial for incorporating neurobiological details critical for describing computational disruptions in mental health and neurological disorders. 


summary: ## Abstract

While synaptic plasticity is often implicated in memory formation, recent studies suggest that other mechanisms might also play a role.  This paper explores a computational model of cerebellar Purkinje cells, which are known to be involved in eyeblink conditioning, demonstrating how these cells can learn to encode temporal relationships between stimuli.  The model proposes that Purkinje cells use an internal pacemaker mechanism to generate a precisely timed pause in their firing rate, which mirrors the timing of the conditioned blink response.  This pause is learned through pairings of a conditioned stimulus (CS) and an unconditioned stimulus (US) and is adaptable to different ISIs (interstimulus intervals). This model suggests that Purkinje cells may contribute to temporal memory encoding by dynamically adjusting their firing patterns based on the learned temporal relationship between stimuli. 


summary: The paper proposes a new algorithm, Activation Relaxation (AR), as a more biologically plausible alternative to backpropagation for training neural networks. AR overcomes challenges with existing local backpropagation approximations, such as complex connectivity schemes and multiple backward phases. This new algorithm demonstrates robust convergence to the correct backpropagation gradients and requires only a single type of neuron and a single backward phase. It can handle arbitrary computation graphs, as illustrated by its success in training deep neural networks on visual classification tasks. Further simplifications of the algorithm, such as addressing the weight-transport problem and nonlinear derivatives, are discussed, further enhancing its biological feasibility while maintaining performance. 


summary: The abstract of this paper is not provided in the text you have given. The text provided is the introduction section of the paper. 


summary: This article presents a full-scale scaffold model of the human hippocampus CA1 area, a brain region crucial for memory formation. The model, which includes 1.6 million neurons and over 1 billion synapses, is based on experimental data from human post-mortem samples. The authors use a novel co-simulation technology to combine high-resolution modeling of specific regions of interest with lower-resolution modeling of other areas. This allows for a more realistic representation of the complex interactions within the hippocampus and a better understanding of how this brain region functions in humans. 


summary: The Neural Engineering Framework (NEF) is a method for implementing high-level algorithms constrained by low-level neurobiological details. This paper expands on the NEF's core principles of (a) specifying the desired tuning curves of neurons, (b) defining computational relationships between neurons, and (c) finding synaptic weights that cause those computations and tuning curves. The authors show how to extend this to include complex spatiotemporal tuning curves, and then apply this approach to produce functional computational models of grid cells, time cells, path integration, sparse representations, probabilistic representations, and symbolic representations in the brain. 


summary: Simulations of neural networks can be used to study how changes in the brain affect its activity. However, some changes happen over long periods of time, like those caused by strokes or brain injuries. This paper presents a model of a network of human brain regions that can simulate these long-term changes. The model uses a modified Wilson-Cowan model to represent the dynamics of each region and synaptic plasticity to adjust the connections between regions. The researchers used different strategies, including different models for plasticity, to achieve a runtime of one second for one second of biological time. 


summary: The paper, "A New Age of Computing and the Brain," discusses the remarkable advances in both brain science and computer science that have led to unprecedented opportunities for collaboration and discovery. The paper highlights the explosion of brain research data, the availability of vast computational power, and the success of neural-inspired machine learning models as key drivers for this new age of computing and the brain. It argues that brain science can become the next "big data science," leveraging computational tools to study and connect the structure and function of the brain in new ways. The authors propose that this collaboration will lead to significant advancements in our understanding of the brain and its mysteries. 


summary: This paper examines the computational capabilities of the brain, highlighting the intrigue it holds for computer scientists.  Despite the brain's impressive power with minimal energy consumption, there's a lack of collaboration between computer scientists and neuroscientists in this field. The abundance of data and models in computational neuroscience makes it difficult for others to contribute. This paper aims to provide background information, current references on data and models in neuroscience, and identify potential opportunities for computer scientists to contribute to the study of brain computation. 


summary: The paper argues that the brain can be viewed as a literal computer, not just metaphorically. It proposes empirical criteria for what makes a physical system genuinely a computational one, going beyond theoretical computer science. Applying these criteria to the brain suggests it is likely an analog computer, highlighting the importance of understanding the distinctions between analog and digital computation for neuroscience, cognitive science, and theoretical computer science. 


summary: Spiking neural networks (SNN) are energy-efficient alternatives to traditional neural networks, but their discontinuous spike times make gradient computation challenging. Previous work has largely focused on approximate gradient methods due to the belief that exact gradients do not exist. This paper demonstrates that exact gradients for SNN with respect to their weights are well-defined by applying the implicit function theorem at discrete spike times. It proposes a novel forward propagation (FP) training algorithm that exploits the causality structure between spikes to compute exact gradients. FP allows for parallelization and provides insights into the effectiveness of related methods like Hebbian learning and surrogate gradient approximations. 


summary: Brain-inspired computing, which leverages the brain's efficiency in solving cognitive tasks, is a promising solution to challenges faced by deep learning today. However, current research in neuromorphic computing focuses on deterministic operations. This paper argues that incorporating temporal information encoding in probabilistic neuromorphic systems may offer a better approach. It explores superparamagnetic tunnel junctions as a potential pathway to enable a new generation of brain-inspired computing that combines the benefits of how information is encoded and how computing occurs in the brain. The paper presents a hardware-algorithm co-design analysis, demonstrating 97.41% accuracy for a state-compressed 3-layer spintronics enabled stochastic spiking network on the MNIST dataset, achieving high spiking sparsity through temporal information encoding. 


summary: Synthetic biology aims to create large-scale genetic networks in living cells that perform complex tasks. While digital and analog computing paradigms have been implemented in cells, they face challenges in scaling due to resource limitations, noise, and incompatibility with graded biological signals. This study explores a new approach to synthetic biology by investigating the potential of neuromorphic computing, which leverages the nonlinear and robust nature of biological systems. The authors present a theoretical framework for implementing neuromorphic computing in living cells and demonstrate its feasibility through in silico simulations. This approach offers a promising alternative to traditional digital and analog computing paradigms for designing more robust and scalable genetic circuits. 


summary: This paper explores the concept of neural timescales from a computational perspective, focusing on how they reflect information in dynamic environments. The authors discuss three key directions where computational methods can contribute to a deeper understanding of neural timescales: 
1.  **Data analysis methods:** These techniques allow researchers to capture different timescales of neural dynamics across various recording modalities. 
2. **Computational models:**  These models provide a mechanistic explanation for the emergence of diverse timescales in neural activity.
3. **Task-optimized models in machine learning:**  These models help uncover the functional relevance of neural timescales in relation to brain function. 
The authors argue that this integrative approach, combined with empirical findings, could lead to a more comprehensive understanding of how neural timescales link brain structure, dynamics, and behavior. 


summary: Recent work suggests goal-driven training of neural networks can be used to model neural activity in the brain.  We demonstrate this in the head direction system, where the connectivity and functional organization have been characterized.  We trained recurrent neural networks (RNNs) to estimate head direction through integration of angular velocity. We found that two distinct classes of neurons observed in the head direction system, the Ring neurons and the Shifter neurons, emerged naturally in artificial neural networks as a result of training. Furthermore, connectivity analysis and in-silico neurophysiology revealed structural and mechanistic similarities between artificial networks and the head direction system. Our results show that optimization of RNNs in a goal-driven task can recapitulate the structure and function of biological circuits, suggesting that artificial neural networks can be used to study the brain at the level of both neural activity and anatomical organization. 


summary: This paper extends the ideas of [1] and further explores the possibility of the brain acting as a quantum computer. The authors propose that the voltage difference between two axons creates an environment for ions to undergo spatial superposition. This superposition is influenced by metric perturbations (such as gravitational waves), leading to a unique evolution of the quantum state. This evolution encodes the information being processed within the axon tract. Upon decoherence, which is essentially a measurement, the final spatial state of the ions is determined and reset for the next impulse.  Synchronization between multiple tracts allows for a complete picture of a quantum computing circuit.  The authors estimate that the corpus callosum alone could process upwards of 15 million quantum states every millisecond. 


summary: This research developed a computational model of the brain cortex using the Brian simulator. The model focuses on connection weights and their effect on the interactivity and connectivity of cortical neurons, both within the same layer and across multiple layers. The study investigated the impact of input noise on the synchronization of neuronal networks in the model. While synchronization is crucial for understanding brain function, it has been largely overlooked in simulated cortical models. This research aimed to fill that gap by analyzing the synchronization of neuronal groups connected across different layers and exploring how input noise influences this synchronization. 


summary: Dragonflies are highly skilled predators that can selectively attend to a single target within a swarm. This ability likely relies on "small target motion detector" (STMD) neurons, which exhibit facilitation – an enhanced response to targets moving on continuous paths. This study used a hybrid computational model incorporating detailed morphological data and nonlinear NMDA receptor properties to investigate if these factors contribute to facilitation in dragonfly STMD neurons. The model successfully generated potent facilitation for targets moving on continuous trajectories, including a "spotlight" of maximal sensitivity near the target's last known location, mirroring in vivo recordings. Notably, the model did not produce a traveling wave of facilitation. This research supports a significant role for the high dendritic density in dragonfly neurons in enhancing nonlinear facilitation. 


summary: Spiking neural networks (SNNs) are increasingly used in basic and applied neuroscience research, as they offer a biologically plausible framework for modeling neural computation.  However, the simulation of large-scale SNNs can be computationally expensive, especially when using CPU-based simulation environments.  Here, we present a method for efficient parameter calibration and real-time simulation of SNNs using the GeNN and NEST simulation frameworks.  Our approach leverages the strengths of both frameworks, allowing for accurate and efficient simulation of complex SNNs.  We demonstrate the effectiveness of our method by simulating a range of SNNs, including a model of the hippocampus and a network of spiking neurons for object recognition. Our results show that our method can significantly reduce the simulation time and improve the accuracy of SNN simulations.


summary: The human brain can learn new things without forgetting old ones, unlike artificial neural networks, which suffer from catastrophic interference. This paper proposes a novel computational model for life-long learning inspired by the hippocampus, which uses novelty-based dopamine modulation and lateral inhibitory plasticity to address catastrophic interference. The model incorporates principles of biological plausibility, including the role of dopamine in novelty detection and the influence of lateral inhibition on synaptic plasticity.  It demonstrates that novelty-based dopamine modulation plays a crucial role in regulating learning and memory in the face of new information, potentially explaining the brain's remarkable ability for lifelong learning without catastrophic forgetting.  The model also introduces a novel method for implementing lateral inhibitory plasticity in neural networks, which further contributes to the prevention of catastrophic interference. 


summary: This paper explores how neural activity represents probability distributions. It focuses on three major proposals: Probabilistic Population Codes (PPCs), Distributed Distributional Codes (DDCs), and Neural Sampling Codes (NSCs). The paper provides a unified language to compare these proposals, highlighting their similarities and differences. It also reviews empirical data supporting these proposals and discusses how alternative proposals might explain these data. The paper concludes by outlining challenges in resolving the debate and suggests future directions for research. 


summary: Multiple studies have shown how dendrites enable some neurons to perform linearly non-separable computations. However, many neurons, such as granule cells, have modest dendritic trees and are electrically compact, making it impossible to decompose them into independent subunits. This study upgrades the integrate and fire neuron to account for saturation due to interacting synapses. This model, a single layer neuron with interacting synapses, can perform linearly non-separable computations, demonstrating that all neurons, due to having at least one layer, can potentially implement these computations. 


summary: This document does not contain an abstract. 


summary: Sleep is essential for optimal cognitive function, but its precise role in restoring brain activity is unclear.  We propose that sleep restores an optimal computational regime in cortical networks by promoting a balanced state of excitation and inhibition, and by resetting synaptic weights.  We tested this hypothesis using a computational model of the neocortex and found that sleep-like activity restored the balance of excitation and inhibition in the network.  This balance was disrupted during wakefulness, leading to a decrease in network performance.  We also found that sleep-like activity led to a reduction in synaptic weights, which is known to be important for memory consolidation.  These findings suggest that sleep plays a crucial role in restoring the brain’s computational capacity by promoting a balance of excitation and inhibition, and by resetting synaptic weights.  This suggests that sleep is not simply a passive state of rest, but an active process that is essential for optimal brain function.


summary: ## Abstract

Hippocampal place cells (PC) exhibit replay during sleep or rest, where their activity pattern resembles that of movement periods. Recent studies have shown that replay trajectories in a reconfigurable maze conform to the spatial layout, suggesting a role in learning flexible navigation. Existing models struggle to replicate this layout-conforming replay and lack biological plausibility for how replay drives downstream circuits. 

This paper proposes a computational model to generate layout-conforming replay and explain its contribution to flexible navigation learning. The model incorporates shortest path distance for place field firing, a Hebbian-like rule for synaptic strength learning, and a continuous attractor network with feedback inhibition. This model captures layout-conforming replay during rest and goal-directed navigation, highlighting how replay drives learning in the striatum through a novel three-factor learning rule. 


summary: The primary somatosensory cortex (S1) of mammals is critically important in the perception of touch and works closely with other sensory and motor cortical regions in permitting coordinated activity with tasks involving grasp. The communication of these cortical areas with the thalamus is crucial for maintaining functions, such as sleep and wakefulness, considering that the thalamocortical (TC) circuit is essential for cerebral rhythmic activity. A greater understanding of S1 cortical circuits will help us gain insights into neural coding and be of assistance in determining how disease states such as schizophrenia, epilepsy and Parkinson’s disease lead to sensory deficits or uncoordinated movement. The Blue Brain Project (BBP) has developed a highly detailed model of rat S1 incorporating anatomical and physiological information from a wide range of experimental studies. However, its reproducibility and use by the community, as well as its extension or modification to connect to other regions or update model features, are limited by certain constraints. Here we implemented the original BBP S1 model in NetPyNE to make it more accessible and simpler to scale, modify and extend. NetPyNE is a python package that provides a high-level interface to the NEURON simulator, and allows the definition of complex multiscale models using an intuitive declarative standardized language. Conversion to NetPyNE also makes it easier to connect to previous models developed within the platform, such as our primary motor cortex model. We demonstrate the capabilities of the NetPyNE S1 model by running simulations on a high performance

summary: This article discusses how emerging theoretical computing frameworks, combining top-down algorithms with bottom-up physics approaches, can be beneficial for developing neural computing technologies. These frameworks can help bridge the gap between artificial neural networks and neuromorphic hardware, ultimately leading to a more comprehensive understanding of brain computations. The article also argues that incorporating neurobiological details is crucial for understanding neural computational disruptions in mental health and neurological disorders. 


summary: Connectivity inferred from neural activity in strongly recurrent networks can be systematically biased. This bias is due to the confounding effects of recurrent dynamics, which make it challenging to disentangle true connectivity from activity-dependent correlations. We developed a theoretical framework to quantify and correct for this bias, which we validated using simulated neural networks. We found that recurrent dynamics can lead to systematic underestimation of connectivity strength, and that the degree of bias depends on the network's recurrent strength and the timescale of the neuronal dynamics. Our results suggest that existing methods for inferring connectivity from neural activity may be prone to systematic errors, particularly in networks with strong recurrent interactions. We propose a correction method to address this bias, which could improve the accuracy of connectivity inference in a wide range of applications. 


summary: This paper presents a hybrid machine-learning and computational-neuroscience approach that transforms analytical models of sensory neurons and synapses into deep neural network (DNN) neuronal units with the same biophysical properties. The DNN-model architecture comprises parallel and differentiable equations that can be used for backpropagation in neuro-engineering applications, and offers a significant simulation run-time improvement compared to traditional methods. The focus of the paper is on auditory neurons and synapses, but the approach can be extended to other neuron and synapse types. 


summary: This paper proposes recurrent spiking neural networks (RSNNs) trained to perform multiple cognitive tasks, inspired by cognitive neuroscience experiments. These RSNNs are designed to capture the dynamic nature of neurocognitive activity, and are trained using input-output examples. The paper aims to understand the dynamic mechanisms that drive RSNN performance by analyzing the trained network's structure and dynamics. The authors believe that combining multitasking and spiking activity within a single system provides valuable insights into the principles of neural computation. 


summary: This paper proposes a novel computational model for grid maps in neural populations, drawing inspiration from the hexagonal grid structure observed in the entorhinal cortex. The model leverages a continuous attractor network, utilizing the principles of lateral inhibition and Hebbian learning. Notably, the model demonstrates a capacity for flexible adaptation to different grid scales and orientations, aligning with experimental findings. Further, the study explores the interplay of grid cell firing patterns with place cell responses, suggesting a mechanism for spatial memory encoding. Finally, the paper provides insights into the potential role of grid cells in path integration, highlighting their contribution to spatial navigation. 


summary: Synapses in the brain are constantly changing, with these changes occurring on a spectrum of timescales, from minutes to hours (synaptic rewiring) to milliseconds (short-term synaptic plasticity). This paper explores the computational potential of a network (multi-plasticity network, MPN) that relies solely on synaptic modulation during inference, rather than traditional recurrent neural networks (RNNs) that store information in their internal state. The MPN outperforms its RNN counterparts on several neuroscience-relevant tests and achieves comparable performance on a battery of neuroscience tasks. This research suggests that synaptic modulations play a significant role in brain computation and highlights the need for further investigation into this area. 


summary: Normative models of neural computation offer simplified yet lucid mathematical descriptions of murky biological phenomena. Previously, online Principal Component Analysis (PCA) was used to model a network of single-compartment neurons accounting for weighted summation of upstream neural activity in the soma and Hebbian/anti-Hebbian synaptic learning rules. However, synaptic plasticity in biological neurons often depends on the integration of synaptic currents over a dendritic compartment rather than total current in the soma. Motivated by this observation, we model a pyramidal neuronal network using online Canonical Correlation Analysis (CCA). Given two related datasets represented by distal and proximal dendritic inputs, CCA projects them onto the subspace which maximizes the correlation between their projections. First, adopting a normative approach and starting from a single-channel CCA objective function, we derive an online gradient-based optimization algorithm whose steps can be interpreted as the operation of a pyramidal neuron. To model networks of pyramidal neurons, we introduce a novel multi-channel CCA objective function, and derive from it an online gradient-based optimization algorithm whose steps can be interpreted as the operation of a pyramidal neuron network including its architecture, dynamics, and synaptic learning rules. Next, we model a neuron with more than two dendritic compartments by deriving its operation from a known objective function for multi-view CCA. Finally, we confirm the functionality of our networks via numerical simulations. Overall, our work presents a simplified but informative abstraction of learning in a pyramidal neuron network, and demonstrates how such networks can integrate multiple sources of inputs. 


summary: **Abstract**

Deep learning methods are revolutionizing artificial intelligence and have the potential to contribute to our understanding of the brain. While deep neural networks (DNNs) are often seen as black boxes, recent work has shown that their internal representations can be surprisingly interpretable, revealing connections to human cognition. In this review, we discuss how DNNs can be used to study the brain in several ways: (i) as models of brain function, (ii) as tools for analyzing brain data, and (iii) as experimental probes of neural circuits. We argue that these applications can be used to gain insights into the neural computations underlying cognitive processes and that DNNs hold promise for the development of new diagnostic and therapeutic tools for neurological and psychiatric disorders.

summary: The error-backpropagation (backprop) algorithm is the most common solution to the credit assignment problem in artificial neural networks. However, it's unclear if the brain uses a similar strategy to modify its synapses. Recent models try to bridge this gap, but they either struggle to backpropagate error signals across layers or require a multi-phase learning process. This paper introduces Bursting Cortico-Cortical Networks (BurstCCN), a model that addresses these issues by integrating known properties of cortical networks, including bursting activity, short-term plasticity (STP), and dendrite-targeting interneurons. BurstCCN relies on burst multiplexing via connection-type-specific STP to propagate backprop-like error signals within deep cortical networks. These error signals are encoded at distal dendrites and induce burst-dependent plasticity. The paper demonstrates that BurstCCN can effectively backpropagate errors through multiple layers using a single-phase learning process, learns complex image classification tasks (MNIST and CIFAR-10), and approximates backprop-derived gradients. The results suggest that cortical features across different levels underlie single-phase efficient deep learning in the brain. 


summary: Synaptic plasticity allows cortical circuits to learn new tasks and to adapt to changing environments. This paper explores how to train recurrent neural networks in tasks similar to those used to train animals in neuroscience laboratories, and how computations emerge in the trained networks. Surprisingly, artificial networks and real brains can use similar computational strategies. The authors discuss ways to build neural network models to investigate how the brain could solve cognitive tasks, emphasizing the biological plausibility of the models and techniques. This paper aims to provide a valuable tool to investigate mechanisms that networks could use to perform tasks. 


summary: Assemblies are large populations of neurons believed to imprint memories, concepts, words, and other cognitive information. We identify a repertoire of operations on assemblies. These operations correspond to properties of assemblies observed in experiments, and can be shown, analytically and through simulations, to be realizable by generic, randomly connected populations of neurons with Hebbian plasticity and inhibition. Assemblies and their operations constitute a computational model of the brain which we call the Assembly Calculus, occupying a level of detail intermediate between the level of spiking neurons and synapses and that of the whole brain. The resulting computational system can be shown, under assumptions, to be, in principle, capable of carrying out arbitrary computations. We hypothesize that something like it may underlie higher human cognitive functions such as reasoning, planning, and language. In particular, we propose a plausible brain architecture based on assemblies for implementing the syntactic processing of language in cortex, which is consistent with recent experimental results. 


summary: The neocortex, a hierarchical distributed network, processes information through signals exchanged between its areas. This paper proposes a hypothesis that gives computational meaning to these interarea signals, based on biological constraints and existing theories like predictive coding and reinforcement learning.  Two types of feedforward signals (observation and deviation) and three types of feedback signals (prediction, plan, and intention) are identified.  The paper maps these signals to specific pathways within the neocortex, proposing that the feedforward corticocortical pathway carries observation signals, the feedback corticocortical pathway carries prediction signals, and the corticothalamic pathway carries deviation signals. Finally, the paper hypothesizes that the thalamocortical pathway mediated by matrix relay cells transmits intentions, while the corticocortical pathway from IT cells to the first layer transmits plans. 


summary: This paper discusses the collaborative relationship between neuroscience and artificial intelligence, highlighting how these fields can work together to achieve new discoveries. Researchers in neuroscience can propose hypotheses that are difficult or costly to test using traditional methods.  Artificial intelligence experts can then use connectionist approaches to implement these hypotheses, leading to new paradigms in both fields. The paper uses the example of the tripartite synapse between neurons and astrocytes to illustrate this symbiotic relationship. 


summary: ## Abstract:

This study investigates the impact of biophysical parameters on signal transfer and representation within a spiking neural network trained to decode arbitrary noise patterns. The Brendel model is used as a foundation, where membrane voltage is interpreted as an error signal from predicted input, linking predictive coding to biophysical parameters. The research focuses on how basic membrane characteristics and synaptic delay influence information flow in this trained network. The authors analyze the effects of these parameters on the network's ability to represent and transfer information, highlighting the importance of biophysical properties in shaping neural computations. 


summary: ## Abstract 

Brain activity exhibits complex spontaneous dynamics, especially during conscious states. While changes in spontaneous and evoked dynamics are empirically observed across different brain states, a formal understanding of their multi-scale nature remains elusive. This study uses mean-field models of conductance-based Adaptive Exponential (AdEx) integrate-and-fire neurons with spike-frequency adaptation, constrained by human anatomy and empirical local circuit parameters, to simulate global dynamics mimicking different human brain states.  The model captures essential characteristics of human wakefulness and sleep states, demonstrating that variations in spike-frequency adaptation coupled with the human connectome can generate macroscopic dynamics.  This work provides insights into the relationship between microscopic neuromodulatory processes and macroscopic brain activity. 


summary: This paper presents a large-scale cerebellar network model for supervised learning, along with a cerebellum-inspired neuromorphic architecture. The model incorporates 3.5 million neurons, significantly scaling up state-of-the-art neuromorphic designs. The architecture is implemented on a reconfigurable neuromorphic system to replicate cerebellar dynamics during the optokinetic response. Experiments show real-time operation with high system throughput, suggesting the model offers a theoretical and engineering basis for brain-inspired computing and further cerebellar learning exploration. 


summary: The human brain exhibits an impressive capability for computation, which inspires the development of artificial neural networks. However, despite significant advancements in understanding both biological and artificial neural networks, a fundamental gap exists between them and traditional silicon computers, hindering our ability to fully leverage the unique computational power of neural networks. This gap stems from the lack of a concrete and low-level neural machine code, restricting our access to neural computation. 
This study introduces a neural machine code and programming framework for the reservoir computer, a recurrent neural network known for its simple governing equations and ability to perform a variety of computer-like functions. The framework enables the manipulation of a reservoir computer using a low-level, neural-inspired language, bridging the gap between neural and silicon computers. This approach allows for the programming of complex tasks on a reservoir computer using a neural machine code, unlocking new possibilities for exploring and harnessing the computational power of these systems. 


summary: Several recent studies attempt to address the biological implausibility of the well-known backpropagation (BP) method. While promising methods such as feedback alignment, direct feedback alignment, and their variants like sign-concordant feedback alignment tackle BP's weight transport problem, their validity remains controversial owing to a set of other unsolved issues. In this work, we answer the question of whether it is possible to realize random backpropagation solely based on mechanisms observed in neuroscience. We propose a hypothetical framework consisting of a new microcircuit architecture and its supporting Hebbian learning rules. Comprising three types of cells and two types of synaptic connectivity, the proposed microcircuit architecture computes and propagates error signals through local feedback connections and supports the training of multi-layered spiking neural networks with a globally defined spiking error function. We employ the Hebbian rule operating in local compartments to update synaptic weights and achieve supervised learning in a biologically plausible manner. Finally, we interpret the proposed framework from an optimization point of view and show its equivalence to sign-concordant feedback alignment. The proposed framework is benchmarked on several datasets including MNIST and CIFAR10, demonstrating promising BP-comparable accuracy. 


summary: The provided text is a list of references, and does not contain the abstract. The user specifically requested the abstract of the paper.  Please provide the abstract of the paper for a better summary. 


summary: Most dendritic studies have focused on the apical zone of pyramidal neurons receiving feedback connections, used for learning. However, recent research suggests that the apical input (context) from feedback and lateral connections is more diverse, with greater implications for learning and processing. This context can be categorized as proximal, distal, universal, and feedback, and it influences the transmission of feedforward signals. Context-sensitive neurons flexibly integrate context with the feedforward current, amplifying it when both are coherent and attenuating it otherwise. This results in the propagation of more coherent signals, leading to faster learning with fewer neurons. The universality of this mechanism is demonstrated by its successful application in artificial networks, where fewer neurons are needed to process real-world data. 


summary: This paper proposes the "backpropagation-based recollection hypothesis," which suggests that weak, fast-fading action potentials traveling backward from post-synaptic to pre-synaptic neurons mediate memory recall, imagination, language understanding, and naming. The hypothesis posits that these signals originate in neurons that respond uniquely to specific stimuli, and then travel backward to reactivate the same neuronal populations that were active during perception. This backward propagation effectively recreates an "offline" experience similar to the original. The paper supports this hypothesis with a review of existing literature, providing evidence for the existence of backpropagating signals with specific properties. Additionally, simulations using spiking neural network models demonstrate the computational feasibility of this mechanism for mapping objects to their names. 


summary: This paper proposes the use of parallel, recurrent cascade models to capture complex subcellular computations in neurons. These models, inspired by multi-layer recurrent artificial neural networks, allow for representation of interactions between different compartments within a neuron, like sodium, calcium, and NMDA spikes. The paper highlights the potential of these models for integration into larger neural networks, enabling training for complex tasks and exploring the algorithmic implications of physiological phenomena. 


summary: Artificial Neural Networks (ANNs), inspired by biology, are increasingly used to model behavioral and neural data, an approach called neuroconnectionism.  While ANNs are hailed as the best current models of brain information processing, they have also been criticized for failing to account for basic cognitive functions. This paper argues against focusing on the limitations of current ANNs and instead proposes viewing neuroconnectionism as a large-scale research program centered around ANNs as a computational language for expressing falsifiable theories about brain computation. The paper outlines the core of the program, its underlying computational framework, and its tools for testing neuroscientific hypotheses. It reviews past and present neuroconnectionist projects and their responses to challenges, concluding that the research program is highly progressive, generating novel insights into brain function. 


summary: ## Abstract

This paper investigates the efficiency of biologically inspired learning algorithms for training spiking neural networks (SNNs) to perform motor control tasks. The authors compare two approaches: spike-timing dependent reinforcement learning (STDP-RL), where individual neurons learn based on reward signals, and a novel variation of evolutionary strategies (EVOL) that optimizes the network at a population level. The study utilizes the CartPole reinforcement learning problem to evaluate the effectiveness of these methods, analyzing both training efficiency and the resulting network dynamics. The research highlights the potential of SNNs for solving control problems and emphasizes the need for efficient learning strategies in this emerging field.  


summary: This paper proposes a low-power, efficient VLSI architecture for BCI spike sorting, called "Zydeco-Style," designed for BCI implants enabled by low-power Bluetooth Low Energy (BLE) technology for wireless communication. The architecture addresses the limitations of traditional spike sorting techniques, particularly for VLSI implementations, aiming to improve the accuracy and power efficiency of BCI implants. The proposed architecture adheres to industrial standards and human body implant restrictions, enabling improved usability and mobility of BCI implants for controlling prosthetic limbs. 


summary: This paper explores the role of astrocytes (glial cells) in the self-repair of neuromorphic hardware systems based on Spiking Neural Networks (SNNs). The authors investigate how astrocytes can be modeled to mimic their self-repair capabilities in the brain, specifically focusing on how they regulate synaptic strength. They argue that this astrocyte-based mechanism can be used to mitigate hardware faults in neuromorphic systems, leading to improved accuracy and convergence for unsupervised learning tasks. They demonstrate the effectiveness of their approach on the MNIST and F-MNIST datasets and provide their source code and trained models for further exploration. 


summary: Neuromorphic computing is a crucial future technology for the computing industry, but it has yet to achieve its promise and has struggled to establish a cohesive research community. A large part of the challenge is that full realization of the potential of brain inspiration requires advances in both device hardware, computing architectures, and algorithms. This simultaneous development across technology scales is unprecedented in the computing field. This article presents a strategy, framed by market and policy pressures, for moving past these current technological and cultural hurdles to realize its full impact across technology. Achieving the full potential of brain-derived algorithms as well as post-complementary metal-oxide-semiconductor (CMOS) scaling neuromorphic hardware requires appropriately balancing the near-term opportunities of deep learning applications with the long-term potential of less understood opportunities in neural computing. 


summary: The neural basis of human consciousness is a major question in neuroscience. While we know that brain activity is dynamic during consciousness, it remains unclear how this dynamic activity emerges from the fixed network of anatomical connections in the brain (the connectome). The authors propose a new method for analyzing brain activity, called "connectome harmonic decomposition (CHD)" that leverages the structure of the brain's connectome to analyze the functional connectivity of the brain.  CHD allows for analyzing how brain structure and function relate at multiple scales and provides a framework for understanding how brain activity changes when we are conscious vs. unconscious. 


summary: Classical computational neuroscience has yielded accurate, yet computationally demanding, models of neuronal processes. To overcome this limitation, this paper proposes a hybrid machine-learning and computational-neuroscience approach that translates analytical models of auditory sensory neurons and synapses into artificial neural networks (ANNs). These ANNs maintain biophysical properties while offering significant speedup, enabling integration into large-scale simulation frameworks and real-time neuro-engineering applications. The approach demonstrates generalizability across various analytical models and holds promise for accelerating the development of large-scale brain networks and ANN-based treatment of neurological conditions. 


summary: Computational neuroscience aims to rebalance the complex dynamic system of the human brain through controlled interventions. However, a commonly agreed definition of a brain state is still lacking, hindering our ability to predict and induce transitions between them. This paper reviews recent advances in computational neuroscience that enable robust definition and manipulation of brain states. It proposes a framework for determining the functional hierarchical organization of a brain state and discusses the use of sophisticated whole-brain computational models to predict and design novel interventions. 


summary: ## Abstract:

Sophisticated machine learning struggles to transition onto battery-operated devices due to the high-power consumption of neural networks. Researchers have turned to neuromorphic engineering, inspired by biological neural networks, for more efficient solutions. While previous research focused on artificial neurons and synapses, an essential component has been overlooked: dendrites. Dendrites transmit inputs from synapses to the neuron's soma, applying both passive and active transformations. However, neuromorphic circuits replace these sophisticated computational channels with metallic interconnects. In this study, we introduce a versatile circuit that emulates a segment of a dendrite which exhibits gain, introduces delays, and performs integration. We show how sound localisation - a biological example of dendritic computation - is not possible with the existing passive dendrite circuits but can be achieved using this proposed circuit. We also find that dendrites can form bursting neurons. This significant discovery suggests the potential to fabricate neural networks solely comprised of dendrite circuits. 


summary: Cybernetical neuroscience is a new scientific field that applies methods from cybernetics (the science of control and communication in living organisms, machines, and society) to the mathematical models used in computational neuroscience. It also focuses on the practical applications of the insights gained from studying these models. The paper outlines the key tasks, methods, and some of the results of cybernetical neuroscience. 


summary: We introduce a biologically plausible local learning rule that provably increases the robustness of neural dynamics to noise in nonlinear recurrent neural networks with homogeneous nonlinearities. Our learning rule achieves higher noise robustness without sacrificing performance on the task and without requiring any knowledge of the particular task. The plasticity dynamics—an integrable dynamical system operating on the weights of the network—maintains a multiplicity of conserved quantities, most notably the network’s entire temporal map of input to output trajectories. The outcome of our learning rule is a synaptic balancing between the incoming and outgoing synapses of every neuron. This synaptic balancing rule is consistent with many known aspects of experimentally observed heterosynaptic plasticity, and moreover makes new experimentally testable predictions relating plasticity at the incoming and outgoing synapses of individual neurons. Overall, this work provides a novel, practical local learning rule that exactly preserves overall network function and, in doing so, provides new conceptual bridges between the disparate worlds of the neurobiology of heterosynaptic plasticity, the engineering of regularized noise-robust networks, and the mathematics of integrable Lax dynamical systems. 


summary: This paper describes the authors' experience using Google Compute Platform (GCP) with Slurm to run large-scale simulations of a detailed brain motor cortex circuit model. The model simulates over 10,000 biophysically detailed neurons and 30 million synaptic connections. Parameter exploration using grid search parameter sweeps and evolutionary algorithms was used to optimize and evaluate the model's parameters and responses, requiring tens of thousands of simulations. The paper discusses best practices and solutions to issues encountered during the process and presents preliminary results from running simulations on GCP. 


summary: The field of basal cognition seeks to understand how adaptive, context-specific behavior occurs in non-neural biological systems.  Embryogenesis and regeneration require plasticity in many tissue types to achieve structural and functional goals in diverse circumstances. Thus, advances in both evolutionary cell biology and regenerative medicine require an understanding of how non-neural tissues could process information. Neurons evolved from ancient cell types that used bioelectric signaling to perform computation. However, it has not been shown whether or how non-neural bioelectric cell networks can support computation. We generalize connectionist methods to non-neural tissue architectures, showing that a minimal non-neural Bio-Electric Network (BEN) model that utilizes the general principles of bioelectricity (electrodiffusion and gating) can compute. We characterize BEN behaviors ranging from elementary logic gates to pattern detectors, using both fixed and transient inputs to recapitulate various biological scenarios. We characterize the mechanisms of such networks using dynamicalsystems and information-theory tools, demonstrating that logic can manifest in bidirectional, continuous, and relatively slow bioelectrical systems, complementing conventional neural-centric architectures. Our results reveal a variety of non-neural decision-making processes as manifestations of general cellular biophysical mechanisms and suggest novel bioengineering approaches to construct functional tissues for regenerative medicine and synthetic biology as well as new machine learning architectures. 


summary: The paper proposes a new brain-inspired computational model for action recognition. This model incorporates biologically plausible mechanisms for spiking neural networks, focusing on learning spatio-temporal patterns. These mechanisms bridge the gap between the brain's capabilities and the demands of action recognition tasks. The model's effectiveness is evaluated against a benchmark dataset (DVS-128 Gesture), where it outperforms previous biologically plausible models and shows competitive results with deep supervised models. 


summary: The membrane potential of individual neurons depends on a large number of interacting biophysical processes operating on spatial-temporal scales spanning several orders of magnitude. The multi-scale nature of these processes dictates that accurate prediction of membrane potentials in specific neurons requires utilization of detailed simulations. Unfortunately, constraining parameters within biologically detailed neuron models can be difficult, leading to poor model fits. This obstacle can be overcome partially by numerical optimization or detailed exploration of parameter space. However, these processes, which currently rely on central processing unit (CPU) computation, often incur exponential increases in computing time for marginal improvements in model behavior. As a result, model quality is often compromised to accommodate compute resources. Here, we present a simulation environment, NeuroGPU, that takes advantage of the inherent parallelized structure of graphics processing unit (GPU) to accelerate neuronal simulation. NeuroGPU can simulate most of biologically detailed models 800x faster than traditional simulators when using multiple GPU cores, and even 10-200 times faster when implemented on relatively inexpensive GPU systems. We demonstrate the power of NeuorGPU through large-scale parameter exploration to reveal the response landscape of a neuron. Finally, we accelerate numerical optimization of biophysically detailed neuron models to achieve highly accurate fitting of models to simulation and experimental data. Thus, NeuroGPU enables the rapid simulation of multi-compartment, biophysically detailed neuron models on commonly used computing systems accessible by many scientists. 


summary: Neurons are complex computational units with numerous non-linear processes, particularly in their dendrites. Biophysical models capture these processes directly by modeling physiological variables, such as ion channels, current flow, and membrane capacitance. However, cascade models, which treat neurons as cascades of linear and non-linear operations, offer another way to capture these complexities. While cascade models can capture single-cell computation, they struggle to capture sub-cellular, regenerative dendritic phenomena, such as the interaction between sodium, calcium, and NMDA spikes. This paper introduces parallel, recurrent cascade models, which model a neuron as a cascade of parallel linear and non-linear operations connected recurrently. This approach enables the capture of these additional phenomena. The paper discusses potential implications and uses for artificial intelligence, arguing that parallel, recurrent cascade models offer a unifying tool for understanding single-cell computation and exploring the algorithmic implications of physiological phenomena. 


summary: Predictive coding (PC) is a prominent model of neural computation, but its mapping onto the cortex has unresolved issues. Current implementations often use separate value and error neurons and require symmetric forward and backward weights, lacking empirical confirmation. This work proposes a constrained version of PC in the linear regime that aligns with cortical hierarchy and empirical observations. By introducing a disentangling-inspired constraint on hidden-layer neural activities, we derive an upper bound for the PC objective. Optimization of this upper bound leads to a biologically plausible network with multi-compartmental neurons and non-Hebbian learning rules, resembling experimental findings. The network does not require one-to-one connectivity or signal multiplexing, suggesting that these features are not essential for cortical learning. The normative nature of our algorithm enables analytical understanding of network components and allows for interpreting parameters as physiological quantities, bridging PC with experimental measurements. 


summary: This paper proposes a novel recurrent spiking neural network (RSNN) that incorporates neuronal-plasticity and reward-propagation mechanisms. The proposed RSNN utilizes a reward-based learning rule that is inspired by biological neural networks, which allows it to learn efficiently from sparse rewards. The authors demonstrate the effectiveness of their approach on a number of benchmark tasks, including image classification, object recognition, and memory association. They show that their RSNN outperforms traditional DNNs in terms of accuracy, efficiency, and biological plausibility. 


summary: The human brain is unmatched in its learning capabilities, energy efficiency, and scale. While conventional electronics-based methods have fallen short in replicating these features, recent advancements in photonic and electronic memristive materials, device technologies, and 3D integrated circuits offer promising alternatives. This paper proposes a brain-derived neuromorphic computing system built with artificial electronic, ionic, and photonic materials, devices, and circuits. This system aims to mimic the brain's structure and function, incorporating bio-plausible learning algorithms to enable general self-learning. The paper argues that this approach can lead to a better understanding of the relationship between neuronal and network-level properties and overall system behavior. 


summary: This paper introduces a novel set of methods and analyses for studying the dynamics of the *Caenorhabditis elegans* worm connectome. It addresses the question of how concurrent activity of independent neuronal elements gives rise to a rich behavioral repertoire, focusing on the dynamic implications of the spatial separation of neurons. The research is based on the *C. elegans* connectome, which consists of 302 neurons and their anatomical links, and aims to understand how neurons functionally interact in the context of the entire network and how the resultant dynamics regulate participating neurons. The paper investigates the *dynome*, the combination of the structural connectome and dynamic model, and proposes methods to analyze its functional consequences. 


summary: The brain processes and integrates multiple timescales into one meaningful whole. Recent evidence suggests that the brain displays a complex multiscale temporal organization. Different regions exhibit different timescales as described by the concept of intrinsic neural timescales (INT); however, their function and neural mechanisms remain unclear. We review recent literature on INT and propose that they are key for input processing. Specifically, they are shared across different species, i.e., input sharing. This suggests a role of INT in encoding inputs through matching the inputs' stochastics with the ongoing temporal statistics of the brain's neural activity, i.e., input encoding. Following simulation and empirical data, we point out input integration versus segregation and input sampling as key temporal mechanisms of input processing. This deeply grounds the brain within its environmental and evolutionary context. It carries major implications in understanding mental features and psychiatric disorders, as well as going beyond the brain in integrating timescales into artificial intelligence. 


summary: ## Abstract:

Working memory, the ability to preserve information for further processing, is essential for complex cognitive functions like reasoning, understanding, and learning. Although its neural mechanism is still not fully understood, there is evidence suggesting that persistent activity in the prefrontal cortex plays a crucial role in maintaining memory. This paper proposes a computational model of working memory based on spike-timing-dependent plasticity (STDP), which could potentially explain the observed phenomena related to persistent activity. The model utilizes a biologically plausible network of neurons and incorporates synaptic plasticity rules that reflect the known properties of STDP. The authors show that their model can reproduce the observed experimental phenomena, including the persistence of neuronal firing during the delay period, the reduction in accuracy when persistent activity is inhibited, and the ability to predict animal behavior during recall. The model provides a new understanding of how information can be encoded and retrieved in working memory. 


summary: This review considers computational psychiatry from the perspective of explaining mental illness in terms of brain function. It argues that a generative model can be used to understand both sentient processing in the brain and the scientific process in psychiatry. The review examines how the brain, as an organ of inference and prediction, processes information and how dysconnections in neuronal networks can lead to aberrant beliefs and false inferences. The paper then explores how computational models of neuronal circuits and synaptic gain control can be used to test these process theories. Finally, it discusses the potential of computational neuropsychology, computational phenotyping, and a computational nosology for psychiatry. 


summary: This paper investigates the use of Quantum Computing and Neuromorphic Computing to enhance the safety, reliability, and explainability of Multi-Agent Reinforcement Learning (MARL) in autonomous robotics. The research employed Quantum Approximate Optimization Algorithm (QAOA) to explore large solution spaces and Neuromorphic Computing for parallel and distributed processing. The combination of these technologies holds promise for improving the performance of MARL in autonomous robotics.  


summary: The synaptic organization in the Caenorhabditis elegans neural network reveals a surprising discrete clustered organization of the synapses. This organization can support local compartmentalized activities where several computations may be performed in parallel along a single neurite. This capacity of simply structured neurons can greatly enhance the computational power of compact neural networks. 


summary: This paper proposes a novel model of neuroplasticity based on a horizontal-vertical integration of neural processing. The model combines a network of neurons with adaptive transmission links (horizontal plane) with internal parameters governing external membrane-expressed parameters (vertical dimension). This approach separates neural transmission and information storage, providing a significant conceptual advancement over traditional synaptic weight models. The authors emphasize the importance of the individual neuron as a self-programming device, contrasting it with the passive view of neurons solely determined by input. This new approach aims to improve the development of third-wave AI systems and build a flexible memory system capable of processing facts and events automatically.  


summary: Grand efforts in neuroscience are working toward mapping the connectomes of many new species, including the near completion of the Drosophila melanogaster. It is important to ask whether these models could benefit artificial intelligence. In this work we ask two fundamental questions: (1) where and when biological connectomes can provide use in machine learning, (2) which design principles are necessary for extracting a good representation of the connectome. Toward this end, we translate the motor circuit of the C. Elegans nematode into artificial neural networks at varying levels of biophysical realism and evaluate the outcome of training these networks on motor and non-motor behavioral tasks. We demonstrate that biophysical realism need not be upheld to attain the advantages of using biological circuits. We also establish that, even if the exact wiring diagram is not retained, the architectural statistics provide a valuable prior. Finally, we show that while the C. Elegans locomotion circuit provides a powerful inductive bias on locomotion problems, its structure may hinder performance on tasks unrelated to locomotion such as visual classification problems. 


summary: Deep learning relies heavily on backpropagation (BP) for training artificial neural networks, but BP is often criticized for its biological implausibility. This paper introduces a generalized predictive coding (PC) algorithm called Z-IL that operates directly on computational graphs, enabling it to perform exact reverse differentiation. This means that Z-IL can now replicate the weight update of BP on any neural network, offering a biologically plausible and mathematically equivalent alternative to BP. Additionally, Z-IL provides a novel, local, and parallel implementation of BP.  


summary: In neural computation, the essential information is generally encoded into the neurons via their spiking configurations, activation values or (attractor) dynamics. The synapses and their associated plasticity mechanisms are, by contrast, mainly used to process this information and implement the crucial learning features. Here, we propose a novel Turing complete paradigm of neural computation where the essential information is encoded into discrete synaptic states, and the updating of this information achieved via synaptic plasticity mechanisms. More specifically, we prove that any 2-counter machine—and hence any Turing machine—can be simulated by a rational-weighted recurrent neural network employing spike-timing-dependent plasticity (STDP) rules. The computational states and counter values of the machine are encoded into discrete synaptic strengths. The transitions between those synaptic weights are then achieved via STDP. These considerations show that a Turing complete synaptic-based paradigm of neural computation is theoretically possible and potentially exploitable. They support the idea that synapses are not only crucially involved in information processing and learning features, but also in the encoding of essential information. This approach represents a paradigm shift in the field of neural computation. 


